# 大数据相关技术原理总结

## 一、Hadoop

### 1、HDFS 的读流程和写流程

#### HDFS 读数据流程

![HDFS 读数据流程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/HDFS%E8%AF%BB%E6%B5%81%E7%A8%8B.png)

1. 客户端通过Distributed FileSystem 向 NameNode请求下载文件，NameNode查询元数据，找到文件所在DataNode地址，返回给客户端
2. 客户端挑选最近的一台DataNode 服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Package为单位做校验）
4. 客户端以Package为单位接收，现在本地缓存，然后写入目标文件

#### HDFS 写数据流程

![HDFS 写数据流程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/HDFS%E5%86%99%E6%B5%81%E7%A8%8B.png)

1. 客户端通过Distributed FileSystem 向 NameNode 请求上传文件，NameNode 先检查文件是否存在，父目录是否存在
2. NameNode返回是否可以上传文件
3. 客户端请求第一块Block上传到哪几个DataNode服务器上
4. NameNode返回DataNode节点。假设返回3个，分别是dn1、dn2、dn3
5. 客户端通过FSOutputStream模块向dn1请求上传数据，dn1收到请求会继续调用dn2、然后dn2调用dn3，这样通信管道建立完成
6. dn1、dn2、dn3逐级应答客户端
7. 客户端开始往dn1开始上传第一个Block，以Package为单位，dn1收到一个Package就会传给dn2,dn2传给dn3；dn1每传一个Package会放入应答队列等待应答
8. 当一个Block请求上传完成之后，客户端再次请求NameNode上传下一个Block的服务器（重复3-8步骤）



### 2、Shuffle及优化

#### Shuffle过程

![mapreduce的shuffle过程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/mapreduce%E7%9A%84shuffle%E8%BF%87%E7%A8%8B.png)

Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle

过程：

1. MapTask收集我们map()方法输出的kv对，放到环形缓冲区中
2. 当环形缓冲区达到80%时，不断溢出文件到本地磁盘，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程及合并过程中，都要调用Partitioned进行分区和针对key进行排序
5. ReduceTask根据自己的分区号，去各个MapTask机器上取相应的分区数据
6. ReduceTask会取到同一个分区但是来自不同MapTask的结果文件，ReduceTask再次将这些文件进行合（归并排序）
7. 合并成大文件后，shuffle过程结束，开始进入ReduceTask运算逻辑



#### 优化

##### Map阶段

1. 增大环形缓冲区的大小（比如从100M增大到200M）
2. 增大环形缓冲区的溢写比例（比如由80%增大到90%）
3. 减少溢写文件merge次数
4. 不影响业务的情况下，采用Combiner提前合并，减少I/O

##### Reduce阶段

1. 合理设置Map和Reduce数
2. 设置Map和Reduce共存：即Map达到运行到一定时间的时候，Reduce也开始运行，减少Reduce等待时间
3. 规避使用Reduce。Reduce在连接数据集时会产生大量的网络消耗
4. 增加Reduce去拿Map中数据的并行数
5. 增加Reduce端存储数据内存的大小

##### I/O传输

采用数据压缩的方式，减少网络I/O的时间



### 3、Yarn

#### Yarn组成

Yarn由ResourceManaget、NodeManager、ApplicationMaster、Container组成

ResourceManager：负责处理客户端请求，进行资源的分配和调度

NodeManger：处理ResourceManger的命令，管理相应节点上的资源

ApplicationMaster：为应用程序申请资源并且分配任务

Container：资源的抽象，里面封装了CPU、内存等资源

#### Yarn工作机制

![yarn工作机制流程图](D:\github\MyKnowledgeRepository\picture\Yarn工作机制流程.png)

1. client向RM发起任务请求
2. RM返回给client 资源路径信息
3. client根据返回的信息，将程序资源上传到HDFS
4. 程序提交完毕之后申请运行任务
5. RM将请求信息传递给Schedule（资源调度器）；Schedule分配Container用于启动ApplicationMaster；ApplicationManager与指定的NodeManager通信，要求Container中启动ApplicationMaster
8. ApplicationMaster初始化任务，并向RM申请运行任务所需要的资源
9. RM返回相应的资源（即运行任务的NodeManager）
10. ApplicationMaster与对应的NodeManager通信，申请Container启动任务
11. Container中的应用程序会将需要的资源从HDFS中下载到本地，再启动任务
12. 运行过程中，会将运行的状态和进度汇报给ApplicationMaster，client会轮询ApplicationMaster获取状态
13. 运行完成之后，Container会注销掉，并把运行资源还回去；ApplicationMaster向RM注销自己

#### Yarn调度器

FIFO调度器：支持单队列，先进先出

容量调度器：支持多队列，保证先进入的任务优先执行

公平调度器：支持多队列，保证每个任务公平享有队列资源



多队列的好处：

1. 避免某个程序出现死循环，把资源都耗尽
2. 实现任务的降低，特殊时期保证重要任务的队列资源充足



Apache默认的资源调度器是容量调度器

CDH默认的资源调度器是公平调度器



## 二、Zookeeper

### 1、选举机制

1. 半数机制：集群中半数以上机器存活，所以Zookeeper适合安装奇数台服务器

2. Zookeeper虽然没有在配置文件中指定Master和Slave，但Zookeeper工作的时候，是有一个Leader节点，其他是Follower节点，Leader是通过内部选举产生的

3. 选举过程：

   假设有五台机器，它们的id分别是1-5。机器是依次启动的，id1先启动，但目前只有一台服务器，所以它的选举状态时looking；

   id2启动，与id1进行通信，交换自己的选举结果，id2胜出，但由于没有超过半数以上机器，所以id2的选举状态也是looking；

   id3启动，id3胜出，此时有超过半数以上机器，所以id3当选为leader

   接下来的id4、id5启动，但由于前面的机器已经选择了id3，所以id4和id5当Follwer



## 三、Spark

### 1、基于Yarn的Spark架构与作业提交流程

#### Yarn-Client

![Yarn-Client的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClient的Spark架构启动流程.png)

1. spark_submit脚本提交程序，在本地机器上启动driver
2. driver程序向ResourceManager申请启动ApplicationMaster
3. ResourceManager通知一个NodeManager去启动ApplicationMaster
4. ApplicationMaster启动完成之后向ResourceManager申请运行Executor
5. ResourceManager分配Container返回给ApplicationMaster
6. ApplicationMaster与对应的NodeManger通信，申请和启动Container
7. NodeManager启动Executor进程
8. Executor启动完毕后反向注册driver



#### Yarn-Cluster

![YarnCluster的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClusterd的Spark架构流程.png)



1. 本地机器提交Spark Application到ResourceManager
2. ResourceManager找到一个NodeManager启动ApplicationMaster
3. NodeManager启动ApplicationMaster并启动相应的Driver进程
4. ApplicationMaster向ResouceManager申请启动Executor
5. ResourceManager分配container返回给ApplicationMaster
6. ApplicationMaster与相应的NodeManager通信，申请启动exetutor
7. NodeManager启动executor进程
8. Executor进程启动完毕之后找ApplicationMaster进行反向注册



#### yarn-client和yarn-cluster模式的不同之处

YarnClient模式下，driver运行在本地机器

YarnCluster模式下，driver运行在集群某个NodeManager的节点上面



### 2、Spark的两种核心Shuffle

#### HashShuffle

##### 未优化的HashShuffle

![未优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\未优化的HashShuffle.png)

1. 第一个Stage，每个task都会创建下一个Stage的task数量相同的文件
2. 第二个Stage，每个task会到各个节点上面，拉取第一个stage上每个task的属于自己的输出文件



##### 优化的HashShuffle（开启map端文件合并）

![优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\优化后的HashShuffle.png)

1. 第一个Stage，每个task依旧会创建下一个Stage，task数量相同的输出文件
2. 但是下一个task，直接复用上一个task的输出文件
3. 这样，第二个Stage，就会拉取少量的输出文件

#### SortShuffle

![SortShuffle](D:\github\MyKnowledgeRepository\picture\SortShuffle.png)



![bypass SortShuffle](D:\github\MyKnowledgeRepository\picture\bypass SortShuffle.png)

在sortShuffle中可以设置一个阈值，默认是200;当reduce task数量少于等于200时，即map task创建的文件少于等于200时，不会进行排序，依旧会输出reduce task的数量，最后将所有的数据文件合并成一个大文件



#### HashShuffle和SortShuffle的区别

1. SortShuffle会对每个reduce task要处理的数据进行排序
2. SortShuffle只会写入一个磁盘文件，不同reduce task的任务通过offset来界定



### 3、Spark共享变量

#### 共享变量原理

默认情况下，如果在某个算子函数中使用到了某个外部变量，那么这个变量的值会拷贝到每个task中。每个task只能操作自己的那份变量副本。如果多个task想要共享变量，这种方式是做不到的。



所以Spark提供了两种共享变量：广播变量和累加器



广播变量使用变量，仅仅会为每个节点(即Executor)拷贝一份，减少了网络传输和内存消耗

累加器可以让多个task共同操作一份变量，主要进行累加操作

#### 广播变量

广播变量是只读，它只会为每个节点拷贝一份副本，而不是为每个task拷贝一份副本。

它最大的好处就是减少了网络传输和内存消耗

#### 累加器

累加器主要用于多个节点对一个变量进行共享性操作。

Accumulator只提供给了累计操作。提供了多个task对一个变量共享的操作

但是task只能对Accumulator进行累加操作，不能读取

只有Driver程序可以读取Accumulator的值



### 4、Spark实现TopN的获取

算法思路：

1. 读入数据
2. 将数据通过map映射成KV格式的数据
3. 按照key对数据进行聚合（比如使用reduceByKey、groupBykey等）
4. 计算各个分区的TopN，使用SortMap来实现。也就是TreeMap，它实现了SortMap的接口
5. 规约所有的TopN的SortMap，得到最终的ToN SortMap



```java
/*
    *   程序入口函数
    * */
public void run() {
        /*
        *   读入inputPath中的数据
        * */
        JavaRDD<String> lines = jsc.textFile(inputPath, 1);

        /*
        *   将rdd规约到9个分区
        * */
        JavaRDD<String> rdd = lines.coalesce(9);

        /*
        *   将输入转化为kv格式
        *   key是规约的主键, value是排序参考的个数
        *   注: 这里的key并不唯一, 即相同的key可能有多条记录, 所以下面我们规约key成唯一键
        *   输入:line, 输出:kv
        * */
        JavaPairRDD<String, Integer> kv = rdd.mapToPair(
            new PairFunction<String, String, Integer>() {
            public Tuple2<String, Integer> call(String s) throws Exception {
                String[] tokens = s.split(",");
                return new Tuple2<String, Integer>(tokens[0], Integer.parseInt(tokens[1]));
            }
        });

        /*
        *   规约主键成为唯一键
        *   输入:kv, 输出:kv
        * */
        JavaPairRDD<String, Integer> uniqueKeys = kv.reduceByKey(new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer i1, Integer i2) throws Exception {
                return i1 + i2;
            }
        });

        /*
        *   计算各个分区的topN
        *   这里通过广播变量拿到了topN具体个数, 每个分区都保留topN, 所有分区总个数: partitionNum * topN
        *   输入:kv, 输出:SortMap, 长度topN
        * */
        JavaRDD<SortedMap<Integer, String>> partitions = uniqueKeys.mapPartitions(new FlatMapFunction<Iterator<Tuple2<String,Integer>>, SortedMap<Integer, String>>() {
            public Iterable<SortedMap<Integer, String>> call(Iterator<Tuple2<String, Integer>> iter) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                while (iter.hasNext()) {
                    Tuple2<String, Integer> tuple = iter.next();
                    topN.put(tuple._2, tuple._1);

                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return Collections.singletonList(topN);
            }
        });

        /*
        *   规约所有分区的topN SortMap, 得到最终的SortMap, 长度topN
        *   reduce过后, 数据已经到了本地缓存, 这是最后结果
        *   输入: SortMap, 长度topN, 当然有partitionNum个, 输出:SortMap, 长度topN
        * */
        SortedMap<Integer, String> finalTopN = partitions.reduce(new Function2<SortedMap<Integer, String>, SortedMap<Integer, String>, SortedMap<Integer, String>>() {
            public SortedMap<Integer, String> call(SortedMap<Integer, String> m1, SortedMap<Integer, String> m2) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                for (Map.Entry<Integer, String> entry : m1.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                for (Map.Entry<Integer, String> entry : m2.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return topN;
            }
        });
```



### 5、SparkStreaming消费Kafka中的数据的方式？或者说SparkStreaming与Kafka的集成方式？

#### 一、基于Receiver的方式

这种方式是通过receiver来获取数据，使用的是Kafka的高级API，receiver从kafka中获取的数据都是存储在Spark Executor的内存中（如果数据量突然暴增，大量batch堆积，很容易出现内存溢出问题），然后启动job去处理数据。



然而在这种方式下，很可能会因为底层的失败而丢失数据，如果用启动高可靠机制，让数据零丢失，需要开启预写日志机制（Writer Ahead Log,WAL）。该机制会同步地将接收到Kafka的数据写入到分布式存储系统上的预写日志中（比如HDFS）。这样即使底层节点的失败了，也可以从预写日志中的数据进行恢复。

#### 二、基于Direct的方式

基于Direct的方式会周期性地查询Kafka，来获得每个topic的partition的最新offset，从而定义每个offset的范围。当处理数据的job启动时，会使用Kafka的低级API来获取Kafka指定offset范围的数据。



**优点：**

简单并行读取：如果要读取多个partition的数据，spark会创建跟kafka partition相同的分区数，并且会并行地从Kafka中读取数据。

高性能：如果要保证数据零丢失，基于Receiver的方式是要开启WAL机制。这种方式是效率低下的，因为数据复制了两份。因为kafka本身就有高可靠机制，他自己会复制一份，WAL又复制了一份。而基于Direct的方式，不依赖于Receiver，不需要开启WAL，只要kafka做了数据的备份，就可以通过kafka的副本进行恢复



#### 两种方式的对比

基于Receiver的方式使用了Kafka的高级API来在Zookeeper中保存消费过offset，配合着WAL机制可以保证数据的高可靠和零丢失。但是无法保证数据被处理一次且仅一次，可能会处理两次。因为spark和Zookeeper之间可能不是同步的



基于Direct的方式使用了Kafka的低级API，spark streaming自己负责追踪offset，并保存checkpoint。spark自己一定是同步的，所以可以保证数据是消费一次且仅一次。

#### 知识补充（Kafka的高级API和低级API）

**高级API**

优点：

1. 简单
2. 不需要管理offset，由Zookeeper管理
3. 不需要管理分区、副本等情况

缺点：

1. 不能控制offset(对于某些特殊需求来说)
2. 不能细化控制分区、副本、Zookeeper



**低级API**

优点：

1. 能够自己控制offset，想从哪里读取数据就哪里读取数据
2. 能够控制分区数，对分区数进行负载均衡
3. 降低了对Zookeeper的依赖

缺点：

1. 太过于复杂，需要自己管理offset，连接哪个分区



### 6、SparkStreaming窗口函数原理

窗口函数就是在原来sparkstreaming的批次的基础上，再次进行封装，每次计算多个批次的数据，同时传入一个滑动步长，用于设置当前任务计算完成后，下次任务开始计算的地方。



![滑动窗口](D:\github\MyKnowledgeRepository\picture\滑动窗口.png)

图中time1就是sparkStreaming计算批次的大小，虚线框以及实线框的大小就是窗口的大小，由虚线框到实线框的距离就是滑动的步长











