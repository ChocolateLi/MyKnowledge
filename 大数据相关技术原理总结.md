# 大数据相关技术原理总结

## 一、Hadoop

### 1、HDFS 的读流程和写流程

#### HDFS 读数据流程

![HDFS 读数据流程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/HDFS%E8%AF%BB%E6%B5%81%E7%A8%8B.png)

1. 客户端通过Distributed FileSystem 向 NameNode请求下载文件，NameNode查询元数据，找到文件所在DataNode地址，返回给客户端
2. 客户端挑选最近的一台DataNode 服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Package为单位做校验）
4. 客户端以Package为单位接收，现在本地缓存，然后写入目标文件

#### HDFS 写数据流程

![HDFS 写数据流程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/HDFS%E5%86%99%E6%B5%81%E7%A8%8B.png)

1. 客户端通过Distributed FileSystem 向 NameNode 请求上传文件，NameNode 先检查文件是否存在，父目录是否存在
2. NameNode返回是否可以上传文件
3. 客户端请求第一块Block上传到哪几个DataNode服务器上
4. NameNode返回DataNode节点。假设返回3个，分别是dn1、dn2、dn3
5. 客户端通过FSOutputStream模块向dn1请求上传数据，dn1收到请求会继续调用dn2、然后dn2调用dn3，这样通信管道建立完成
6. dn1、dn2、dn3逐级应答客户端
7. 客户端开始往dn1开始上传第一个Block，以Package为单位，dn1收到一个Package就会传给dn2,dn2传给dn3；dn1每传一个Package会放入应答队列等待应答
8. 当一个Block请求上传完成之后，客户端再次请求NameNode上传下一个Block的服务器（重复3-8步骤）



### 2、Shuffle及优化

#### Shuffle过程

![mapreduce的shuffle过程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/mapreduce%E7%9A%84shuffle%E8%BF%87%E7%A8%8B.png)

Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle

过程：

1. MapTask收集我们map()方法输出的kv对，放到环形缓冲区中
2. 当环形缓冲区达到80%时，不断溢出文件到本地磁盘，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程及合并过程中，都要调用Partitioned进行分区和针对key进行排序
5. ReduceTask根据自己的分区号，去各个MapTask机器上取相应的分区数据
6. ReduceTask会取到同一个分区但是来自不同MapTask的结果文件，ReduceTask再次将这些文件进行合（归并排序）
7. 合并成大文件后，shuffle过程结束，开始进入ReduceTask运算逻辑



#### 优化

##### Map阶段

1. 增大环形缓冲区的大小（比如从100M增大到200M）
2. 增大环形缓冲区的溢写比例（比如由80%增大到90%）
3. 减少溢写文件merge次数
4. 不影响业务的情况下，采用Combiner提前合并，减少I/O

##### Reduce阶段

1. 合理设置Map和Reduce数
2. 设置Map和Reduce共存：即Map达到运行到一定时间的时候，Reduce也开始运行，减少Reduce等待时间
3. 规避使用Reduce。Reduce在连接数据集时会产生大量的网络消耗
4. 增加Reduce去拿Map中数据的并行数
5. 增加Reduce端存储数据内存的大小

##### I/O传输

采用数据压缩的方式，减少网络I/O的时间



### 3、Yarn

#### Yarn组成

Yarn由ResourceManaget、NodeManager、ApplicationMaster、Container组成

ResourceManager：负责处理客户端请求，进行资源的分配和调度

NodeManger：处理ResourceManger的命令，管理相应节点上的资源

ApplicationMaster：为应用程序申请资源并且分配任务

Container：资源的抽象，里面封装了CPU、内存等资源

#### Yarn工作机制

![yarn工作机制流程图](D:\github\MyKnowledgeRepository\picture\Yarn工作机制流程.png)

1. client向RM发起任务请求
2. RM返回给client 资源路径信息
3. client根据返回的信息，将程序资源上传到HDFS
4. 程序提交完毕之后申请运行任务
5. RM将请求信息传递给Schedule（资源调度器）；Schedule分配Container用于启动ApplicationMaster；ApplicationManager与指定的NodeManager通信，要求Container中启动ApplicationMaster
8. ApplicationMaster初始化任务，并向RM申请运行任务所需要的资源
9. RM返回相应的资源（即运行任务的NodeManager）
10. ApplicationMaster与对应的NodeManager通信，申请Container启动任务
11. Container中的应用程序会将需要的资源从HDFS中下载到本地，再启动任务
12. 运行过程中，会将运行的状态和进度汇报给ApplicationMaster，client会轮询ApplicationMaster获取状态
13. 运行完成之后，Container会注销掉，并把运行资源还回去；ApplicationMaster向RM注销自己

#### Yarn调度器

FIFO调度器：支持单队列，先进先出

容量调度器：支持多队列，保证先进入的任务优先执行

公平调度器：支持多队列，保证每个任务公平享有队列资源



多队列的好处：

1. 避免某个程序出现死循环，把资源都耗尽
2. 实现任务的降级，特殊时期保证重要任务的队列资源充足



Apache默认的资源调度器是容量调度器

CDH默认的资源调度器是公平调度器



### 4、HDFS感知机架（副本存储节点选择）

**低版本Hadoop副本选择**

第一个副本在client所在的节点上。如果节点在集群外，则随机选择

第二个副本和第一个副本位于不同的机架随机节点上

第三个副本和第二个副本位于相同机架，节点随机选择

![**低版本Hadoop副本选择**](D:\github\MyKnowledgeRepository\picture\低版本Hadoop机架选择.png)



**Hadoop 2.7.2副本节点选择**

第一个副本在client所在的节点上。如果client在集群外，则随机选择。

第二个副本和第一个副本位于相同机架，随机节点

第三个副本位于不同机架上，随机节点

![Hadoop 2.7.2副本节点选择](D:\github\MyKnowledgeRepository\picture\hadoop2.7.2版本的副本选择.png)





## 二、Hive

### 1、Hive架构

![Hive架构](D:\github\MyKnowledgeRepository\picture\Hive架构.png)

1. Hive存储的数据在HDFS上
2. Hive分析数据的底层是MapReduce
3. Hive程序运行是在Yarn上

### 2、Hive和数据库比较

1. 数据存储位置不同。Hive的数据一般存储在HDFS上，数据库的数据一般存储在块设备或者本地文件系统
2. 数据更新不同。Hive是不建议对数据改写的，而数据库的数据是需要经常修改的
3. 数据规模不同。Hive一般处理大规模数据，数据库一般处理小规模数据。

### 3、内部表和外部表

内部表：表删除，数据也删除。即元数据和原始数据都删除

外部表：表删除，数据还在。即元数据删了，但原始数据还在



### 4、4个By的区别

order by：全局排序，只有一个MapReduce

sort by：分区内有序。每个MapReduce内部排序，局部排序，不是全局排序

distribute by：类似于MapReduce的partition，分区，跟sort by结合使用。disttibute by语句要在sort by语句之前

cluster by：当distribute by和sort by字段相同时，可以考虑使用cluster by代替。但他的排序只能是升序排序



### 5、窗口函数

指定分析函数的数据窗口大小，窗口的大小随着行的变换而变化。一般跟在聚合函数后，只对聚合函数有效，不是全局。



over(partition by ... order by ...)关键字



Rank函数

row_number():按顺序排序，1 2 3 4

rank():按顺序排序，重复跳过，总数不变 1 2 2 4

dense_rank():按顺序排序，重复不跳过，总数减少 1 2 2 3 



### 6、手写HQL

#### 手写HQL 第1题

score表结构：uid,subject_id,score

数据集如下

```
1001 01 90
1001 02 85
...
```

问题：找出所有科目成绩都大于某一学科平均成绩的学生

思路：

1. 求出每个学科平均成绩

   ```sql
   select uid,score,
   avg(score) over(partition by subject_id) avg_score
   from score;t1
   ```

   

2. 根据是否大于平均成绩 记录flag，大于记为0否则记为1

   ```sql
   select uid,if(score>avg_score,0,1) flag from t1;t2
   ```

   

3. 根据学生id进行分组统计flag的和，和为0，说明所有学科都大于平均成绩

   ```sql
   select uid from t2
   group by uid
   having sum(flag)=0;
   ```

最终SQL

```sql
select uid from(
	select uid,if(score>avg_score) flag from(
    	select uid,score,avg(score) over(partition by subject_id) avg_score
        from score
    )t1
)t2
group by uid
having sum(flag)=0;
```

#### 手写HQL 第2题

有如下用户访问数据，action表

```text
userid	visitDate	visitCount
u01		2017/1/21	5
u02		2017/1/23	6
u01		2017/1/23	6
...
```

问题：要求使用SQL统计出每个用户的累积访问次数，如下表

```
userid	月份	  小计	累积
u01		2017-1	11	   11
u01		2017-2	12	   23
...
```

思路：

1. 先修改数据格式

   ```sql
   select userid,
   date_format(regexp_replace(visitDate,'/','%'),'yyyy-MM') mn,
   visitCount
   from action;t1
   ```

2. 计算每人每月访问量

   ```sql
   select userid,mn,sum(visitCount) mn_count
   from t1
   group by userid,mn;t2
   ```

3. 按月累加访问量

   ```sql
   select userid,mn,mn_count,
   sum(mn_count) over(partition by userid order by mn)
   from t2;
   ```

最终SQL

```sql
select userid,mn,mn_count,
sum(mn_count) over(partition by userid order by mn)
from(
	select userid,mn,sum(visitCount) mn_count from(
    	select userid,
		date_format(regexp_replace(visitDate,'/','%'),'yyyy-MM') mn,
		visitCount
		from action
    ) t1
    group by userid,mn
)t2
```

总结：

date_form() 日期格式化函数

regexp_replace() 格式替换函数，针对字符串

sum() over() 可以实现累加求和的效果

group x y：表示将所有具有相同x字段和相同y字段的记录放到一个分组里



#### 手写HQL 第3题

有50W个京东店铺，每个顾客访问过任何一个店铺的任何一个商品都会产生一条访问日志，访问日志表名为visit，用户id为user_id，店铺名称为shop

```
user_id	shop
u1		a
u2		b
u1		b
...
```

问题：

（1）统计店铺的UV(访客数)

```sql
select shop,count(distinct user_id) group by shop;
```

（2）每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数

思路：

1. 查询每个店铺被每个用户访问的次数

   ```sql
   select shop,user_id,count(*) ct
   from visit
   group by shop,user_id;t1
   ```

   

2. 计算每个店铺被用户访问的排名

   ```sql
   select shop user_id,ct,
   rank() over(partition by shop order by ct desc) rk
   from t1;t2
   ```

   

3. 取每个店铺排名前3

   ```sql
   select shop,user_id,ct
   from t2
   where rk<=3;
   ```

最终SQL

```sql
select shop,user_id,ct from(
	select shop user_id,ct,
	rank() over(partition by shop order by ct desc) rk
	from(
    	select shop,user_id,count(*) ct
		from visit
		group by shop,user_id
    )t1
) t2
where rk<=3
```

总结：

count 和 distinct一起使用，可以实现计算非重复结果的数目

rank() over() 可以实现排序



#### 手写HQL 第4题

已知一个表order1，字段date,order_id,user_id,amount。数据样例：2017-01-01,10028555,10002154,33

问题：

（1）给出2017年每个月的订单数、用户数、总成交金额

```sql
select date_format(date,'yyyy-MM'),count(order_id),count(distinct user_id),sum(amount)
from order1
where date_format(date,'yyyy') = '2017'
group by date_format(date,'yyyy-MM')
```

（2）给出2017年11月的新客数(指在11月才有第一笔订单)（不理解）

```sql
select count(user_id)
from order1
group by user_id
having date_format(min(date),'yyyy-MM') = '2017-11';
```

总结：

不理解min(date)



#### 手写HQL 第5题

日志如下user_age表，请求出所有用户和活跃用户的总数及平均年龄（活跃用户指连续两天都有访问记录的用户）

```
date	user_id		age
2019-02-11,test_1,23
2019-02-11,test_2,13
...
```

思路：

1. 按照日期及用户分组，按照日期排序并给出排名（给日期排名的作用是后面用来求活跃用户的）

   ```sql
   select date,user_id,min(age) age,
   rank() over(partition by user_id order by date) rk
   from user_age
   group by date,user_id;t1
   ```

2. 计算日期和排名的差值

   ```sql
   select user_id,age,date_sub(date,rk) flag
   where t1;t2
   ```

3. 过滤出差值大于等于2的用户，即为连续两天的用户

   ```sql
   select user_id,min(age) age
   from t2
   group by user_id,age
   having count(*)>=2;t3
   ```

4. 对数据去重处理（一个用户可以在两个不同的时间点连续登录），例如：a用户在1月10号 1月11号 1月20号 1月24号 4天登录

   ```sql
   select user_id,min(age) age
   from t3
   group by user_id;t4
   ```

5. 实在不会，太难了





## 三、HBase

### 1、HBase逻辑结构和物理存储结构

**逻辑结构**

不同的列族放在不同文件夹存储的

row_key是有序的，按字典序

Region是一张表的切片，而且是横向切分的

store真正放在HDFS上存储的东西

![HBase逻辑结构](D:\github\MyKnowledgeRepository\picture\HBase逻辑结构.png)



**物理结构**

HBase实现随机读写操作完全靠时间戳

![物理结构](D:\github\MyKnowledgeRepository\picture\HBase物理存储结构.png)



还有一种数据结构cell

cell：由{row_key,column Family,column,TimeStamp}唯一确定单元。cell中数据是没有类型的，全部是字节码形式存储。



### 2、HBase架构图（同时也是它的存储结构）

![HBase架构图](D:\github\MyKnowledgeRepository\picture\HBase架构图.png)



Master：管理RegionServer，处理Region的分配或转移

RegionServer：负责存储HBase实际的数据

Store：一个Store对应HBase表中的一个列族

Mem Store：内存存储，用来保存当前数据的操作

HFile：实际存储的物理文件。StoreFile是以HFile的形式存储在HDFS上

HLog：WAL,预写入日志机制



### 3、HBase原理

#### 读流程

1. client向Zookeeper请求meta表所在的位置regionserver
2. Zookeeper查询并返回mata表位置给client
3. client向请求meta表所在的服务器regionserver
4. regeionserver返回meta表数据
5. client向regionserver发起读请求
6. regionserver查询MenStore和StoreFile的数据，并返回给client

![HBase读流程](D:\github\MyKnowledgeRepository\picture\HBase读流程.png)



#### 写流程

1. client向Zookeeper请求meta表所在的位置regionserver
2. Zookeeper查询并返回mata表位置给client
3. client向请求meta表所在的服务器regionserver
4. regeionserver返回meta表数据
5. client向regionserver发起写请求
6. regionserver将数据写入到HLog，为了数据的持久化和恢复
7. regionserver将数据写入到内存Mem Store
8. 反馈client写入成功

![HBase写流程](D:\github\MyKnowledgeRepository\picture\HBase写流程.png)



#### flush

1. 当MemStore数据达到一定阈值时，会把MemStore的数据flush到StoreFile中，将内存中的数据删除，同时删除HLog中的历史数据
2. 数据是存储到HDFS上

![flush](D:\github\MyKnowledgeRepository\picture\MemSore Flush过程.png)



#### compact

当StoreFile越来越多，会触发compact合并操作，把多个StoreFile合并成一个大的StoreFile

![compact](D:\github\MyKnowledgeRepository\picture\compact过程.png)



#### Split

当StoreFile越来越大，Region也越来越大，达到一定阈值，会触发Split操作，将Region一分为二



#### 4、Region预分区

预分区的目的主要是创建表的时候指定分区数，提前规划好表有多少个分区数，以及每个分区的区间范围，这样在存储的时候，rowkey会按照分区的区间存储，可以避免region热点问题



#### 5、RowKey设计原则

唯一性。确保每一条数据唯一

散列性。将rowkey均匀地散列在各个区间。所以使用散列性需要设计好预分区

长度原则。rowkey是存储到内存中的，如果rowkey太大的话，会占用大量的内存空间。

#### 6、RowKey如何设计

设计RowKey的主要目的是让数据均匀地分布在所有的region分区中，在一定程度上防止数据倾斜。

1. 通过随机数、hash、散列值(UUID)的方式将key打散
2. 字符串反转。时间戳是有规律的，但反过来就没规律了。还有手机号

#### 7、Phoenix二级索引的原理

Phoenix是一个框架。这个框架可以使用SQL语句的形式去操作HBase。

它的底层原理就是会把我们写的SQL语句翻译成HBase的指令，再用指令去操作HBase分布式集群。



HBase一级索引默认是开启的，我们程序员是不能干预的。









## 四、Zookeeper

### 1、选举机制

1. 半数机制：集群中半数以上机器存活，所以Zookeeper适合安装奇数台服务器

2. Zookeeper虽然没有在配置文件中指定Master和Slave，但Zookeeper工作的时候，是有一个Leader节点，其他是Follower节点，Leader是通过内部选举产生的

3. 选举过程：

   假设有五台机器，它们的id分别是1-5,同时他们是最新启动的，没有历史数据。

   机器是依次启动的，id1先启动，但目前只有一台服务器，他发出去的报文没人响应，所以它的选举状态是looking状态；

   id2启动，与id1进行通信，交换自己的选举结果。由于双方都没有历史数据数据，id值较大的会胜出，因此id2胜出。但由于没有超过半数以上机器同意选举它，所以id1和id2的选举状态都保持looking状态；

   id3启动，根据前面的理论，id3胜出，此时有超过半数以上机器选举它，所以id3当选为leader
   
   接下来的id4、id5启动，但由于前面的机器已经选择了id3，所以id4和id5当Follwer

### 2、什么是CAP法则？Zookeeper符合哪两个？

CAP法则是指在分布式系统中，一致性、可用性、分区容错性三者不可兼得

分区容错是指区间通信可能失败，系统不能在时限内保持数据一致性。

一致性是指在分布式所有数据备份中，在同一时刻是否同样的值。

可用性是指集群一部分节点出现故障后，集群整体是否还能响应客户端的读写请求



Zookeeper符合强一致性和高可用性！



### 3、Paxos算法

Paxos算法是一种基于消息传递的分布式一致性算法



它要解决的问题是：一个分布式系统就某个值(也就是协议)达成一致的问题

一个典型的场景就是分布式数据库系统中，如果各节点初始状态一致，每个节点执行相同的操作系列，那么最后他们会得到一个一致的状态。



但它有一个前提条件：通信是保证可靠的不会被篡改的



Paxos算法大致流程（大白话讲）：

在整个提议和投票过程中，最重要的就是“提议者”和“接受者”

第一阶段：因为存在多个“提议者”,如果都提意见，那么“接受者”就很为难。所以要先明确哪个“提议者”是意见领袖，有权提出提议。“接受者”就要处理这个提议

第二个阶段：由上述意见领袖提出提议，“接收者”反馈意见，如果多数“接收者”接受了这个提议，那么提议就通过了。



怎么明确意见领袖？

通过编号。每个“提议者”在第一阶段先报个号，谁的号大谁就是意见领袖。



## 五、Kafka

### 1、Kafka的架构

![Kafka的架构](D:\github\MyKnowledgeRepository\picture\Kafka架构.png)

组成：生产者、Broker(Kafka服务器)、消费者、Zookeeper

注意：Zookeeper只保存了Broker的id信息，以及消费者的offset信息；没有生产者的信息



Consumer：消息生产者，向Kafka发送消息的客户端

Consumer Group：这是Kafka实现广播和单播的手段。所谓的广播就是把消息发给所有的Comsumer，单播就是发给任意一个Consumer。

Broker：一台Kafka服务器就是一个Broker。一个集群由多个Broker组成

Topic：主题。可以理解为一个队列，里面存储相应信息的内容。一个Broker可以拥有多个Topic

Partition：分区。为了实现扩展性，一个非常大的Topic可以分布在多个Broker上，一个Topic可以有多个	Partition，每个Partition都是一个有序的队列。（一个Partition只能被一个消费者组的一个消费者消费，但它可以同时被不同消费者组的消费者消费）

Offset：偏移量。Partition里面的每条消息，都会分配一个有序的id，这个id就是Offset。他记录着你要消费的位置



### 2、Kafka的作用（为什么使用Kafka？即为什么使用消息队列？消息队列的作用是什么？）

解耦和扩展性：将生产者和消费者解耦开来，可以独立地扩展他们相应的功能

缓冲和削峰：如果上游数据突然暴增，下游可能扛不住。所有可以通过消息队列进行缓冲，下游队列可以慢慢从消息队列取数据处理。

异步处理：有时候并不需要立即处理数据，可以通过消息队列的异步处理机制，它允许用户把消息放入消息队列里，但不立即处理它。



### 3、ISR副本同步队列

一般副本设置2个或3个

副本的优势：提高可靠性

副本的劣势：增加网络IO



副本操作是以分区为单位的，每个副本都有自己的主副本和从副本。

主副本叫Leader，从副本叫Follower。处于同步状态下的副本叫 ISR（in-Sync-Replicas）



生产者和消费者都是和Leader交互读写数据，不和Follower交互。



如果Leader挂了，会从副本同步队列中选择一个Follower作为新的Leader



### 4、Kafka的数据不丢失机制

从生产者、消费者、Broker三方面回答。

1. 生产者数据的不丢失

   ACK机制：当Kafka发送消息的时候，都会有一个消息确认反馈机制，确保消息是否正常收到，有0，1，-1三种状态。

   ack=0，异步发送，消息发送完，立即发送下一条消息

   ack=1，Producer等待Leader确认收到数据，才发送下一条消息

   ack=-1，Producer等到Follower确认，才发送下一条消息

   

2. 消费者数据的不丢失

   通过Offset来保证每次消费的记录

   

3. Broker数据不丢失

   通过备份副本，来保证数据不丢失

   

### 5、Kafka数据是放在内存上还是磁盘上，为什么速度那么快？

放在磁盘上。

速度快的原因：

1. Kafka本身就是分布式集群，同时采用了分区技术，并发度高
2. 顺序写入磁盘。硬盘是机械结构，每次读写的操作是“寻址->写入”。寻址的过程中是很耗时的。为了提高读写速度，它采用了顺序写入。



### 6、Kafka消费过的消息如何再消费？

得从Offset下手，因为它记录着消费者的消费位置。而Offset一般是定义在Zookeeper上，如果要重复消费，需要重设Zookeeper上的Offset。可以通过Redis记录Offset的checkpoint点，当想重复消费的时候，读取Redis上的checkpoint点进行Zookeeper上Offset的重设。



### 7、Kafka数据重复怎么处理？

可以到下一级处理：Spark Streaming，Redis，Hive中去重



### 8、Kafka消息数据积压，Kafka消费能力不足怎么处理？

1. 可以增加Topic的分区数，增加消费者组的消费者数，分区数=消费者数（二者缺一不可）
2. 如果是下游数据处理不及时，可以增加每次批次拉取的数量。批次拉取的数据量太少，导致拉取数据+处理数据的速度< 生产数据的速度，处理数据小于生产数据，也会导致数据积压。



## 六、Spark

### 1、基于Yarn的Spark架构与作业提交流程

#### Yarn-Client

![Yarn-Client的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClient的Spark架构启动流程.png)

1. spark_submit脚本提交程序，在本地机器上启动driver
2. driver程序向ResourceManager申请启动ApplicationMaster
3. ResourceManager通知一个NodeManager去启动ApplicationMaster
4. ApplicationMaster启动完成之后向ResourceManager申请运行Executor
5. ResourceManager分配Container返回给ApplicationMaster
6. ApplicationMaster与对应的NodeManger通信，申请和启动Container
7. NodeManager启动Executor进程
8. Executor启动完毕后反向注册driver



#### Yarn-Cluster

![YarnCluster的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClusterd的Spark架构流程.png)



1. 本地机器提交Spark Application到ResourceManager
2. ResourceManager找到一个NodeManager启动ApplicationMaster
3. NodeManager启动ApplicationMaster并启动相应的Driver进程
4. ApplicationMaster向ResouceManager申请启动Executor
5. ResourceManager分配container返回给ApplicationMaster
6. ApplicationMaster与相应的NodeManager通信，申请启动exetutor
7. NodeManager启动executor进程
8. Executor进程启动完毕之后找ApplicationMaster进行反向注册



#### yarn-client和yarn-cluster模式的不同之处

YarnClient模式下，driver运行在本地机器

YarnCluster模式下，driver运行在集群某个NodeManager的节点上面



### 2、Spark的两种核心Shuffle

#### HashShuffle

##### 未优化的HashShuffle

![未优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\未优化的HashShuffle.png)

1. 第一个Stage，每个task都会创建下一个Stage的task数量相同的文件
2. 第二个Stage，每个task会到各个节点上面，拉取第一个stage上每个task的属于自己的输出文件



##### 优化的HashShuffle（开启map端文件合并）

![优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\优化后的HashShuffle.png)

1. 第一个Stage，每个task依旧会创建下一个Stage，task数量相同的输出文件
2. 但是下一个task，直接复用上一个task的输出文件
3. 这样，第二个Stage，就会拉取少量的输出文件

#### SortShuffle

![SortShuffle](D:\github\MyKnowledgeRepository\picture\SortShuffle.png)



![bypass SortShuffle](D:\github\MyKnowledgeRepository\picture\bypass SortShuffle.png)

在sortShuffle中可以设置一个阈值，默认是200;当reduce task数量少于等于200时，即map task创建的文件少于等于200时，不会进行排序，依旧会输出reduce task的数量，最后将所有的数据文件合并成一个大文件



#### HashShuffle和SortShuffle的区别

1. SortShuffle会对每个reduce task要处理的数据进行排序
2. SortShuffle只会写入一个磁盘文件，不同reduce task的任务通过offset来界定



### 3、Spark共享变量

#### 共享变量原理

默认情况下，如果在某个算子函数中使用到了某个外部变量，那么这个变量的值会拷贝到每个task中。每个task只能操作自己的那份变量副本。如果多个task想要共享变量，这种方式是做不到的。



所以Spark提供了两种共享变量：广播变量和累加器



广播变量使用变量，仅仅会为每个节点(即Executor)拷贝一份，减少了网络传输和内存消耗

累加器可以让多个task共同操作一份变量，主要进行累加操作

#### 广播变量

广播变量是只读，它只会为每个节点拷贝一份副本，而不是为每个task拷贝一份副本。

它最大的好处就是减少了网络传输和内存消耗

#### 累加器

累加器主要用于多个节点对一个变量进行共享性操作。

Accumulator只提供给了累计操作。提供了多个task对一个变量共享的操作

但是task只能对Accumulator进行累加操作，不能读取

只有Driver程序可以读取Accumulator的值



### 4、Spark实现TopN的获取

算法思路：

1. 读入数据
2. 将数据通过map映射成KV格式的数据
3. 按照key对数据进行聚合（比如使用reduceByKey、groupBykey等）
4. 计算各个分区的TopN，使用SortMap来实现。也就是TreeMap，它实现了SortMap的接口
5. 规约所有的TopN的SortMap，得到最终的ToN SortMap



```java
/*
    *   程序入口函数
    * */
public void run() {
        /*
        *   读入inputPath中的数据
        * */
        JavaRDD<String> lines = jsc.textFile(inputPath, 1);

        /*
        *   将rdd规约到9个分区
        * */
        JavaRDD<String> rdd = lines.coalesce(9);

        /*
        *   将输入转化为kv格式
        *   key是规约的主键, value是排序参考的个数
        *   注: 这里的key并不唯一, 即相同的key可能有多条记录, 所以下面我们规约key成唯一键
        *   输入:line, 输出:kv
        * */
        JavaPairRDD<String, Integer> kv = rdd.mapToPair(
            new PairFunction<String, String, Integer>() {
            public Tuple2<String, Integer> call(String s) throws Exception {
                String[] tokens = s.split(",");
                return new Tuple2<String, Integer>(tokens[0], Integer.parseInt(tokens[1]));
            }
        });

        /*
        *   规约主键成为唯一键
        *   输入:kv, 输出:kv
        * */
        JavaPairRDD<String, Integer> uniqueKeys = kv.reduceByKey(new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer i1, Integer i2) throws Exception {
                return i1 + i2;
            }
        });

        /*
        *   计算各个分区的topN
        *   这里通过广播变量拿到了topN具体个数, 每个分区都保留topN, 所有分区总个数: partitionNum * topN
        *   输入:kv, 输出:SortMap, 长度topN
        * */
        JavaRDD<SortedMap<Integer, String>> partitions = uniqueKeys.mapPartitions(new FlatMapFunction<Iterator<Tuple2<String,Integer>>, SortedMap<Integer, String>>() {
            public Iterable<SortedMap<Integer, String>> call(Iterator<Tuple2<String, Integer>> iter) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                while (iter.hasNext()) {
                    Tuple2<String, Integer> tuple = iter.next();
                    topN.put(tuple._2, tuple._1);

                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return Collections.singletonList(topN);
            }
        });

        /*
        *   规约所有分区的topN SortMap, 得到最终的SortMap, 长度topN
        *   reduce过后, 数据已经到了本地缓存, 这是最后结果
        *   输入: SortMap, 长度topN, 当然有partitionNum个, 输出:SortMap, 长度topN
        * */
        SortedMap<Integer, String> finalTopN = partitions.reduce(new Function2<SortedMap<Integer, String>, SortedMap<Integer, String>, SortedMap<Integer, String>>() {
            public SortedMap<Integer, String> call(SortedMap<Integer, String> m1, SortedMap<Integer, String> m2) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                for (Map.Entry<Integer, String> entry : m1.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                for (Map.Entry<Integer, String> entry : m2.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return topN;
            }
        });
```



### 5、SparkStreaming消费Kafka中的数据的方式？或者说SparkStreaming与Kafka的集成方式？

#### 一、基于Receiver的方式

这种方式是通过receiver来获取数据，使用的是Kafka的高级API，receiver从kafka中获取的数据都是存储在Spark Executor的内存中（如果数据量突然暴增，大量batch堆积，很容易出现内存溢出问题），然后启动job去处理数据。



然而在这种方式下，很可能会因为底层的失败而丢失数据，如果用启动高可靠机制，让数据零丢失，需要开启预写日志机制（Writer Ahead Log,WAL）。该机制会同步地将接收到Kafka的数据写入到分布式存储系统上的预写日志中（比如HDFS）。这样即使底层节点的失败了，也可以从预写日志中的数据进行恢复。

#### 二、基于Direct的方式

基于Direct的方式会周期性地查询Kafka，来获得每个topic的partition的最新offset，从而定义每个offset的范围。当处理数据的job启动时，会使用Kafka的低级API来获取Kafka指定offset范围的数据。



**优点：**

并行读取：如果要读取多个partition的数据，spark会创建跟kafka partition相同的分区数，并且会并行地从Kafka中读取数据。

高性能：如果要保证数据零丢失，基于Receiver的方式是要开启WAL机制。这种方式是效率低下的，因为数据复制了两份。因为kafka本身就有高可靠机制，他自己会复制一份，WAL又复制了一份。而基于Direct的方式，不依赖于Receiver，不需要开启WAL，只要kafka做了数据的备份，就可以通过kafka的副本进行恢复



#### 两种方式的对比

基于Receiver的方式使用了Kafka的高级API来在Zookeeper中保存消费过offset，配合着WAL机制可以保证数据的高可靠和零丢失。但是无法保证数据被处理一次且仅一次，可能会处理两次。因为spark和Zookeeper之间可能不是同步的



基于Direct的方式使用了Kafka的低级API，spark streaming自己负责追踪offset，并保存checkpoint。spark自己一定是同步的，所以可以保证数据是消费一次且仅一次。

#### 知识补充（Kafka的高级API和低级API）

**高级API**

优点：

1. 简单
2. 不需要管理offset，由Zookeeper管理
3. 不需要管理分区、副本等情况

缺点：

1. 不能控制offset(对于某些特殊需求来说)
2. 不能细化控制分区、副本、Zookeeper



**低级API**

优点：

1. 能够自己控制offset，想从哪里读取数据就哪里读取数据
2. 能够控制分区数，对分区数进行负载均衡
3. 降低了对Zookeeper的依赖

缺点：

1. 太过于复杂，需要自己管理offset，连接哪个分区



### 6、Spark Streaming窗口函数原理

窗口函数就是在原来sparkstreaming的批次的基础上，再次进行封装，每次计算多个批次的数据，同时传入一个滑动步长，用于设置当前任务计算完成后，下次任务开始计算的地方。



![滑动窗口](D:\github\MyKnowledgeRepository\picture\滑动窗口.png)

图中time1就是sparkStreaming计算批次的大小，虚线框以及实线框的大小就是窗口的大小，由虚线框到实线框的距离就是滑动的步长



### 7、Spark Steaming基本原理

接收实时输入数据流，然后将数据拆分成多个batch，比如每一秒的数据封装成一个batch

然后将每个batch交给spark的计算引擎处理，最后会产生一个结果数据流，他也是由一个个batch组成的













