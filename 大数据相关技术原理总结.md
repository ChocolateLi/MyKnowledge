# 大数据相关技术原理总结

## 一、Hadoop

### 1、HDFS 的读流程和写流程

#### HDFS 读数据流程

![HDFS 读数据流程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/HDFS%E8%AF%BB%E6%B5%81%E7%A8%8B.png)

1. 客户端通过Distributed FileSystem 向 NameNode请求下载文件，NameNode查询元数据，找到文件所在DataNode地址，返回给客户端
2. 客户端挑选最近的一台DataNode 服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Package为单位做校验）
4. 客户端以Package为单位接收，现在本地缓存，然后写入目标文件

#### HDFS 写数据流程

![HDFS 写数据流程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/HDFS%E5%86%99%E6%B5%81%E7%A8%8B.png)

1. 客户端通过Distributed FileSystem 向 NameNode 请求上传文件，NameNode 先检查文件是否存在，父目录是否存在
2. NameNode返回是否可以上传文件
3. 客户端请求第一块Block上传到哪几个DataNode服务器上
4. NameNode返回DataNode节点。假设返回3个，分别是dn1、dn2、dn3
5. 客户端通过FSOutputStream模块向dn1请求上传数据，dn1收到请求会继续调用dn2、然后dn2调用dn3，这样通信管道建立完成
6. dn1、dn2、dn3逐级应答客户端
7. 客户端开始往dn1开始上传第一个Block，以Package为单位，dn1收到一个Package就会传给dn2,dn2传给dn3；dn1每传一个Package会放入应答队列等待应答
8. 当一个Block请求上传完成之后，客户端再次请求NameNode上传下一个Block的服务器（重复3-8步骤）



### 2、Shuffle及优化

#### Shuffle过程

![mapreduce的shuffle过程](https://github.com/ChocolateLi/MyKnowledgeRepository/blob/main/picture/mapreduce%E7%9A%84shuffle%E8%BF%87%E7%A8%8B.png)

Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle

过程：

1. MapTask收集我们map()方法输出的kv对，放到环形缓冲区中
2. 当环形缓冲区达到80%时，不断溢出文件到本地磁盘，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程及合并过程中，都要调用Partitioned进行分区和针对key进行排序
5. ReduceTask根据自己的分区号，去各个MapTask机器上取相应的分区数据
6. ReduceTask再次将这些文件进行合（归并排序）
7. 合并成大文件后，shuffle过程结束，开始进入ReduceTask运算逻辑



#### 优化

##### Map阶段

1. 增大环形缓冲区的大小（比如从100M增大到200M）
2. 增大环形缓冲区的溢写比例（比如由80%增大到90%）
3. 减少溢写文件merge次数
4. 不影响业务的情况下，采用Combiner提前合并，减少I/O

##### Reduce阶段

1. 合理设置Map和Reduce数
2. 设置Map和Reduce共存：即Map达到运行到一定时间的时候，Reduce也开始运行，减少Reduce等待时间
3. 规避使用Reduce。Reduce在连接数据集时会产生大量的网络消耗
4. 增加Reduce去拿Map中数据的并行数
5. 增加Reduce端存储数据内存的大小

##### I/O传输

采用数据压缩的方式，减少网络I/O的时间



### 3、Yarn

#### Yarn组成

Yarn由ResourceManaget、NodeManager、ApplicationMaster、Container组成

ResourceManager：负责处理客户端请求，进行资源的分配和调度

NodeManger：处理ResourceManger的命令，管理相应节点上的资源

ApplicationMaster：为应用程序申请资源并且分配任务

Container：资源的抽象，里面封装了CPU、内存等资源

#### Yarn工作机制

![yarn工作机制流程图](D:\github\MyKnowledgeRepository\picture\Yarn工作机制流程.png)

1. client向RM发起任务请求
2. RM返回给client 资源路径信息
3. client根据返回的信息，将程序资源上传到HDFS
4. 程序提交完毕之后申请运行任务
5. RM将请求信息传递给Schedule（资源调度器）；Schedule分配Container用于启动ApplicationMaster；ApplicationManager与指定的NodeManager通信，要求Container中启动ApplicationMaster
8. ApplicationMaster初始化任务，并向RM申请运行任务所需要的资源
9. RM返回相应的资源（即运行任务的NodeManager）
10. ApplicationMaster与对应的NodeManager通信，申请Container启动任务
11. Container中的应用程序会将需要的资源从HDFS中下载到本地，再启动任务
12. 运行过程中，会将运行的状态和进度汇报给ApplicationMaster，client会轮询ApplicationMaster获取状态
13. 运行完成之后，Container会注销掉，并把运行资源还回去；ApplicationMaster向RM注销自己

#### Yarn调度器

FIFO调度器：支持单队列，先进先出

容量调度器：支持多队列，保证先进入的任务优先执行

公平调度器：支持多队列，保证每个任务公平享有队列资源



多队列的好处：

1. 避免某个程序出现死循环，把资源都耗尽
2. 实现任务的降级，特殊时期保证重要任务的队列资源充足



Apache默认的资源调度器是容量调度器

CDH默认的资源调度器是公平调度器



### 4、HDFS感知机架（副本存储节点选择）

**低版本Hadoop副本选择**

第一个副本在client所在的节点上。如果节点在集群外，则随机选择

第二个副本和第一个副本位于不同的机架随机节点上

第三个副本和第二个副本位于相同机架，节点随机选择

![**低版本Hadoop副本选择**](D:\github\MyKnowledgeRepository\picture\低版本Hadoop机架选择.png)



**Hadoop 2.7.2副本节点选择**

第一个副本在client所在的节点上。如果client在集群外，则随机选择。

第二个副本和第一个副本位于相同机架，随机节点

第三个副本位于不同机架上，随机节点

![Hadoop 2.7.2副本节点选择](D:\github\MyKnowledgeRepository\picture\hadoop2.7.2版本的副本选择.png)





## 二、Hive

### 1、Hive架构

![Hive架构](D:\github\MyKnowledgeRepository\picture\Hive架构.png)

1. Hive存储的数据在HDFS上
2. Hive分析数据的底层是MapReduce
3. Hive程序运行是在Yarn上

### 2、Hive和数据库比较

1. 数据存储位置不同。Hive的数据一般存储在HDFS上，数据库的数据一般存储在块设备或者本地文件系统
2. 数据更新不同。Hive是不建议对数据改写的，而数据库的数据是需要经常修改的
3. 数据规模不同。Hive一般处理大规模数据，数据库一般处理小规模数据。

### 3、内部表和外部表

内部表：表删除，数据也删除。即元数据和原始数据都删除

外部表：表删除，数据还在。即元数据删了，但原始数据还在



### 4、4个By的区别

order by：全局排序，只有一个MapReduce

sort by：分区内有序。每个MapReduce内部排序，局部排序，不是全局排序

distribute by：类似于MapReduce的partition，分区，跟sort by结合使用。disttibute by语句要在sort by语句之前

cluster by：当distribute by和sort by字段相同时，可以考虑使用cluster by代替。但他的排序只能是升序排序



### 5、窗口函数

窗口函数可以指定分析函数的数据窗口大小，他和普通的聚合函数不同的是，他不会对结果进行分组，它使得输出中的行数与输入的行数相同。



它的用法是在聚合函数后面跟上over关键字

over(partition by ... order by ...)关键字



Rank函数

row_number():按顺序排序，1 2 3 4

rank():按顺序排序，重复跳过，总数不变 1 2 2 4

dense_rank():按顺序排序，重复不跳过，总数减少 1 2 2 3 



### 6、left join、right join和inner join的区别？

left join（左连接）：返回左表中的所有记录以及右表中的联接字段相等的记录

right join（右连接）：返回右表中的所有记录以及和左表中的联接字段相等的记录

inner join（等值连接）：只返回两个表中连接字段相等的记录



### 7、Hive优化

#### 优化的根本思想

1、尽早过滤数据，减少每个阶段的数据量。数据量少了，减少了网络传输

2、减少job数。所谓的job就是你提交一个复杂的SQL后，hive会把复杂的sql转换成若干个mapreduce的job任务，每个job对应你sql中的部分逻辑，每个job之间任务是独立的。任务是独立的，但数据是依赖的。就是下一个job依赖上一个job的数据文件，上一个job任务执行完会落盘到hdfs上，你job越多，落盘次数也越多，磁盘IO也越多。

3、解决数据倾斜



#### 优化方法

1、列裁剪和分区裁剪。列裁剪就是在查询的时候读取所需要的列，分区裁剪就是只读区所需要的分区。

2、谓词下推。就是将SQL语句中的where谓词逻辑尽可能提前执行，减少下游处理的数据量。

```sql
select a.uid,a.event_type,b.topic_id,b.title
from calendar_record_log a
left outer join (
  select uid,topic_id,title from forum_topic
  where pt_date = 20190224 and length(content) >= 100
) b on a.uid = b.uid
where a.pt_date = 20190224 and status = 0;
```

3、sort by代替order by。order by会讲结果按某字段全局排序，这会导致所有的map端数据都进入一个Reducer中，在数据量大时，可能长时间都计算不出来。使用sort by可能会启动多个reducer进行排序，保证每个reducer内部局部有序。为了控制map端数据分配到reducer中的key，往往还需要配合distribute by一同使用。如果不加distribute by，map端数据就会随机分配到reducer。

```sql
select uid,upload_time,event_type,record_data
from calendar_record_log
where pt_date >= 20190201 and pt_date <= 20190224
distribute by uid
sort by upload_time desc,event_type desc;
```

4、去重优化。尽量避免使用distinct进行去重，特别是大表操作，使用group by代替。

```sql
1、select distinct key from a
如果a表数据特别庞大的话，distinct key很容易造成数据倾斜，就是key一样的话他会放到
同一个mapreduce进行处理

2、select key from a group by key
效果是等价的，但效率比较高。因为group by可以避免数据倾斜
```



**join方面的优化**

1、小表放前大表放后原则。在编写带有join操作的代码语句时，应该将条目少的表放在join操作符的左边。因为Reduce阶段，位于join操作符左边的表的内容会被加载进内存，载入数据量较小的表可以有效减少OOM即内存溢出。

2、mapjoin()。当小表与大表join时，采用mapjoin，即在map端完成。同时可以避免小表与大表join产生的数据倾斜

```sql
select /*+ mapjoin(b)*/ a.key,a.value
from a join b
on a.key=b.key

b是小表
```

3、利用hive的优化机制减少job数。不论是外关联outer join还是内关联inner join，如果join的key相同，不管有多少个表，都会合并为一个MapReduce任务。

```sql
1个job
select a.val,b.val,c.val 
from a join b on (a.key=b.key1) 
join c on (c.key=b.key1)

2个job
select a.val,b.val,c.val 
from a join b on (a.key=b.key1) 
join c on (c.key=b.key2)
```



### 8、Hive的分区和分桶

**Hive分区**

数据表在Hive上存储上是 HDFS文件，就是文件夹的形式。现在最常的跑T+1数据，按当天时间分区比较多。把每天的数据存储在一个区，就是所谓的文件夹与文件。查找数据时只要指定分区字段就可以查找该分区的数据。创建分区表的时候，通过关键字partition by（column name string）声明该表是分区表

**Hive分桶**

分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。对于hive中每一个表、分区都可以进一步分桶，由列的哈希值/桶的个数来决定每条数据划分到哪个桶中。使用clustered by指定分区依据的列名，还要指定分为多少桶

**分区与分桶的区别**

分区和分桶的最大区别是分桶是随机分割数据库的，分区是非随机分割数据库的





### 8、手写HQL

#### 手写HQL 第1题

score表结构：uid,subject_id,score

数据集如下

```
1001 01 90
1001 02 85
...
```

问题：找出所有科目成绩都大于某一学科平均成绩的学生

思路：

1. 求出每个学科平均成绩

   ```sql
   select uid,score,
   avg(score) over(partition by subject_id) avg_score
   from score;t1
   ```

   **注意：** 窗口函数可以返回和输入时一样的行数

2. 根据是否大于平均成绩 记录flag，大于记为0否则记为1

   ```sql
   select uid,if(score>avg_score,0,1) flag from t1;t2
   ```

   

3. 根据学生id进行分组统计flag的和，和为0，说明所有学科都大于平均成绩

   ```sql
   select uid from t2
   group by uid
   having sum(flag)=0;
   ```

最终SQL

```sql
select uid from(
	select uid,if(score>avg_score) flag from(
    	select uid,score,avg(score) over(partition by subject_id) avg_score
        from score
    )t1
)t2
group by uid
having sum(flag)=0;
```

#### 手写HQL 第2题

有如下用户访问数据，action表

```text
userid	visitDate	visitCount
u01		2017/1/21	5
u02		2017/1/23	6
u01		2017/1/23	6
...
```

问题：要求使用SQL统计出每个用户的累积访问次数，如下表

```
userid	月份	  小计	累积
u01		2017-1	11	   11
u01		2017-2	12	   23
...
```

思路：

1. 先修改数据格式

   ```sql
   select userid,
   date_format(regexp_replace(visitDate,'/','-'),'yyyy-MM') mn,
   visitCount
   from action;t1
   ```

2. 计算每人每月访问量

   ```sql
   select userid,mn,sum(visitCount) mn_count
   from t1
   group by userid,mn;t2
   ```

3. 按月累加访问量

   ```sql
   select userid,mn,mn_count,
   sum(mn_count) over(partition by userid order by mn)
   from t2;
   ```

最终SQL

```sql
select userid,mn,mn_count,
sum(mn_count) over(partition by userid order by mn)
from(
	select userid,mn,sum(visitCount) mn_count from(
    	select userid,
		date_format(regexp_replace(visitDate,'/','-'),'yyyy-MM') mn,
		visitCount
		from action
    ) t1
    group by userid,mn
)t2
```

总结：

date_format() 日期格式化函数

regexp_replace() 格式替换函数，针对字符串

sum() over() 可以实现累加求和的效果

group x y：表示将所有具有相同x字段和相同y字段的记录放到一个分组里



#### 手写HQL 第3题（京东面试题）

有50W个京东店铺，每个顾客访问过任何一个店铺的任何一个商品都会产生一条访问日志，访问日志表名为visit，用户id为user_id，店铺名称为shop

```
user_id	shop
u1		a
u2		b
u1		b
...
```

问题：

（1）统计店铺的UV(访客数)

```sql
select shop,count(distinct user_id) from visit group by shop;
```

（2）每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数

思路：

1. 查询每个店铺被每个用户访问的次数

   ```sql
   select shop,user_id,count(*) ct
   from visit
   group by shop,user_id;t1
   ```

   

2. 计算每个店铺被用户访问的排名

   ```sql
   select shop user_id,ct,
   rank() over(partition by shop order by ct desc) rk
   from t1;t2
   ```

   

3. 取每个店铺排名前3

   ```sql
   select shop,user_id,ct
   from t2
   where rk<=3;
   ```

最终SQL

```sql
select shop,user_id,ct from(
	select shop user_id,ct,
	rank() over(partition by shop order by ct desc) rk
	from(
    	select shop,user_id,count(*) ct
		from visit
		group by shop,user_id
    )t1
) t2
where rk<=3
```

总结：

count 和 distinct一起使用，可以实现计算非重复结果的数目

rank() over() 可以实现排序



#### 手写HQL 第4题

已知一个order表，有date,order_id,user_id,amount。请给出sql进行统计。

样例：2017-1-1，2012000，452115005，34

(1)给出2017年每个月的订单数、用户数、成交金额

```sql
select 
    date_format(data,'yyyy-MM'),
    count(order_id),
    count(distinct user_id),
    sum(amount)
from
	order
where 
	date_format(date,'yyyy')='2017'
group by
	date_format(dt,'yyyy-MM')
```

(2)给出2017年11月的新客数（指在11月才有第一笔订单）

```sql
select 
	count(user_id)
from 
	order
group by
	user_id
having
	date_format(min(date),'yyyy-MM')='2017-11'
```

min 函数返回一列中的最小值。NULL 值不包括在计算中。

第一次、第一笔、第一...，一般都要使用min函数

#### 手写HQL 第5题

请用sql写出所有用户中在今年10月份第一次购买商品的金额，表order字段:user_id,money,paymenttime(2017-10-01),orderid

第一步，先查询出10月份第一次

```sql
select
    user_id,
    min(paymenttime) paymenttime
from
	order
where
	date_format(paymenttime,'yyyy-MM')='2017-10'
group by
	user_id;t1
```

第二步，两表join，找到金额

```sql
select 
    t1.user_id
    t1.paymenttime
    od.money
from
	t1
join
	order od
on
	t1.user_id=od.user_id
and
	t1.paymenttime=od.paymenttime
```

 #### 手写SQL 第6题

有一个线上服务器访问日志格式如下

```
时间						接口						IP地址
2016-11-09 14:22:05		/api/user/login			110.03.523
...
```

求11月9号下午 14点（14-15点），访问/api/user/login接口的top10的ip地址

```sql
select 
    ip,
    interface,
    count(*) ct
from
	table
where
	data_format(date,'yyyy-MM-dd HH')>='2016-11-09 14'
	and
    data_format(date,'yyyy-MM-dd HH')<='2016-11-09 15'
    and
    interface='/api/user/login'
group by
	ip,interface
order by
	ct desc
limit 
	10;t1
```

```sql
select ip from t1;
```

#### 手写HQL 第7题

1、用一条SQL语句查询出每门课程都大于80分的学生姓名

```sql
select
	name
from
	table
group by
	name
having
	min(score)>80
```

2、怎样把下面的表

```
year month amount
1991	1	1.1
1991	2	1.2
1991	3	1.3
1991	4	1.4
1992	1	2.1
1992	2	2.2
1992	3	2.3
1992	4	2.4
...
```

 查看成这样一个结果

```
year	m1	m2	m3	m4
1991	1.1	1.2	1.3	1.4
1992	2.1	2.2	2.3	2.4
```

```sql
select
    year,
    (select amount from table a,table b where month=1 and b.year=a.year) as m1,
    (select amount from table a,table b where month=2 and b.year=a.year) as m2,
    (select amount from table a,table b where month=3 and b.year=a.year) as m3,
    (select amount from table a,table b where month=4 and b.year=a.year) as m4,
from 
	table
group by 
	year
```

自连接操作



3、一个info表

```
date	result
2005-05-09	win
2005-05-09	lose
2005-05-09	lose
2005-05-09	win
2005-05-10	win
2005-05-10	lose
2005-05-10	lose
```

```
date 		win lose
2005-05-09	2	2
2005-05-10	1	2
```

```sql
select
    date,
    sum(case when result='win' then 1 else 0) as win,
    sum(case when result='lose' then 1 else 0) as lose,
from
	info
group by
	date;
```



#### 如何对银行卡、身份证、手机号进行脱敏？（腾讯面试题）

concat()、left()、right()字符串函数组合使用

concat(str1,str2,...)：返回结果为连接参数产生的字符串

left(str,len)：返回从字符串str开始的len最左字符

right(str,len)：返回从字符串str开始的len最右字符

```sql
SELECT 
    CONCAT(LEFT(IdentityCardNo,3), '****' ,RIGHT(IdentityCardNo,4)) AS 身份证号
FROM c_inhabitantinfo;
```



#### 查询一个表（tb1）的字段记录不在另一个表（tb2）中 （万物心选面试题）

```sql
select tb1.* from tb1
left join tb2
on tb1.id=tb2.id
where tb2.id is null;
```

```sql
select tb1.* from tbl
where tb1.id not in (
	select tb2.id from tb2
)
```



## 三、HBase

### 1、HBase逻辑结构和物理存储结构

**逻辑结构**

不同的列族放在不同文件夹存储的

row_key是有序的，按字典序

Region是一张表的切片，而且是横向切分的

store真正放在HDFS上存储的东西

![HBase逻辑结构](D:\github\MyKnowledgeRepository\picture\HBase逻辑结构.png)



**物理结构**

HBase实现随机读写操作完全靠时间戳

![物理结构](D:\github\MyKnowledgeRepository\picture\HBase物理存储结构.png)



还有一种数据结构cell

cell：由{row_key,column Family,column,TimeStamp}唯一确定单元。cell中数据是没有类型的，全部是字节码形式存储。



### 2、HBase架构图（同时也是它的存储结构）

![HBase架构图](D:\github\MyKnowledgeRepository\picture\HBase架构图.png)



Master：管理RegionServer，处理Region的分配或转移

RegionServer：负责存储HBase实际的数据

Store：一个Store对应HBase表中的一个列族

Mem Store：内存存储，用来保存当前数据的操作

HFile：实际存储的物理文件。StoreFile是以HFile的形式存储在HDFS上

HLog：WAL,预写入日志机制



### 3、HBase原理

#### 读流程

1. client向Zookeeper请求meta表所在的位置regionserver
2. Zookeeper查询并返回mata表位置给client
3. client向请求meta表所在的服务器regionserver
4. regeionserver返回meta表数据
5. client向regionserver发起读请求
6. regionserver查询MemStore和StoreFile的数据，并返回给client

![HBase读流程](D:\github\MyKnowledgeRepository\picture\HBase读流程.png)



#### 写流程

1. client向Zookeeper请求meta表所在的位置regionserver
2. Zookeeper查询并返回mata表位置给client
3. client向请求meta表所在的服务器regionserver
4. regeionserver返回meta表数据
5. client向regionserver发起写请求
6. regionserver将数据写入到HLog，为了数据的持久化和恢复
7. regionserver将数据写入到内存Mem Store
8. 反馈client写入成功

![HBase写流程](D:\github\MyKnowledgeRepository\picture\HBase写流程.png)



#### flush

1. 当MemStore数据达到一定阈值时，会把MemStore的数据flush到StoreFile中，将内存中的数据删除，同时删除HLog中的历史数据
2. 数据是存储到HDFS上

![flush](D:\github\MyKnowledgeRepository\picture\MemSore Flush过程.png)



#### compact

当StoreFile越来越多，会触发compact合并操作，把多个StoreFile合并成一个大的StoreFile

![compact](D:\github\MyKnowledgeRepository\picture\compact过程.png)



#### Split

当StoreFile越来越大，Region也越来越大，达到一定阈值，会触发Split操作，将Region一分为二



#### 4、Region预分区

预分区的目的主要是创建表的时候指定分区数，提前规划好表有多少个分区数，以及每个分区的区间范围，这样在存储的时候，rowkey会按照分区的区间存储，可以避免region热点问题



#### 5、RowKey设计原则

唯一性。确保每一条数据唯一

散列性。将rowkey均匀地散列在各个区间。所以使用散列性需要设计好预分区

长度原则。rowkey是存储到内存中的，如果rowkey太大的话，会占用大量的内存空间。

#### 6、RowKey如何设计

设计RowKey的主要目的是让数据均匀地分布在所有的region分区中，在一定程度上防止数据倾斜。

1. 通过随机数、hash、散列值(UUID)的方式将key打散
2. 字符串反转。时间戳是有规律的，但反过来就没规律了。还有手机号

#### 7、Phoenix二级索引的原理

Phoenix是一个框架。这个框架可以使用SQL语句的形式去操作HBase。

它的底层原理就是会把我们写的SQL语句翻译成HBase的指令，再用指令去操作HBase分布式集群。



HBase一级索引默认是开启的，我们程序员是不能干预的。









## 四、Zookeeper

### 1、选举机制

1. 半数机制：集群中半数以上机器存活，所以Zookeeper适合安装奇数台服务器

2. Zookeeper虽然没有在配置文件中指定Master和Slave，但Zookeeper工作的时候，是有一个Leader节点，其他是Follower节点，Leader是通过内部选举产生的

3. 选举过程：

   假设有五台机器，它们的id分别是1-5,同时他们是最新启动的，没有历史数据。

   机器是依次启动的，id1先启动，但目前只有一台服务器，他发出去的报文没人响应，所以它的选举状态是looking状态；

   id2启动，与id1进行通信，交换自己的选举结果。由于双方都没有历史数据数据，id值较大的会胜出，因此id2胜出。但由于没有超过半数以上机器同意选举它，所以id1和id2的选举状态都保持looking状态；

   id3启动，根据前面的理论，id3胜出，此时有超过半数以上机器选举它，所以id3当选为leader
   
   接下来的id4、id5启动，但由于前面的机器已经选择了id3，所以id4和id5当Follwer

### 2、什么是CAP法则？Zookeeper符合哪两个？

CAP法则是指在分布式系统中，一致性、可用性、分区容错性三者不可兼得

分区容错是指区间通信可能失败，系统不能在时限内保持数据一致性。

一致性是指在分布式所有数据备份中，在同一时刻是否同样的值。

可用性是指集群一部分节点出现故障后，集群整体是否还能响应客户端的读写请求



Zookeeper符合强一致性和高可用性！



### 3、Paxos算法

Paxos算法是一种基于消息传递的分布式一致性算法



它要解决的问题是：一个分布式系统就某个值(也就是协议)达成一致的问题

一个典型的场景就是分布式数据库系统中，如果各节点初始状态一致，每个节点执行相同的操作系列，那么最后他们会得到一个一致的状态。



但它有一个前提条件：通信是保证可靠的不会被篡改的



Paxos算法大致流程（大白话讲）：

在整个提议和投票过程中，最重要的就是“提议者”和“接受者”

第一阶段：因为存在多个“提议者”,如果都提意见，那么“接受者”就很为难。所以要先明确哪个“提议者”是意见领袖，有权提出提议。“接受者”就要处理这个提议

第二个阶段：由上述意见领袖提出提议，“接收者”反馈意见，如果多数“接收者”接受了这个提议，那么提议就通过了。



怎么明确意见领袖？

通过编号。每个“提议者”在第一阶段先报个号，谁的号大谁就是意见领袖。



## 五、Kafka

### 1、Kafka的架构

![Kafka的架构](D:\github\MyKnowledgeRepository\picture\Kafka架构.png)

组成：生产者、Broker(Kafka服务器)、消费者、Zookeeper

注意：Zookeeper只保存了Broker的id信息，以及消费者的offset信息；没有生产者的信息



Consumer：消息生产者，向Kafka发送消息的客户端

Consumer Group：这是Kafka实现广播和单播的手段。所谓的广播就是把消息发给所有的Comsumer，单播就是发给任意一个Consumer。

Broker：一台Kafka服务器就是一个Broker。一个集群由多个Broker组成

Topic：主题。可以理解为一个队列，里面存储相应信息的内容。一个Broker可以拥有多个Topic

Partition：分区。为了实现扩展性，一个非常大的Topic可以分布在多个Broker上，一个Topic可以有多个	Partition，每个Partition都是一个有序的队列。（一个Partition只能被一个消费者组的一个消费者消费，但它可以同时被不同消费者组的消费者消费）

Offset：偏移量。Partition里面的每条消息，都会分配一个有序的id，这个id就是Offset。他记录着你要消费的位置



### 2、Kafka的作用（为什么使用Kafka？即为什么使用消息队列？消息队列的作用是什么？）

解耦和扩展性：将生产者和消费者解耦开来，可以独立地扩展他们相应的功能

缓冲和削峰：如果上游数据突然暴增，下游可能扛不住。所有可以通过消息队列进行缓冲，下游队列可以慢慢从消息队列取数据处理。

异步处理：有时候并不需要立即处理数据，可以通过消息队列的异步处理机制，它允许用户把消息放入消息队列里，但不立即处理它。



### 3、ISR副本同步队列

一般副本设置2个或3个

副本的优势：提高可靠性

副本的劣势：增加网络IO



副本操作是以分区为单位的，每个副本都有自己的主副本和从副本。

主副本叫Leader，从副本叫Follower。处于同步状态下的副本叫 ISR（in-Sync-Replicas）



生产者和消费者都是和Leader交互读写数据，不和Follower交互。



如果Leader挂了，会从副本同步队列中选择一个Follower作为新的Leader



### 4、Kafka的数据不丢失机制

从生产者、消费者、Broker三方面回答。

1. 生产者数据的不丢失

   ACK机制：当Kafka发送消息的时候，都会有一个消息确认反馈机制，确保消息是否正常收到，有0，1，-1三种状态。

   ack=0，异步发送，消息发送完，立即发送下一条消息

   ack=1，Producer等待Leader确认收到数据，才发送下一条消息

   ack=-1，Producer等到Follower确认，才发送下一条消息

   

2. 消费者数据的不丢失

   通过Offset来保证每次消费的记录

   

3. Broker数据不丢失

   通过备份副本，来保证数据不丢失

   

### 5、Kafka数据是放在内存上还是磁盘上，为什么速度那么快？

放在磁盘上。

速度快的原因：

1. Kafka本身就是分布式集群，同时采用了分区技术，并发度高
2. 顺序写入磁盘。硬盘是机械结构，每次读写的操作是“寻址->写入”。寻址的过程中是很耗时的。为了提高读写速度，它采用了顺序写入。



### 6、Kafka消费过的消息如何再消费？

得从Offset下手，因为它记录着消费者的消费位置。而Offset一般是定义在Zookeeper上，如果要重复消费，需要重设Zookeeper上的Offset。可以通过Redis记录Offset的checkpoint点，当想重复消费的时候，读取Redis上的checkpoint点进行Zookeeper上Offset的重设。



### 7、Kafka数据重复怎么处理？

可以到下一级处理：Spark Streaming，Redis，Hive中去重



### 8、Kafka消息数据积压，Kafka消费能力不足怎么处理？

1. 可以增加Topic的分区数，增加消费者组的消费者数，分区数=消费者数（二者缺一不可）
2. 如果是下游数据处理不及时，可以增加每次批次拉取的数量。批次拉取的数据量太少，导致拉取数据+处理数据的速度< 生产数据的速度，处理数据小于生产数据，也会导致数据积压。



## 六、Spark

### 1、基于Yarn的Spark架构与作业提交流程

#### Yarn-Client

![Yarn-Client的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClient的Spark架构启动流程.png)

1. spark_submit脚本提交程序，在本地机器上启动driver
2. driver程序向ResourceManager申请启动ApplicationMaster
3. ResourceManager通知一个NodeManager去启动ApplicationMaster
4. ApplicationMaster启动完成之后向ResourceManager申请运行Executor
5. ResourceManager分配Container返回给ApplicationMaster
6. ApplicationMaster与对应的NodeManger通信，申请和启动Container
7. NodeManager启动Executor进程
8. Executor启动完毕后反向注册driver



#### Yarn-Cluster

![YarnCluster的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClusterd的Spark架构流程.png)



1. 本地机器提交Spark Application到ResourceManager
2. ResourceManager找到一个NodeManager启动ApplicationMaster
3. NodeManager启动ApplicationMaster并启动相应的Driver进程
4. ApplicationMaster向ResouceManager申请启动Executor
5. ResourceManager分配container返回给ApplicationMaster
6. ApplicationMaster与相应的NodeManager通信，申请启动exetutor
7. NodeManager启动executor进程
8. Executor进程启动完毕之后找ApplicationMaster进行反向注册



#### yarn-client和yarn-cluster模式的不同之处

YarnClient模式下，driver运行在本地机器

YarnCluster模式下，driver运行在集群某个NodeManager的节点上面



### 2、Spark的两种核心Shuffle

#### HashShuffle

##### 未优化的HashShuffle

![未优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\未优化的HashShuffle.png)

1. 第一个Stage，每个task都会创建下一个Stage的task数量相同的文件
2. 第二个Stage，每个task会到各个节点上面，拉取第一个stage上每个task的属于自己的输出文件



##### 优化的HashShuffle（开启map端文件合并）

![优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\优化后的HashShuffle.png)

1. 第一个Stage，每个task依旧会创建下一个Stage，task数量相同的输出文件
2. 但是下一个task，直接复用上一个task的输出文件
3. 这样，第二个Stage，就会拉取少量的输出文件

#### SortShuffle

![SortShuffle](D:\github\MyKnowledgeRepository\picture\SortShuffle.png)



![bypass SortShuffle](D:\github\MyKnowledgeRepository\picture\bypass SortShuffle.png)

在sortShuffle中可以设置一个阈值，默认是200;当reduce task数量少于等于200时，即map task创建的文件少于等于200时，不会进行排序，依旧会输出reduce task的数量，最后将所有的数据文件合并成一个大文件



#### HashShuffle和SortShuffle的区别

1. SortShuffle会对每个reduce task要处理的数据进行排序
2. SortShuffle只会写入一个磁盘文件，不同reduce task的任务通过offset来界定



### 3、Spark共享变量

#### 共享变量原理

默认情况下，如果在某个算子函数中使用到了某个外部变量，那么这个变量的值会拷贝到每个task中。每个task只能操作自己的那份变量副本。如果多个task想要共享变量，这种方式是做不到的。



所以Spark提供了两种共享变量：广播变量和累加器



广播变量，仅仅会为每个节点(即Executor)拷贝一份，减少了网络传输和内存消耗

累加器可以让多个task共同操作一份变量，主要进行累加操作

#### 广播变量

广播变量是只读，它只会为每个节点拷贝一份副本，而不是为每个task拷贝一份副本。

它最大的好处就是减少了网络传输和内存消耗

#### 累加器

累加器主要用于多个节点对一个变量进行共享性操作。

Accumulator只提供给了累计操作。提供了多个task对一个变量共享的操作

但是task只能对Accumulator进行累加操作，不能读取

只有Driver程序可以读取Accumulator的值



### 4、Spark实现TopN的获取

算法思路：

1. 读入数据
2. 将数据通过map映射成KV格式的数据
3. 按照key对数据进行聚合（比如使用reduceByKey、groupBykey等）
4. 计算各个分区的TopN，使用SortMap来实现。也就是TreeMap，它实现了SortMap的接口
5. 规约所有的TopN的SortMap，得到最终的ToN SortMap



```java
/*
    *   程序入口函数
    * */
public void run() {
        /*
        *   读入inputPath中的数据
        * */
        JavaRDD<String> lines = jsc.textFile(inputPath, 1);

        /*
        *   将rdd规约到9个分区
        * */
        JavaRDD<String> rdd = lines.coalesce(9);

        /*
        *   将输入转化为kv格式
        *   key是规约的主键, value是排序参考的个数
        *   注: 这里的key并不唯一, 即相同的key可能有多条记录, 所以下面我们规约key成唯一键
        *   输入:line, 输出:kv
        * */
        JavaPairRDD<String, Integer> kv = rdd.mapToPair(
            new PairFunction<String, String, Integer>() {
            public Tuple2<String, Integer> call(String s) throws Exception {
                String[] tokens = s.split(",");
                return new Tuple2<String, Integer>(tokens[0], Integer.parseInt(tokens[1]));
            }
        });

        /*
        *   规约主键成为唯一键
        *   输入:kv, 输出:kv
        * */
        JavaPairRDD<String, Integer> uniqueKeys = kv.reduceByKey(new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer i1, Integer i2) throws Exception {
                return i1 + i2;
            }
        });

        /*
        *   计算各个分区的topN
        *   这里通过广播变量拿到了topN具体个数, 每个分区都保留topN, 所有分区总个数: partitionNum * topN
        *   输入:kv, 输出:SortMap, 长度topN
        * */
        JavaRDD<SortedMap<Integer, String>> partitions = uniqueKeys.mapPartitions(new FlatMapFunction<Iterator<Tuple2<String,Integer>>, SortedMap<Integer, String>>() {
            public Iterable<SortedMap<Integer, String>> call(Iterator<Tuple2<String, Integer>> iter) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                while (iter.hasNext()) {
                    Tuple2<String, Integer> tuple = iter.next();
                    topN.put(tuple._2, tuple._1);

                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return Collections.singletonList(topN);
            }
        });

        /*
        *   规约所有分区的topN SortMap, 得到最终的SortMap, 长度topN
        *   reduce过后, 数据已经到了本地缓存, 这是最后结果
        *   输入: SortMap, 长度topN, 当然有partitionNum个, 输出:SortMap, 长度topN
        * */
        SortedMap<Integer, String> finalTopN = partitions.reduce(new Function2<SortedMap<Integer, String>, SortedMap<Integer, String>, SortedMap<Integer, String>>() {
            public SortedMap<Integer, String> call(SortedMap<Integer, String> m1, SortedMap<Integer, String> m2) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                for (Map.Entry<Integer, String> entry : m1.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                for (Map.Entry<Integer, String> entry : m2.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return topN;
            }
        });
```



### 5、SparkStreaming消费Kafka中的数据的方式？或者说SparkStreaming与Kafka的集成方式？

#### 一、基于Receiver的方式

这种方式是通过receiver来获取数据，使用的是Kafka的高级API，receiver从kafka中获取的数据都是存储在Spark Executor的内存中（如果数据量突然暴增，大量batch堆积，很容易出现内存溢出问题），然后启动job去处理数据。



然而在这种方式下，很可能会因为底层的失败而丢失数据，如果用启动高可靠机制，让数据零丢失，需要开启预写日志机制（Writer Ahead Log,WAL）。该机制会同步地将接收到Kafka的数据写入到分布式存储系统上的预写日志中（比如HDFS）。这样即使底层节点的失败了，也可以从预写日志中的数据进行恢复。

#### 二、基于Direct的方式

基于Direct的方式会周期性地查询Kafka，来获得每个topic的partition的最新offset，从而定义每个offset的范围。当处理数据的job启动时，会使用Kafka的低级API来获取Kafka指定offset范围的数据。



**优点：**

并行读取：如果要读取多个partition的数据，spark会创建跟kafka partition相同的分区数，并且会并行地从Kafka中读取数据。

高性能：如果要保证数据零丢失，基于Receiver的方式是要开启WAL机制。这种方式是效率低下的，因为数据复制了两份。因为kafka本身就有高可靠机制，他自己会复制一份，WAL又复制了一份。而基于Direct的方式，不依赖于Receiver，不需要开启WAL，只要kafka做了数据的备份，就可以通过kafka的副本进行恢复



#### 两种方式的对比

基于Receiver的方式使用了Kafka的高级API来在Zookeeper中保存消费过offset，配合着WAL机制可以保证数据的高可靠和零丢失。但是无法保证数据被处理一次且仅一次，可能会处理两次。因为spark和Zookeeper之间可能不是同步的



基于Direct的方式使用了Kafka的低级API，spark streaming自己负责追踪offset，并保存checkpoint。spark自己一定是同步的，所以可以保证数据是消费一次且仅一次。

#### 知识补充（Kafka的高级API和低级API）

**高级API**

优点：

1. 简单
2. 不需要管理offset，由Zookeeper管理
3. 不需要管理分区、副本等情况

缺点：

1. 不能控制offset(对于某些特殊需求来说)
2. 不能细化控制分区、副本、Zookeeper



**低级API**

优点：

1. 能够自己控制offset，想从哪里读取数据就哪里读取数据
2. 能够控制分区数，对分区数进行负载均衡
3. 降低了对Zookeeper的依赖

缺点：

1. 太过于复杂，需要自己管理offset，连接哪个分区



### 6、Spark Streaming窗口函数原理

窗口函数就是在原来sparkstreaming的批次的基础上，再次进行封装，每次计算多个批次的数据，同时传入一个滑动步长，用于设置当前任务计算完成后，下次任务开始计算的地方。



![滑动窗口](D:\github\MyKnowledgeRepository\picture\滑动窗口.png)

图中time1就是sparkStreaming计算批次的大小，虚线框以及实线框的大小就是窗口的大小，由虚线框到实线框的距离就是滑动的步长



### 7、Spark Steaming基本原理

Spark Streaming是Spark Code API的一种扩展，它支持从多种数据源读取数据，比如kafka、flume。

它的基本原理是：

接收实时输入数据流，然后将数据拆分成多个batch，比如每一秒的数据封装成一个batch

然后将每个batch交给spark的计算引擎处理，最后会产生一个结果数据流，他也是由一个个batch组成的



### 8、Hadoop和Spark应用场景

Hadoop底层使用MapReduce计算架构，只有map和reduce两种操作，表达能力比较欠缺，而且在MR过程中会重复的读写hdfs，造成大量的磁盘io读写操作，所以适合高时延环境下批处理计算的应用；

Spark是基于内存的分布式计算架构，数据分析更加快速，所以适合低时延环境下计算的应用；

同时spark更适用于机器学习之类的迭代式应用。spark与hadoop最大的区别在于迭代式计算模型。



### 9、RDD持久化原理？

调用cache()或者persist()方法即可。cache的底层就是persist()无参数版本。persist(MEMORY_ONLY)，将数据持久化到内存中



### 10、checkpoint检查点机制

应用场景：当spark应用程序特别复杂，从初始的RDD到最后整个应用的完成有很多步骤，而且应用运行时间特别长，适合使用checkpoint功能。



原因：对于特别复杂spark应用，会出现某个反复使用的RDD，即使之前持久化过，但由于节点故障导致数据丢失，没有容错机制，又需要重新计算一次数据。



实现原理：首先找到stage最后的finalRDD，然后按照RDD依赖关系往回找，找到使用checkpoint的RDD,然后标记这个RDD，再重新启动一个线程将checkpoint之前的RDD缓存到HDFS中，最后RDD依赖关系从checkpoint的位置切断。



### 11、checkpoint和持久化机制的区别？

最主要的区别是持久化只是将数据保存在BlockManager中，RDD的血缘关系不变的。但是checkpoint执行之后，RDD没有所谓的依赖RDD了。

持久化丢失数据的可能性更大，因为节点故障可能会导致磁盘内存数据的丢失，但是checkpoint数据通常保存在高可用的文件系统中，比如HDFS，所以数据丢失可能性比较低。



### 12、DStream以及基本工作原理

DStream是 spark streaming提供的一种高级抽象，代表一个持续不断的数据流。

DStream由一组时间序列上连续的RDD表示，每个RDD只包含一段时间的数据。

Spark Streaming一定是有一个输入的DStream接收数据，按照时间划分成一个一个batch，并转化为RDD，RDD数据分散在各个子节点的partition中。



### 13、Spark工作机制

1. 用户提交程序(Application)创建SparkContext实例，SparkContext根据RDD对象生成DAG图，将作业(Job)提交给DAGScheduler
2. DAGScheduler将作业(Job)划分成不同的Stage(从末端RDD开始，根据shuffle来划分)，每个Stage都是任务的集合(TaskSet)，以TaskSet为单位提交给TaskScheduler
3. TaskScheduler管理任务(Task)，并通过资源管理器(Cluster Manager)把任务(task)发给集群中的Worker的Executor
4. Worker接收到任务(Task)，启动Executor进程中的线程Task来执行任务



### 14、Spark如何保证宕机迅速恢复？

1. 增加standby master
2. 编写shell脚本，定期检查maste状态，出现宕机后对master进行重启操作



### 15、Spark主备切换原理知道吗？

Master实际上是可以配置两个的，Spark原生的Standalone模式Master是支持主备切换的。当Active Master挂掉之后，我们可以将Standby Master切换为Active Master。

Spark Master主备切换可以基于两种机制，一种是基于文件系统，一种是基于Zookeeper。

基于文件系统的，需要在Active Master挂掉之后手动切换standby Master上。

基于Zookeeper的，可以实现自动的切换。



### 16、RDD中reduceBykey与groupByKey哪个性能好，为什么

reduceByKey会在结果发送给reducer之前会在本地mapper进行聚合，类似于MapReduce中的combiner。这样做的好处在于map端进行一次reduce之后，数据量会大幅度减小，从而减小传输，保证reduce端更快地计算出结果。



groupByKey会对每个RDD中value进行聚合形成序列(iterator)，此操作发生在reduce端，会造成大量的网络传输，如果数据量非常大，可能会形成OutOfMemoryError。



所以在进行大量数据的reduce操作时候建议使用reduceByKey。不仅可以提高速度，还可以防止使用groupByKey造成的内存溢出问题。



### 17、Spark master HA主从切换过程会影响到集群已有作业的运行？为什么

不会。因为程序在运行之前已经申请过资源了，driver和executors通讯，不需要和master进行通讯



### 18、 spark master使用zookeeper进行ha，有哪些源数据保存到Zookeeper里面

spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置。

要包括Application、driver、worker和executors.

standby节点要从zk中，获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的。



### 19、谈谈你对RDD的理解

RDD是spark的最基本数据抽象，它是弹性分布式数据集，代表的是一个不可变、可分区、可并行计算机的集合。

它的作用是：提供了一组抽象的数据模型，将具体的应用逻辑表达成一些列的转换操作。

不同的RDD之间的转换操作会形成依赖关系，进而实现管道化，大大降低数据的复制、磁盘I/O。

RDD的依赖分为宽依赖和窄依赖，用来解决数据容错时的高效性以及划分任务时起动重要作用。



### 20、你是怎么理解Spark的，Spark的特点有哪些？

spark是基于内存的，用于处理大规模数据的分析引擎。

它的组成模块有spark core、spark sql、spark streaming、sparkMLlib、spark graphx

它的特点是：

快：比mapreduce计算速度快

易用：它支持多种计算模型

通用：能够进行离线计算、交互式计算、实时计算等

兼容性：支持yarn的调度，可以处理hadoop的数据



### 21、Spark的几种部署方式

Local：运行在一台机器上

Standalone：构建master + slave的spark集群

yarn：spark客户端直接连接yarn，不需要额外的构建集群

mesos：国内大环境比较少用



### 22、Spark提交作业的参数

腾讯考查了

```
executor-cores —— 每个executor使用的内核数
num-executors —— 启动executors的数量，默认为2
executor-memory —— executor内存大小，默认1G
driver-cores —— driver使用内核数，默认为1
driver-memory —— driver内存大小
```



### 23、简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数？

窄依赖：父RDD的一个分区只会被子RDD的一个分区依赖

宽依赖：父RDD的一个分区会被子RDD的多个RDD依赖



每遇到一个宽依赖则划分为一个stage



stage是一个taskset，stage根据分区数划分成一个个的task



### 24、Repartition和Coalesce 的关系与区别

关系：两者都是改变RDD的partition数量，repartition底层调用了coalesce的方法，coalesce(numPartitions,shuffle=true)



区别：repartition一定会发生shuffle，coalesce根据传入的参数是否发生shuffle

一般情况下，使用repartition增大rdd的分区数，使用coalesce减少rdd的分区数



### 25、当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？

 使用foreachPartition代替foreach，在foreachPartition内获取数据库的连接。  



### 26、Spark读取Kafka数据丢失？

spark streaming读取kafka数据丢失是很大的问题，spark streaming可以通过direct方式读取kafka，提供了checkpoint方式自己去维护和读取kafka的offset，将数据放到HDFS上。



Receiver的方式要确保数据零丢失，需要开启预写入日志机制，同步保存所有kafka的数据到分布式文件系统上，以便发生故障时恢复所有数据。



















