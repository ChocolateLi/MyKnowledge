# 大数据相关技术原理总结

## 一、Hadoop

### 1、HDFS 的读流程和写流程

#### HDFS 读数据流程

![HDFS 读数据流程](D:\github\MyKnowledgeRepository\picture\HDFS读流程.png)

1. 客户端通过Distributed FileSystem 向 NameNode请求下载文件，NameNode查询元数据，找到文件所在DataNode地址，返回给客户端
2. 客户端挑选最近的一台DataNode 服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Package为单位做校验）
4. 客户端以Package为单位接收，现在本地缓存，然后写入目标文件

#### HDFS 写数据流程

![HDFS 写数据流程](D:\github\MyKnowledgeRepository\picture\HDFS写流程.png)

1. 客户端通过Distributed FileSystem 向 NameNode 请求上传文件，NameNode 先检查文件是否存在，父目录是否存在
2. NameNode返回是否可以上传文件
3. 客户端请求第一块Block上传到哪几个DataNode服务器上
4. NameNode返回DataNode节点。假设返回3个，分别是dn1、dn2、dn3
5. 客户端通过FSOutputStream模块向dn1请求上传数据，dn1收到请求会继续调用dn2、然后dn2调用dn3，这样通信管道建立完成
6. dn1、dn2、dn3逐级应答客户端
7. 客户端开始往dn1开始上传第一个Block，以Package为单位，dn1收到一个Package就会传给dn2,dn2传给dn3；dn1每传一个Package会放入应答队列等待应答
8. 当一个Block请求上传完成之后，客户端再次请求NameNode上传下一个Block的服务器（重复3-8步骤）



### 2、Shuffle及优化

#### Shuffle过程

![mapreduce的shuffle过程](D:\github\MyKnowledgeRepository\picture\mapreduce的shuffle过程.png)

Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle

过程：

1. MapTask收集我们map()方法输出的kv对，放到环形缓冲区中
2. 当环形缓冲区达到80%时，不断溢出文件到本地磁盘，可能会溢出多个文件
3. 多个溢出文件会被合并成大的溢出文件
4. 在溢出过程及合并过程中，都要调用Partitioned进行分区和针对key进行排序
5. ReduceTask根据自己的分区号，去各个MapTask机器上取相应的分区数据
6. ReduceTask再次将这些文件进行合（归并排序）
7. 合并成大文件后，shuffle过程结束，开始进入ReduceTask运算逻辑



#### 优化

##### Map阶段

1. 增大环形缓冲区的大小（比如从100M增大到200M）
2. 增大环形缓冲区的溢写比例（比如由80%增大到90%）
4. 不影响业务的情况下，采用Combiner提前合并，减少I/O

##### Reduce阶段

1. 合理设置Map和Reduce数
2. 设置Map和Reduce共存：即Map达到运行到一定时间的时候，Reduce也开始运行，减少Reduce等待时间
3. 规避使用Reduce。Reduce在连接数据集时会产生大量的网络消耗
5. 增加Reduce端存储数据内存的大小

##### I/O传输

采用数据压缩的方式，减少网络I/O的时间



### 3、Yarn

#### Yarn组成

Yarn由ResourceManaget、NodeManager、ApplicationMaster、Container组成

ResourceManager：负责处理客户端请求，进行资源的分配和调度

NodeManger：处理ResourceManger的命令，管理相应节点上的资源

ApplicationMaster：为应用程序申请资源并且分配任务

Container：资源的抽象，里面封装了CPU、内存等资源

#### Yarn工作机制

![yarn工作机制流程图](D:\github\MyKnowledgeRepository\picture\Yarn工作机制流程.png)

1. client向RM发起任务请求
2. RM返回给client 资源路径信息
3. client根据返回的信息，将程序资源上传到HDFS
4. 程序提交完毕之后申请运行任务
5. RM将请求信息传递给Schedule（资源调度器）；Schedule分配Container用于启动ApplicationMaster；
8. ApplicationMaster初始化任务，并向RM申请运行任务所需要的资源
9. RM返回相应的资源（即运行任务的NodeManager）
10. ApplicationMaster与对应的NodeManager通信，申请Container启动任务
11. Container中的应用程序会将需要的资源从HDFS中下载到本地，再启动任务
12. 运行过程中，会将运行的状态和进度汇报给ApplicationMaster，client会轮询ApplicationMaster获取状态
13. 运行完成之后，Container会注销掉，并把运行资源还回去；ApplicationMaster向RM注销自己

#### Yarn调度器

FIFO调度器：支持单队列，先进先出

容量调度器：支持多队列，保证先进入的任务优先执行

公平调度器：支持多队列，保证每个任务公平享有队列资源



多队列的好处：

1. 避免某个程序出现死循环，把资源都耗尽
2. 实现任务的降级，特殊时期保证重要任务的队列资源充足



Apache默认的资源调度器是容量调度器

CDH默认的资源调度器是公平调度器



### 4、HDFS感知机架（副本存储节点选择）

**低版本Hadoop副本选择**

第一个副本在client所在的节点上。如果节点在集群外，则随机选择

第二个副本和第一个副本位于不同的机架随机节点上

第三个副本和第二个副本位于相同机架，节点随机选择

![低版本Hadoop副本选择](D:\github\MyKnowledgeRepository\picture\低版本Hadoop机架选择.png)



**Hadoop 2.7.2副本节点选择**

第一个副本在client所在的节点上。如果client在集群外，则随机选择。

第二个副本和第一个副本位于相同机架，随机节点

第三个副本位于不同机架上，随机节点

![Hadoop 2.7.2副本节点选择](D:\github\MyKnowledgeRepository\picture\hadoop2.7.2版本的副本选择.png)



### 5、MapReduce的执行过程

![MapReduce的执行过程](D:\github\MyKnowledgeRepository\internship_picture\mapreduce执行过程.png)

MapReduce 处理数据过程主要分成 **Map** 和 **Reduce** 两个阶段。首先执行 Map 阶段，再执行 Reduce 阶段

1. 在正式执行 Map 前，需要将输入数据进行 **分片**。所谓分片，就是将输入数据切分为大小相等的数据块，每一块作为单个 Map Task 的输入被处理，以便于多个 Map Task 同时工作。
2. 分片完毕后，多个 Map Task 便可同时工作。每个 Map Task 在读入各自的数据后，进行计算处理，最终输出给 Reduce。Map Task 在输出数据时，需要为每一条输出数据指定一个 Key，这个 Key 值决定了这条数据将会被发送给哪一个 Reduce Task。**Key 值和 Reduce Task 是多对一的关系**，具有相同 Key 的数据会被发送给同一个 Reduce Task，单个 Reduce Task 有可能会接收到多个 Key 值的数据。
3. 在进入 Reduce 阶段之前，MapReduce 框架会对数据按照 Key 值**排序**，使得具有相同 Key 的数据彼此相邻。如果您指定了 **合并操作（Combiner）**，框架会调用 Combiner，它负责对中间过程的输出具有相同 Key 的数据进行本地的聚集，这会有助于降低从Mapper到 Reducer数据传输量。这部分的处理通常也叫做 **洗牌（Shuffle）**。
4. 接下来进入 Reduce 阶段。相同 Key 的数据会到达同一个 Reduce Task。同一个 Reduce Task 会接收来自多个 Map Task 的数据。每个 Reduce Task 会对 Key 相同的多个数据进行 Reduce 操作。最后，一个 Key 的多条数据经过 Reduce 的作用后，将变成一个值。



总结说明：

- map的输出是（key, value）对
- 将某一个map的输出的相同的key的value合并，即**combine**过程
- **决定这个map的结果给哪一个reduce**：通过**hash(key)mod(reduce数目)** 计算出partition的ID，上述的key就被分配给这个partition；**一个partition中可以有多个key，但是同一个key只存在于一个partition中；一个partition对应一个reduce**
- map个数和分片个数有关

```
Map数 = Split数 = FileInputFormat.getSplits()

可以通过JobConf的设置
conf.setNumMapTasks(int num)方法手动设置
```

- Reduce数量跟partition相同

```
Reducer的个数是由用户独立设置的，在默认情况下只有一个Reducer。 它的个数既可以使用命令行参数设置（mapreduce.job.reduces=number），也可以在程序中制定（job.setNumReduceTasks(number)）

太少的reduce会使得reduce运行的节点处于过度负载状态，太多的reduce对shuffle过程有不利影响，会导致作业的输出都是些小文件。
```







## 二、Hive

### 1、Hive架构

![Hive架构](D:\github\MyKnowledgeRepository\picture\Hive架构.png)

1. Hive存储的数据在HDFS上
2. Hive分析数据的底层是MapReduce
3. Hive程序运行是在Yarn上

### 2、Hive和数据库比较

1. 数据存储位置不同。Hive的数据一般存储在HDFS上，数据库的数据一般存储在块设备或者本地文件系统
2. 数据更新不同。Hive是不建议对数据改写的，而数据库的数据是需要经常修改的
3. 数据规模不同。Hive一般处理大规模数据，数据库一般处理小规模数据。

### 3、内部表和外部表

内部表：表删除，数据也删除。即元数据和原始数据都删除

外部表：表删除，数据还在。即元数据删了，但原始数据还在



### 4、4个By的区别

order by：全局排序，只有一个MapReduce

sort by：分区内有序。每个MapReduce内部排序，局部排序，不是全局排序

distribute by：类似于MapReduce的partition，分区，跟sort by结合使用。disttibute by语句要在sort by语句之前

cluster by：当distribute by和sort by字段相同时，可以考虑使用cluster by代替。但他的排序只能是升序排序



### 5、窗口函数

窗口函数可以指定分析函数的数据窗口大小，他和普通的聚合函数不同的是，他不会对结果进行分组，它使得输出中的行数与输入的行数相同。



它的用法是在聚合函数后面跟上over关键字

over(partition by ... order by ...)关键字



Rank函数

row_number():按顺序排序，1 2 3 4

rank():按顺序排序，重复跳过，总数不变 1 2 2 4

dense_rank():按顺序排序，重复不跳过，总数减少 1 2 2 3 



### 6、left join、right join和inner join的区别？

left join（左连接）：返回左表中的所有记录以及右表中的联接字段相等的记录

right join（右连接）：返回右表中的所有记录以及和左表中的联接字段相等的记录

inner join（等值连接）：只返回两个表中连接字段相等的记录



### 7、Hive优化

#### 优化的根本思想

1、尽早过滤数据，减少每个阶段的数据量。数据量少了，减少了网络传输

2、减少job数。所谓的job就是你提交一个复杂的SQL后，hive会把复杂的sql转换成若干个mapreduce的job任务，每个job对应你sql中的部分逻辑，每个job之间任务是独立的。任务是独立的，但数据是依赖的。就是下一个job依赖上一个job的数据文件，上一个job任务执行完会落盘到hdfs上，你job越多，落盘次数也越多，磁盘IO也越多。

3、解决数据倾斜



#### 优化方法

1、列裁剪和分区裁剪。列裁剪就是在查询的时候读取所需要的列，分区裁剪就是只读所需要的分区。

2、谓词下推。就是将SQL语句中的where谓词逻辑尽可能提前执行，减少下游处理的数据量。就是尽早地过滤数据

```sql
select a.uid,a.event_type,b.topic_id,b.title
from calendar_record_log a
left outer join (
  select uid,topic_id,title from forum_topic
  where pt_date = 20190224 and length(content) >= 100
) b on a.uid = b.uid
where a.pt_date = 20190224 and status = 0;
```

3、sort by代替order by。order by会将结果按某字段全局排序，这会导致所有的map端数据都进入一个Reducer中，在数据量大时，可能长时间都计算不出来。使用sort by可能会启动多个reducer进行排序，保证每个reducer内部局部有序。为了控制map端数据分配到reducer中的key，往往还需要配合distribute by一同使用。如果不加distribute by，map端数据就会随机分配到reducer。

```sql
select uid,upload_time,event_type,record_data
from calendar_record_log
where pt_date >= 20190201 and pt_date <= 20190224
distribute by uid
sort by upload_time desc,event_type desc;
```

4、去重优化。尽量避免使用distinct进行去重，特别是大表操作，使用group by代替。因为distinct key很容易造成数据倾斜，就是key一样的话，它会放到同一个mapreduce进行处理。使用group by可以避免数据倾斜

```sql
1、select distinct key from a
如果a表数据特别庞大的话，distinct key很容易造成数据倾斜，就是key一样的话他会放到
同一个mapreduce进行处理

2、select key from a group by key
效果是等价的，但效率比较高。因为group by可以避免数据倾斜
```



**join方面的优化**

1、小表放前大表放后原则。在编写带有join操作的代码语句时，应该将条目少的表放在join操作符的左边。因为Reduce阶段，位于join操作符左边的表的内容会被加载进内存，载入数据量较小的表可以有效减少OOM即内存溢出。

2、mapjoin()。当小表与大表join时，采用mapjoin，即在map端完成。同时可以避免小表与大表join产生的数据倾斜

```sql
select /*+ mapjoin(b)*/ a.key,a.value
from a join b
on a.key=b.key

b是小表
```

3、利用hive的优化机制减少job数。不论是外关联outer join还是内关联inner join，如果join的key相同，不管有多少个表，都会合并为一个MapReduce任务。

```sql
1个job
select a.val,b.val,c.val 
from a join b on (a.key=b.key1) 
join c on (c.key=b.key1)

2个job
select a.val,b.val,c.val 
from a join b on (a.key=b.key1) 
join c on (c.key=b.key2)
```

4、避免笛卡尔积。在两个表进行关联时，写上on关联条件

### 8、Hive的分区和分桶

**Hive分区**

数据表在Hive上存储上是 HDFS文件，就是文件夹的形式。现在最常的跑T+1数据，按当天时间分区比较多。把每天的数据存储在一个区，就是所谓的文件夹与文件。查找数据时只要指定分区字段就可以查找该分区的数据。创建分区表的时候，通过关键字partition by（column name string）声明该表是分区表

**Hive分桶**

分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。对于hive中每一个表、分区都可以进一步分桶，由列的哈希值/桶的个数来决定每条数据划分到哪个桶中。使用clustered by指定分区依据的列名，还要指定分为多少桶

**分区与分桶的区别**

分区和分桶的最大区别是分桶是随机分割数据库的，分区是非随机分割数据库的



### 9、内置函数

**1、nvl**

格式：nvl(value,default_value)，给值为null的数据赋值。如果value为null，则nvl返回default_value的值，否则返回value的值。

**2、行转列**



**3、列转行（一列转多行）**

Split(str,separator)：将字符串按照后面的分隔符切割，转换成字符array

Explode(col)：将hive一列中复杂的array或者map结构拆分成多行

lateral view用于和split,expload等UDTF函数（自定义表生成函数，一行输入，输出多行结果）一起使用，他能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。

Lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一或多行，lateral view再把结果组合，产生一个支持别名表的虚拟表

用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias

测试数据

| movie     | category       |
| --------- | -------------- |
| 《功勋》  | 记录,剧情      |
| 《战狼2》 | 战争,动作,灾难 |

SQL

```sql
SELECT movie,category_name 
FROM movie_info 
lateral VIEW
explode(split(category,",")) movie_info_tmp  AS category_name ;
```

测试结果

```
《功勋》      记录
《功勋》      剧情
《战狼2》     战争
《战狼2》     动作
《战狼2》     灾难
```



### 10、Hive数据倾斜

**Hive数据倾斜表现**

这里单说Hive自身的MR引擎，当执行任务的时候，map task 全部完成，并且99%的reduce task完成，只剩下一个或者少数几个reduce task一直在执行，这种情况下一般都是发生了数据倾斜。说白了，Hive的数据倾斜本质上是MapReduce的数据倾斜。

**Hive数据倾斜的原因**

在MapReduce编程模型中，大量相同的key被分配到一个reduce里，造成一个reduce任务繁重。

1、key分布不均衡

2、业务问题或者业务数据本身的问题，某些数据比较集中。比如join小表，但是key比较集中，导致的就是某些Reduce的值偏高。

**Hive 数据倾斜解决**

1、group by替代distinct，进行去重。如果数据量很大，count(distinct)就会非常慢，最终只有一个Reduce任务

2、group by配置调整。（1）map端预聚合,group by时，combiner在map端做部分预聚合，可以有效减少shuffle数量（2）设置check interval:设置map端预聚合的行数阈值，超过该值就会拆分成job（3）Hive自带了一个均衡数据倾斜饿配置项，其实现方法是在group by时启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再把前面预处理过的数据按key聚合并输出结果，这样就起到均衡的效果。

3、一般来倾斜的key都很少，可以将他们抽样出来，对应的行单独存入临时表中，然后打上随机前缀，最后再进行聚合。



### 11、Hive解决小文件

小文件产生的原因：

1. 动态分区插入数据，产生大量的小文件，从而导致map数量剧增
2. reduce数据越多，小文件也越多。（reduce的个数和输出文件是对应的）
3. 数据源本身就是包含大量的小文件



解决方案：

(1)在Map执行前合并小文件，减少Map数：CombineHiveInputFormat具有对小文件合并的功能

(2)merge，设置merge参数

```sql
SET hive.merge.mapfiles = true; 
-- 默认 true，在 map-only 任务结束时合并小文件 

SET hive.merge.mapredfiles = true; 
-- 默认 false，在 map-reduce 任务结束时合并小文件 

SET hive.merge.size.per.task = 268435456; 
-- 默认 256M 

SET hive.merge.smallfiles.avgsize = 16777216; 
-- 当输出文件的平均大小 小于 16m 该值时，启动一个独立的 map-reduce 任务进行文件 merge
```

(3)开启JVM重用

```sql
set mapreduce.job.jvm.numtasks=10
```





### 12、Hive函数：UDF、UDAF、UDTF的区别？

UDF：单行进入，单行输出

UDAF：多行进入，单行输出

UDTF：单行输入，多行输出



### 13、hive join的原理与机制

Hive join 可分为common join（reduce阶段完成的join）和map join（map阶段完成的join）



**Common join**

map阶段：读取源表的数据，map输出的时候以join on条件中的列为key，如果join有多个关联键，则已这些关联键作为key；Map输出的value为 select 或者 where 中需要用到的列，同时value中还包含了表的 tag 信息，用于表明此 value 对应哪个表

shuffle阶段：根据 key 的值进行 hash ，并将 key/value 的值按照 key 的hash 值推送至不同的 reduce 中，这样确保两个表中相同的key位于同一个 reduce 中

reduce阶段：根据 key 的值完成 join 操作，期间通过 tag 来识别不同表中的数据



以下面的HQL为例，图解其过程

```sql
SELECT a.id,a.dept,b.age
FROM a join b
ON (a.id = b.id);
```

![hive join图解过程](/Users/chenli75/Desktop/MyFile/github/MyKnowledgeRepository/picture/hive join图解过程.png)



**Map join**

MapJoin 通常用于一个很小的表和一个大表进行 join 的场景，使用时使用 /*+mapjoin(小表名称) * /来执行Mapjoin

流程：

首先 Task A 在客户端本地执行，扫描小表b的数据，将其转换成一个 HashTable 的数据结构，并写入本地的文件中，之后将该文件加载到DistributeCache中。

接下来的 Task B 是一个没有 Reduce 的 MapReduce，启动  MapTasks 扫描大表 a，在 Map 阶段，根据 a 的每一条记录去和 DistributeCache 中 b 表对应的 HashTable 关联，并直接输出结果，因为没有 Reduce，所以有多少个 Map Task，就有多少个结果文件。

![Map join](/Users/chenli75/Desktop/MyFile/github/MyKnowledgeRepository/picture/Map join 过程.png)



### 14、手写HQL

#### 手写HQL 第1题

score表结构：uid,subject_id,score

数据集如下

```
1001 01 90
1001 02 85
...
```

问题：找出所有科目成绩都大于某一学科平均成绩的学生

思路：

1. 求出每个学科平均成绩

   ```sql
   select uid,score,
   avg(score) over(partition by subject_id) avg_score
   from score;t1
   ```

   **注意：** 窗口函数可以返回和输入时一样的行数

2. 根据是否大于平均成绩 记录flag，大于记为0否则记为1

   ```sql
   select uid,if(score>avg_score,0,1) flag from t1;t2
   ```

   

3. 根据学生id进行分组统计flag的和，和为0，说明所有学科都大于平均成绩

   ```sql
   select uid from t2
   group by uid
   having sum(flag)=0;
   ```

最终SQL

```sql
select uid from(
	select uid,if(score>avg_score) flag from(
    	select uid,score,avg(score) over(partition by subject_id) avg_score
        from score
    )t1
)t2
group by uid
having sum(flag)=0;
```

#### 手写HQL 第2题

有如下用户访问数据，action表

```text
userid	visitDate	visitCount
u01		2017/1/21	5
u02		2017/1/23	6
u01		2017/1/23	6
...
```

问题：要求使用SQL统计出每个用户的累积访问次数，如下表

```
userid	月份	  小计	累积
u01		2017-1	11	   11
u01		2017-2	12	   23
...
```

思路：

1. 先修改数据格式

   ```sql
   select userid,
   date_format(regexp_replace(visitDate,'/','-'),'yyyy-MM') mn,
   visitCount
   from action;t1
   ```

2. 计算每人每月访问量

   ```sql
   select userid,mn,sum(visitCount) mn_count
   from t1
   group by userid,mn;t2
   ```

3. 按月累加访问量

   ```sql
   select userid,mn,mn_count,
   sum(mn_count) over(partition by userid order by mn)
   from t2;
   ```

最终SQL

```sql
select userid,mn,mn_count,
sum(mn_count) over(partition by userid order by mn)
from(
	select userid,mn,sum(visitCount) mn_count from(
    	select userid,
		date_format(regexp_replace(visitDate,'/','-'),'yyyy-MM') mn,
		visitCount
		from action
    ) t1
    group by userid,mn
)t2
```

总结：

date_format() 日期格式化函数

regexp_replace() 格式替换函数，针对字符串

sum() over() 可以实现累加求和的效果

group x y：表示将所有具有相同x字段和相同y字段的记录放到一个分组里



#### 手写HQL 第3题（京东面试题）

有50W个京东店铺，每个顾客访问过任何一个店铺的任何一个商品都会产生一条访问日志，访问日志表名为visit，用户id为user_id，店铺名称为shop

```
user_id	shop
u1		a
u2		b
u1		b
...
```

问题：

（1）统计店铺的UV(访客数)

```sql
select shop,count(distinct user_id) from visit group by shop;
```

（2）每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数

思路：

1. 查询每个店铺被每个用户访问的次数

   ```sql
   select shop,user_id,count(*) ct
   from visit
   group by shop,user_id;t1
   ```

   

2. 计算每个店铺被用户访问的排名

   ```sql
   select shop user_id,ct,
   rank() over(partition by shop order by ct desc) rk
   from t1;t2
   ```

   

3. 取每个店铺排名前3

   ```sql
   select shop,user_id,ct
   from t2
   where rk<=3;
   ```

最终SQL

```sql
select shop,user_id,ct from(
	select shop user_id,ct,
	rank() over(partition by shop order by ct desc) rk
	from(
    	select shop,user_id,count(*) ct
		from visit
		group by shop,user_id
    )t1
) t2
where rk<=3
```

总结：

count 和 distinct一起使用，可以实现计算非重复结果的数目

rank() over() 可以实现排序



#### 手写HQL 第4题

已知一个order表，有date,order_id,user_id,amount。请给出sql进行统计。

样例：2017-1-1，2012000，452115005，34

(1)给出2017年每个月的订单数、用户数、成交金额

```sql
select 
    date_format(data,'yyyy-MM'),
    count(order_id),
    count(distinct user_id),
    sum(amount)
from
	order
where 
	date_format(date,'yyyy')='2017'
group by
	date_format(dt,'yyyy-MM')
```

(2)给出2017年11月的新客数（指在11月才有第一笔订单）

```sql
select 
	count(user_id)
from 
	order
group by
	user_id
having
	date_format(min(date),'yyyy-MM')='2017-11'
```

min 函数返回一列中的最小值。NULL 值不包括在计算中。

第一次、第一笔、第一...，一般都要使用min函数

#### 手写HQL 第5题

请用sql写出所有用户中在今年10月份第一次购买商品的金额，表order字段:user_id,money,paymenttime(2017-10-01),orderid

第一步，先查询出10月份第一次

```sql
select
    user_id,
    min(paymenttime) paymenttime
from
	order
where
	date_format(paymenttime,'yyyy-MM')='2017-10'
group by
	user_id;t1
```

第二步，两表join，找到金额

```sql
select 
    t1.user_id
    t1.paymenttime
    od.money
from
	t1
join
	order od
on
	t1.user_id=od.user_id
and
	t1.paymenttime=od.paymenttime
```

 #### 手写SQL 第6题

有一个线上服务器访问日志格式如下

```
时间						接口						IP地址
2016-11-09 14:22:05		/api/user/login			110.03.523
...
```

求11月9号下午 14点（14-15点），访问/api/user/login接口的top10的ip地址

```sql
select 
    ip,
    interface,
    count(*) ct
from
	table
where
	data_format(date,'yyyy-MM-dd HH')>='2016-11-09 14'
	and
    data_format(date,'yyyy-MM-dd HH')<='2016-11-09 15'
    and
    interface='/api/user/login'
group by
	ip,interface
order by
	ct desc
limit 
	10;t1
```

```sql
select ip from t1;
```

#### 手写HQL 第7题

1、用一条SQL语句查询出每门课程都大于80分的学生姓名

```sql
select
	name
from
	table
group by
	name
having
	min(score)>80
```

2、怎样把下面的表

```
year month amount
1991	1	1.1
1991	2	1.2
1991	3	1.3
1991	4	1.4
1992	1	2.1
1992	2	2.2
1992	3	2.3
1992	4	2.4
...
```

 查看成这样一个结果

```
year	m1	m2	m3	m4
1991	1.1	1.2	1.3	1.4
1992	2.1	2.2	2.3	2.4
```

```sql
select
    year,
    (select amount from table a,table b where month=1 and b.year=a.year) as m1,
    (select amount from table a,table b where month=2 and b.year=a.year) as m2,
    (select amount from table a,table b where month=3 and b.year=a.year) as m3,
    (select amount from table a,table b where month=4 and b.year=a.year) as m4,
from 
	table
group by 
	year
```

自连接操作



3、一个info表

```
date	result
2005-05-09	win
2005-05-09	lose
2005-05-09	lose
2005-05-09	win
2005-05-10	win
2005-05-10	lose
2005-05-10	lose
```

```
date 		win lose
2005-05-09	2	2
2005-05-10	1	2
```

```sql
select
    date,
    sum(case when result='win' then 1 else 0) as win,
    sum(case when result='lose' then 1 else 0) as lose,
from
	info
group by
	date;
```



#### 如何对银行卡、身份证、手机号进行脱敏？（腾讯面试题）

concat()、left()、right()字符串函数组合使用

concat(str1,str2,...)：返回结果为连接参数产生的字符串

left(str,len)：返回从字符串str开始的len最左字符

right(str,len)：返回从字符串str开始的len最右字符

```sql
SELECT 
    CONCAT(LEFT(IdentityCardNo,3), '****' ,RIGHT(IdentityCardNo,4)) AS 身份证号
FROM c_inhabitantinfo;
```



#### 查询一个表（tb1）的字段记录不在另一个表（tb2）中 （万物心选面试题）

```sql
select tb1.* from tb1
left join tb2
on tb1.id=tb2.id
where tb2.id is null;
```

```sql
select tb1.* from tbl
where tb1.id not in (
	select tb2.id from tb2
)
```



## 三、HBase

### 1、HBase逻辑结构和物理存储结构

**逻辑结构**

不同的列族放在不同文件夹存储的

row_key是有序的，按字典序

Region是一张表的切片，而且是横向切分的

store真正放在HDFS上存储的东西

![HBase逻辑结构](D:\github\MyKnowledgeRepository\picture\HBase逻辑结构.png)



**物理结构**

HBase实现随机读写操作完全靠时间戳

![物理结构](D:\github\MyKnowledgeRepository\picture\HBase物理存储结构.png)



还有一种数据结构cell

cell：由{row_key,column Family,column,TimeStamp}唯一确定单元。cell中数据是没有类型的，全部是字节码形式存储。



### 2、HBase架构图（同时也是它的存储结构）

![HBase架构图](D:\github\MyKnowledgeRepository\picture\HBase架构图.png)



Master：管理RegionServer，处理Region的分配或转移

RegionServer：负责存储HBase实际的数据

Store：一个Store对应HBase表中的一个列族

Mem Store：内存存储，用来保存当前数据的操作

HFile：实际存储的物理文件。StoreFile是以HFile的形式存储在HDFS上

HLog：WAL,预写入日志机制



### 3、HBase原理

#### 读流程

1. client向Zookeeper请求meta表所在的位置regionserver
2. Zookeeper查询并返回mata表位置给client
3. client请求meta表所在的服务器regionserver
4. regeionserver返回meta表数据
5. client向regionserver发起读请求
6. regionserver查询MemStore和StoreFile的数据，并返回给client

![HBase读流程](D:\github\MyKnowledgeRepository\picture\HBase读流程.png)



#### 写流程

1. client向Zookeeper请求meta表所在的位置regionserver
2. Zookeeper查询并返回mata表位置给client
3. client向请求meta表所在的服务器regionserver
4. regeionserver返回meta表数据
5. client向regionserver发起写请求
6. regionserver将数据写入到HLog，为了数据的持久化和恢复
7. regionserver将数据写入到内存Mem Store
8. 反馈client写入成功

![HBase写流程](D:\github\MyKnowledgeRepository\picture\HBase写流程.png)



#### flush

1. 当MemStore数据达到一定阈值时，会把MemStore的数据flush到StoreFile中，将内存中的数据删除，同时删除HLog中的历史数据
2. 数据是存储到HDFS上

![flush](D:\github\MyKnowledgeRepository\picture\MemSore Flush过程.png)



#### compact

当StoreFile越来越多，会触发compact合并操作，把多个StoreFile合并成一个大的StoreFile

![compact](D:\github\MyKnowledgeRepository\picture\compact过程.png)



#### Split

当StoreFile越来越大，Region也越来越大，达到一定阈值，会触发Split操作，将Region一分为二



#### 4、Region预分区

预分区的目的主要是创建表的时候指定分区数，提前规划好表有多少个分区数，以及每个分区的区间范围，这样在存储的时候，rowkey会按照分区的区间存储，可以避免region热点问题



#### 5、RowKey设计原则

唯一性。确保每一条数据唯一

散列性。将rowkey均匀地散列在各个区间。所以使用散列性需要设计好预分区

长度原则。rowkey是存储到内存中的，如果rowkey太大的话，会占用大量的内存空间。

#### 6、RowKey如何设计

设计RowKey的主要目的是让数据均匀地分布在所有的region分区中，在一定程度上防止数据倾斜。

1. 通过随机数、hash、散列值(UUID)的方式将key打散
2. 字符串反转。时间戳是有规律的，但反过来就没规律了。还有手机号

#### 7、Phoenix二级索引的原理

Phoenix是一个框架。这个框架可以使用SQL语句的形式去操作HBase。

它的底层原理就是会把我们写的SQL语句翻译成HBase的指令，再用指令去操作HBase分布式集群。



HBase一级索引默认是开启的，我们程序员是不能干预的。









## 四、Zookeeper

### 1、选举机制

1. 半数机制：集群中半数以上机器存活，所以Zookeeper适合安装奇数台服务器

2. Zookeeper虽然没有在配置文件中指定Master和Slave，但Zookeeper工作的时候，是有一个Leader节点，其他是Follower节点，Leader是通过内部选举产生的

3. 选举过程：

   假设有五台机器，它们的id分别是1-5,同时他们是最新启动的，没有历史数据。

   机器是依次启动的，id1先启动，但目前只有一台服务器，他发出去的报文没人响应，所以它的选举状态是looking状态；

   id2启动，与id1进行通信，交换自己的选举结果。由于双方都没有历史数据数据，id值较大的会胜出，因此id2胜出。但由于没有超过半数以上机器同意选举它，所以id1和id2的选举状态都保持looking状态；

   id3启动，根据前面的理论，id3胜出，此时有超过半数以上机器选举它，所以id3当选为leader
   
   接下来的id4、id5启动，但由于前面的机器已经选择了id3，所以id4和id5当Follwer

### 2、什么是CAP法则？Zookeeper符合哪两个？

CAP法则是指在分布式系统中，一致性、可用性、分区容错性三者不可兼得

一致性是指在分布式所有数据备份中，在同一时刻是否同样的值。

可用性是指集群一部分节点出现故障后，集群整体是否还能响应客户端的读写请求

分区容错是指区间通信可能失败，系统不能在时限内保持数据一致性。



Zookeeper符合强一致性和高可用性！



### 3、Paxos算法

Paxos算法是一种基于消息传递的分布式一致性算法



它要解决的问题是：一个分布式系统就某个值(也就是协议)达成一致的问题

一个典型的场景就是分布式数据库系统中，如果各节点初始状态一致，每个节点执行相同的操作系列，那么最后他们会得到一个一致的状态。



但它有一个前提条件：通信是保证可靠的不会被篡改的



Paxos算法大致流程（大白话讲）：

Paxos算法有提议和投票过程，在整个提议和投票过程中，最重要的就是“提议者”和“接受者”

第一阶段：因为存在多个“提议者”,如果都提意见，那么“接受者”就很为难。所以要先明确哪个“提议者”是意见领袖，有权提出提议。“接受者”就要处理这个提议

第二个阶段：由上述意见领袖提出提议，“接收者”反馈意见，如果多数“接收者”接受了这个提议，那么提议就通过了。



怎么明确意见领袖？

通过编号。每个“提议者”在第一阶段先报个号，谁的号大谁就是意见领袖。



## 五、Kafka

### 1、Kafka的架构

![Kafka的架构](D:\github\MyKnowledgeRepository\picture\Kafka架构.png)

组成：生产者、Broker(Kafka服务器)、消费者、Zookeeper

注意：Zookeeper只保存了Broker的id信息，以及消费者的offset信息；没有生产者的信息



Producer：消息生产者，向Kafka发送消息的客户端

Consumer Group：这是Kafka实现广播和单播的手段。所谓的广播就是把消息发给所有的Comsumer，单播就是发给任意一个Consumer。

Broker：一台Kafka服务器就是一个Broker。一个集群由多个Broker组成

Topic：主题。可以理解为一个队列，里面存储相应信息的内容。一个Broker可以拥有多个Topic。（主题类似于文件系统中的文件夹，事件就是该文件夹中的文件）

Partition：分区。为了实现扩展性，一个非常大的Topic可以分布在多个Broker上，一个Topic可以有多个	Partition，每个Partition都是一个有序的队列。（一个Partition只能被一个消费者组的一个消费者消费，但它可以同时被不同消费者组的消费者消费）

Offset：偏移量。Partition里面的每条消息，都会分配一个有序的id，这个id就是Offset。他记录着你要消费的位置



### 2、Kafka的作用（为什么使用Kafka？即为什么使用消息队列？消息队列的作用是什么？）

解耦和扩展性：将生产者和消费者解耦开来，可以独立地扩展他们相应的功能

缓冲和削峰：如果上游数据突然暴增，下游可能扛不住。所有可以通过消息队列进行缓冲，下游队列可以慢慢从消息队列取数据处理。

异步处理：有时候并不需要立即处理数据，可以通过消息队列的异步处理机制，它允许用户把消息放入消息队列里，但不立即处理它。



### 3、ISR副本同步队列

一般副本设置2个或3个

副本的优势：提高可靠性

副本的劣势：增加网络IO



副本操作是以分区为单位的，每个副本都有自己的主副本和从副本。

主副本叫Leader，从副本叫Follower。处于同步状态下的副本叫 ISR（in-Sync-Replicas）



生产者和消费者都是和Leader交互读写数据，不和Follower交互。



如果Leader挂了，会从副本同步队列中选择一个Follower作为新的Leader



### 4、Kafka的数据不丢失机制

从生产者、消费者、Broker三方面回答。

1. 生产者数据的不丢失

   ACK机制：当Kafka发送消息的时候，都会有一个消息确认反馈机制，确保消息是否正常收到，有0，1，-1三种状态。

   ack=0，异步发送，消息发送完，立即发送下一条消息

   ack=1，Producer等待Leader确认收到数据，才发送下一条消息

   ack=-1，Producer等到Follower确认，才发送下一条消息

   

2. 消费者数据的不丢失

   通过Offset来保证每次消费的记录

   

3. Broker数据不丢失

   通过备份副本，来保证数据不丢失

   

### 5、Kafka数据是放在内存上还是磁盘上，为什么速度那么快？

放在磁盘上。

速度快的原因：

1. Kafka本身就是分布式集群，同时采用了分区技术，并发度高
2. 顺序写入磁盘。硬盘是机械结构，每次读写的操作是“寻址->写入”。寻址的过程中是很耗时的。为了提高读写速度，它采用了顺序写入。



### 6、Kafka消费过的消息如何再消费？

得从Offset下手，因为它记录着消费者的消费位置。而Offset一般是定义在Zookeeper上，如果要重复消费，需要重设Zookeeper上的Offset。可以通过Redis记录Offset的checkpoint点，当想重复消费的时候，读取Redis上的checkpoint点进行Zookeeper上Offset的重设。



### 7、Kafka数据重复怎么处理？

可以到下一级处理：Spark Streaming，Redis，Hive中去重



### 8、Kafka消息数据积压，Kafka消费能力不足怎么处理？

1. 可以增加Topic的分区数，增加消费者组的消费者数，分区数=消费者数（二者缺一不可）
2. 如果是下游数据处理不及时，可以增加每次批次拉取的数量。批次拉取的数据量太少，导致拉取数据+处理数据的速度< 生产数据的速度，处理数据小于生产数据，也会导致数据积压。



## 六、Spark

### 1、基于Yarn的Spark架构与作业提交流程

#### Yarn-Client

![Yarn-Client的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClient的Spark架构启动流程.png)

1. spark_submit脚本提交程序，在本地机器上启动driver
2. driver程序向ResourceManager申请启动ApplicationMaster
3. ResourceManager通知一个NodeManager去启动ApplicationMaster
4. ApplicationMaster启动完成之后向ResourceManager申请运行Executor的资源
5. ResourceManager分配Container返回给ApplicationMaster
6. ApplicationMaster与对应的NodeManger通信，申请和启动Container
7. NodeManager启动Executor进程
8. Executor启动完毕后反向注册driver



#### Yarn-Cluster

![YarnCluster的spark架构](D:\github\MyKnowledgeRepository\picture\YarnClusterd的Spark架构流程.png)



1. 本地机器提交Spark Application到ResourceManager
2. ResourceManager找到一个NodeManager启动ApplicationMaster
3. NodeManager启动ApplicationMaster并启动相应的Driver进程
4. ApplicationMaster向ResouceManager申请启动Executor
5. ResourceManager分配container返回给ApplicationMaster
6. ApplicationMaster与相应的NodeManager通信，申请启动exetutor
7. NodeManager启动executor进程
8. Executor进程启动完毕之后找ApplicationMaster进行反向注册



#### yarn-client和yarn-cluster模式的不同之处

YarnClient模式下，driver运行在本地机器

YarnCluster模式下，driver运行在集群某个NodeManager的节点上面



### 2、Spark的两种核心Shuffle

#### HashShuffle

##### 未优化的HashShuffle

![未优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\未优化的HashShuffle.png)

1. 第一个Stage，每个task都会创建下一个Stage的task数量相同的文件
2. 第二个Stage，每个task会到各个节点上面，拉取第一个stage上每个task的属于自己的输出文件



##### 优化的HashShuffle（开启map端文件合并）

![优化的HashShuffle](D:\github\MyKnowledgeRepository\picture\优化后的HashShuffle.png)

1. 第一个Stage，每个task依旧会创建下一个Stage，task数量相同的输出文件
2. 但是下一个task，直接复用上一个task的输出文件
3. 这样，第二个Stage，就会拉取少量的输出文件

#### SortShuffle

![SortShuffle](D:\github\MyKnowledgeRepository\picture\SortShuffle.png)



![bypass SortShuffle](D:\github\MyKnowledgeRepository\picture\bypass SortShuffle.png)

在sortShuffle中可以设置一个阈值，默认是200;当reduce task数量少于等于200时，即map task创建的文件少于等于200时，不会进行排序，依旧会输出reduce task的数量，最后将所有的数据文件合并成一个大文件



#### HashShuffle和SortShuffle的区别

1. SortShuffle会对每个reduce task要处理的数据进行排序
2. SortShuffle只会写入一个磁盘文件，不同reduce task的任务通过offset来界定



### 3、Spark共享变量

#### 共享变量原理

默认情况下，如果在某个算子函数中使用到了某个外部变量，那么这个变量的值会拷贝到每个task中。每个task只能操作自己的那份变量副本。如果多个task想要共享变量，这种方式是做不到的。



所以Spark提供了两种共享变量：广播变量和累加器



广播变量，仅仅会为每个节点(即Executor)拷贝一份，减少了网络传输和内存消耗

累加器可以让多个task共同操作一份变量，主要进行累加操作

#### 广播变量

广播变量是只读，它只会为每个节点拷贝一份副本，而不是为每个task拷贝一份副本。

它最大的好处就是减少了网络传输和内存消耗

#### 累加器

累加器主要用于多个节点对一个变量进行共享性操作。

Accumulator只提供给了累计操作。提供了多个task对一个变量共享的操作

但是task只能对Accumulator进行累加操作，不能读取

只有Driver程序可以读取Accumulator的值



### 4、Spark实现TopN的获取

算法思路：

1. 读入数据
2. 将数据通过map映射成KV格式的数据
3. 按照key对数据进行聚合（比如使用reduceByKey、groupBykey等）
4. 计算各个分区的TopN，使用SortMap来实现。也就是TreeMap，它实现了SortMap的接口
5. 规约所有的TopN的SortMap，得到最终的ToN SortMap



```java
/*
    *   程序入口函数
    * */
public void run() {
        /*
        *   读入inputPath中的数据
        * */
        JavaRDD<String> lines = jsc.textFile(inputPath, 1);

        /*
        *   将rdd规约到9个分区
        * */
        JavaRDD<String> rdd = lines.coalesce(9);

        /*
        *   将输入转化为kv格式
        *   key是规约的主键, value是排序参考的个数
        *   注: 这里的key并不唯一, 即相同的key可能有多条记录, 所以下面我们规约key成唯一键
        *   输入:line, 输出:kv
        * */
        JavaPairRDD<String, Integer> kv = rdd.mapToPair(
            new PairFunction<String, String, Integer>() {
            public Tuple2<String, Integer> call(String s) throws Exception {
                String[] tokens = s.split(",");
                return new Tuple2<String, Integer>(tokens[0], Integer.parseInt(tokens[1]));
            }
        });

        /*
        *   规约主键成为唯一键
        *   输入:kv, 输出:kv
        * */
        JavaPairRDD<String, Integer> uniqueKeys = kv.reduceByKey(new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer i1, Integer i2) throws Exception {
                return i1 + i2;
            }
        });

        /*
        *   计算各个分区的topN
        *   这里通过广播变量拿到了topN具体个数, 每个分区都保留topN, 所有分区总个数: partitionNum * topN
        *   输入:kv, 输出:SortMap, 长度topN
        * */
        JavaRDD<SortedMap<Integer, String>> partitions = uniqueKeys.mapPartitions(new FlatMapFunction<Iterator<Tuple2<String,Integer>>, SortedMap<Integer, String>>() {
            public Iterable<SortedMap<Integer, String>> call(Iterator<Tuple2<String, Integer>> iter) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                while (iter.hasNext()) {
                    Tuple2<String, Integer> tuple = iter.next();
                    topN.put(tuple._2, tuple._1);

                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return Collections.singletonList(topN);
            }
        });

        /*
        *   规约所有分区的topN SortMap, 得到最终的SortMap, 长度topN
        *   reduce过后, 数据已经到了本地缓存, 这是最后结果
        *   输入: SortMap, 长度topN, 当然有partitionNum个, 输出:SortMap, 长度topN
        * */
        SortedMap<Integer, String> finalTopN = partitions.reduce(new Function2<SortedMap<Integer, String>, SortedMap<Integer, String>, SortedMap<Integer, String>>() {
            public SortedMap<Integer, String> call(SortedMap<Integer, String> m1, SortedMap<Integer, String> m2) throws Exception {
                final int N = topNum.getValue();
                SortedMap<Integer, String> topN = new TreeMap<Integer, String>();
                for (Map.Entry<Integer, String> entry : m1.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                for (Map.Entry<Integer, String> entry : m2.entrySet()) {
                    topN.put(entry.getKey(), entry.getValue());
                    if (topN.size() > N) {
                        topN.remove(topN.firstKey());
                    }
                }
                return topN;
            }
        });
```



### 5、SparkStreaming消费Kafka中的数据的方式？或者说SparkStreaming与Kafka的集成方式？

有两种方式，一个是基于Receiver的方式，一个是基于Direct的方式



基于Receiver的方式使用了Kafka的高级API，在Zookeeper中保存消费过offset，配合着WAL机制可以保证数据的高可靠和零丢失。但是无法保证数据被处理一次且仅一次，可能会处理两次。因为spark和Zookeeper之间可能不是同步的



基于Direct的方式使用了Kafka的低级API，spark streaming自己负责追踪offset，并保存checkpoint。spark自己一定是同步的，所以可以保证数据是消费一次且仅一次。



#### 一、基于Receiver的方式

这种方式是通过receiver来获取数据，使用的是Kafka的高级API，receiver从kafka中获取的数据都是存储在Spark Executor的内存中（如果数据量突然暴增，大量batch堆积，很容易出现内存溢出问题），然后启动job去处理数据。



然而在这种方式下，很可能会因为底层的失败而丢失数据，如果用启动高可靠机制，让数据零丢失，需要开启预写日志机制（Writer Ahead Log,WAL）。该机制会同步地将接收到Kafka的数据写入到分布式存储系统上的预写日志中（比如HDFS）。这样即使底层节点的失败了，也可以从预写日志中的数据进行恢复。

#### 二、基于Direct的方式

基于Direct的方式会周期性地查询Kafka，来获得每个topic的partition的最新offset，从而定义每个offset的范围。当处理数据的job启动时，会使用Kafka的低级API来获取Kafka指定offset范围的数据。



**优点：**

并行读取：如果要读取多个partition的数据，spark会创建跟kafka partition相同的分区数，并且会并行地从Kafka中读取数据。

高性能：如果要保证数据零丢失，基于Receiver的方式是要开启WAL机制。这种方式是效率低下的，因为数据复制了两份。因为kafka本身就有高可靠机制，他自己会复制一份，WAL又复制了一份。而基于Direct的方式，不依赖于Receiver，不需要开启WAL，只要kafka做了数据的备份，就可以通过kafka的副本进行恢复



#### 两种方式的对比

基于Receiver的方式使用了Kafka的高级API来在Zookeeper中保存消费过offset，配合着WAL机制可以保证数据的高可靠和零丢失。但是无法保证数据被处理一次且仅一次，可能会处理两次。因为spark和Zookeeper之间可能不是同步的



基于Direct的方式使用了Kafka的低级API，spark streaming自己负责追踪offset，并保存checkpoint。spark自己一定是同步的，所以可以保证数据是消费一次且仅一次。

#### 知识补充（Kafka的高级API和低级API）

**高级API**

优点：

1. 简单
2. 不需要管理offset，由Zookeeper管理
3. 不需要管理分区、副本等情况

缺点：

1. 不能控制offset(对于某些特殊需求来说)
2. 不能细化控制分区、副本、Zookeeper



**低级API**

优点：

1. 能够自己控制offset，想从哪里读取数据就哪里读取数据
2. 能够控制分区数，对分区数进行负载均衡
3. 降低了对Zookeeper的依赖

缺点：

1. 太过于复杂，需要自己管理offset，连接哪个分区



### 6、Spark Streaming窗口函数原理

窗口函数就是在原来sparkstreaming的批次的基础上，再次进行封装，每次计算多个批次的数据，同时传入一个滑动步长，用于设置当前任务计算完成后，下次任务开始计算的地方。



![滑动窗口](D:\github\MyKnowledgeRepository\picture\滑动窗口.png)

图中time1就是sparkStreaming计算批次的大小，虚线框以及实线框的大小就是窗口的大小，由虚线框到实线框的距离就是滑动的步长



### 7、Spark Steaming基本原理

Spark Streaming是Spark Code API的一种扩展，它支持从多种数据源读取数据，比如kafka、flume。

它的基本原理是：

接收实时输入数据流，然后将数据拆分成多个batch（微批次），比如每一秒的数据封装成一个batch

然后将每个batch交给spark的计算引擎处理，最后会产生一个结果数据流，他也是由一个个batch组成的



### 8、Hadoop和Spark应用场景

Hadoop底层使用MapReduce计算架构，只有map和reduce两种操作，表达能力比较欠缺，而且在MR过程中会重复的读写hdfs，造成大量的磁盘io读写操作，所以适合高时延环境下批处理计算的应用；

Spark是基于内存的分布式计算架构，数据分析更加快速，所以适合低时延环境下计算的应用；

同时spark更适用于机器学习之类的迭代式应用。

spark与hadoop最大的区别在于迭代式计算模型。



### 9、RDD持久化原理？

调用cache()或者persist()方法即可。cache的底层就是persist()无参数版本。persist(MEMORY_ONLY)，将数据持久化到内存中



### 10、checkpoint检查点机制

应用场景：当spark应用程序特别复杂，从初始的RDD到最后整个应用的完成有很多步骤，而且应用运行时间特别长，适合使用checkpoint功能。



原因：对于特别复杂spark应用，会出现某个反复使用的RDD，即使之前持久化过，但由于节点故障导致数据丢失，没有容错机制，又需要重新计算一次数据。



实现原理：首先找到stage最后的finalRDD，然后按照RDD依赖关系往回找，找到使用checkpoint的RDD,然后标记这个RDD，再重新启动一个线程将checkpoint之前的RDD缓存到HDFS中，最后RDD依赖关系从checkpoint的位置切断。



### 11、checkpoint和持久化机制的区别？

最主要的区别是持久化只是将数据保存在BlockManager中，RDD的血缘关系不变的。但是checkpoint执行之后，RDD没有所谓的依赖了。

持久化丢失数据的可能性更大，因为节点故障可能会导致磁盘内存数据的丢失，但是checkpoint数据通常保存在高可用的文件系统中，比如HDFS，所以数据丢失可能性比较低。



### 12、DStream以及基本工作原理

DStream是 spark streaming提供的一种高级抽象，代表一个持续不断的数据流。

DStream由一组时间序列上连续的RDD表示，每个RDD只包含一段时间的数据。

Spark Streaming一定是有一个输入的DStream接收数据，按照时间划分成一个一个batch，并转化为RDD，RDD数据分散在各个子节点的partition中。



### 13、Spark工作机制

1. 用户提交程序(Application)创建SparkContext实例，SparkContext根据RDD对象生成DAG图，将作业(Job)提交给DAGScheduler
2. DAGScheduler将作业(Job)划分成不同的Stage(从末端RDD开始，根据shuffle来划分)，每个Stage都是任务的集合(TaskSet)，以TaskSet为单位提交给TaskScheduler
3. TaskScheduler管理任务(Task)，并通过资源管理器(Cluster Manager)把任务(task)发给集群中的Worker
4. Worker接收到任务(Task)，启动Executor进程中的线程Task来执行任务



### 14、Spark如何保证宕机迅速恢复？

1. 增加standby master
2. 编写shell脚本，定期检查maste状态，出现宕机后对master进行重启操作



### 15、Spark主备切换原理知道吗？

Master实际上是可以配置两个的，Spark原生的Standalone模式Master是支持主备切换的。当Active Master挂掉之后，我们可以将Standby Master切换为Active Master。

Spark Master主备切换可以基于两种机制，一种是基于文件系统，一种是基于Zookeeper。

基于文件系统的，需要在Active Master挂掉之后手动切换standby Master上。

基于Zookeeper的，可以实现自动的切换。



### 16、RDD中reduceBykey与groupByKey哪个性能好，为什么

reduceByKey会在结果发送给reducer之前会在本地mapper进行聚合，类似于MapReduce中的combiner。这样做的好处在于map端进行一次reduce之后，数据量会大幅度减小，从而减小网络传输，保证reduce端更快地计算出结果。



groupByKey会对每个RDD中value进行聚合形成序列(iterator)，此操作发生在reduce端，会造成大量的网络传输，如果数据量非常大，可能会形成OutOfMemoryError。



所以在进行大量数据的reduce操作时候建议使用reduceByKey。不仅可以提高速度，还可以防止使用groupByKey造成的内存溢出问题。



### 17、Spark master HA主从切换过程会影响到集群已有作业的运行？为什么

不会。因为程序在运行之前已经申请过资源了，driver和executors通讯，不需要和master进行通讯

（Executor启动完毕之后反向注册给Driver）

### 18、 spark master使用zookeeper进行HA，有哪些源数据保存到Zookeeper里面

spark通过这个参数spark.deploy.zookeeper.dir指定master元数据在zookeeper中保存的位置。

要包括Application、driver、worker和executors.

standby节点要从zk中，获得元数据信息，恢复集群运行状态，才能对外继续提供服务，作业提交资源申请等，在恢复前是不能接受请求的。



### 19、谈谈你对RDD的理解

RDD是spark的最基本数据抽象，它是弹性分布式数据集，代表的是一个不可变、可分区、可并行计算机的集合。

它的作用是：提供了一组抽象的数据模型，将具体的应用逻辑表达成一系列的转换操作。

不同的RDD之间的转换操作会形成依赖关系，进而实现管道化，大大降低数据的复制、磁盘I/O。

RDD的依赖分为宽依赖和窄依赖，用来解决数据容错时的高效性以及划分任务时起到重要作用。



### 20、你是怎么理解Spark的，Spark的特点有哪些？

spark是基于内存的，用于处理大规模数据的分析引擎。

它的组成模块有spark core、spark sql、spark streaming、sparkMLlib、spark graphx

它的特点是：

快：比mapreduce计算速度快

通用：它支持多种计算模型，能够进行离线计算、交互式计算、实时计算等

兼容性：支持yarn的调度，可以处理hadoop的数据



### 21、Spark的几种部署方式

Local：运行在一台机器上

Standalone：构建master + slave的spark集群

yarn：spark客户端直接连接yarn，不需要额外的构建集群

mesos：国内大环境比较少用



### 22、Spark提交作业的参数

腾讯考查了

```
executor-cores —— 每个executor使用的内核数
num-executors —— 启动executors的数量，默认为2
executor-memory —— executor内存大小，默认1G
driver-cores —— driver使用内核数，默认为1
driver-memory —— driver内存大小
```



### 23、简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage又根据什么决定task个数？

窄依赖：父RDD的一个分区只会被子RDD的一个分区依赖（一对一）

宽依赖：父RDD的一个分区会被子RDD的多个RDD依赖（一对多）



每遇到一个宽依赖则划分为一个stage



stage是一个taskset，stage根据分区数划分成一个个的task



### 24、Repartition和Coalesce 的关系与区别

关系：两者都是改变RDD的partition数量，repartition底层调用了coalesce的方法，coalesce(numPartitions,shuffle=true)



区别：repartition一定会发生shuffle，coalesce根据传入的参数是否发生shuffle

一般情况下，使用repartition增大rdd的分区数，使用coalesce减少rdd的分区数



在调优方面的应用，一般在filter算子之后可以根据数据量的大小调用coalesce算子减少分区数，从而避免数据倾斜



### 25、当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？

 使用foreachPartition代替foreach，在foreachPartition内获取数据库的连接。  



### 26、Spark读取Kafka数据丢失？

spark streaming读取kafka数据丢失是很大的问题，spark streaming可以通过direct方式读取kafka，提供了checkpoint方式自己去维护和读取kafka的offset，将数据放到HDFS上。

Receiver的方式要确保数据零丢失，需要开启预写入日志机制，同步保存所有kafka的数据到分布式文件系统上，以便发生故障时恢复所有数据。



### 27、RDD、DataFrame、DataSet三者的区别与联系

RDD是一个懒执行的、不可变的、可分区的，可并行计算的的数据集合。它的优点是简单易用，但它的缺点是序列化和反序列成本比较高

DataFrame也是一个分布式数据容器，它像传统数据库的二维表格，有记录数据的结构信息，即schema

Dataset是Spark最新数据抽象，是DataFrame API的一个扩展，具有类型安全检查。样例类被用在DataSet中定义数据的结构信息，样例类的每个属性的名称直接映射到DataSet中的字段名称。Dataframe是Dataset的特列，DataFrame=Dataset[Row]，所以可以通过as方法将Dataframe转换为Dataset。

**三者的共性**：

- 三者都是spark平台下的分布式弹性数据集
- 三者有许多共同的函数，如 filter，排序等；都有partition的概念，支持持久化操作
- 三者都有惰性机制，只有遇到action算子时才会执行运算。



### 28、用户自定义函数

1、UDF:用户自定义函数。一路输入，一路输出。比如：year,date_add

写一个函数，通过spark.udf.register(name,fuction)方法进行注册

2、UDAF:用户自定义聚合函数。多路输入，一路输出。比如：count、sum

自定义一个继承UserDefinedAggregateFunction类，实现里面的方法。

3、UDTF:用户自定义表函数。一路输入，多路输出。比如explode。一般不用自定义



### 29、 Spark sql小文件解决方案

spark sql小文件是指文件大小显著小于hdfs block块大小的文件，hive调度的MR任务可以通过简单设置几个小文件合并的参数解决小文件问题，但spark本身并不支持小文件合并功能的。

spark产生小文件的原因：spark生成的文件数量直接取决于RDD里partition的数量。RDD的分区与任务并行度相关，当任务并行度过高时，很容易产生很多的小文件。





## 七、Flink

Flink重点：容错机制、状态管理、时间窗口、时间语义

### Flink基础

#### 1、简单介绍一下Flink

Flink是一个分布式处理引擎框架，用于对无界和有界数据流进行状态计算。它提供多种抽象API供用户使用。

DataSet API，对静态数据进行批处理操作，将静态数据抽象成分布式的数据集。

DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流。

Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表。

此外，Flink还提供机器学习、图计算等领域库



#### 2、Flink相比传统的Spark Streaming区别？

重要区别：Flink是标准的实时处理引擎，基于事件驱动。而Spark Streaming是微批模型

两个框架的主要区别：

1. SparkStreaming在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink在运行时主要包括：JobManager、TaskManager和Slot

2. 任务调度：SparkStreaming连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming会依次创建DStreamGraph、JobGenarator、JobScheduler。

   Flink根据用户提交的代码生成StreamGraph，经过优化生成JobGraph，然后提交给JobManager进行处理，JobManager会根据JobGraph生成ExecutionGraph，ExecutionGraph是Flink调度最核心的数据结构，JobManager根据ExecutionGraph进行调度。

3. 时间机制：SparkStreaming只支持有限的时间机制。Flink支持三种：处理时间、事件时间、注入时间。同时也支持watermark机制来处理滞后的数据

4. 容错机制：对于SparkStreaming任务，我们设置checkpoint，发生故障，我们可以从上次checkpoint恢复，但是这个只能使得数据不丢失，可能会重复处理，不能做到恰好一次处理语义。

   Flink则使用两阶段提交协议来解决这个问题



#### 3、Flink的组件栈有哪些？

![Flink的组件栈](D:\github\MyKnowledgeRepository\picture\Flink组件栈.png)

Flink是一个分层架构系统，每一层所包含的组件都提供特定的抽象，用于服务上层组件。

flink组件栈包括deploy层、runtime层、API层、libraries层。

deploy层：该层涉及到Flink的部署模式，支持 local、cluster、cloud等多种部署模式。

runtime层：该层提供了支持flink计算的核心实现，比如：支持分布式stream处理、JobGraph到ExecutionGraph的映射、调度等，为上层API层提供基础服务

API层：主要实现了面向无界Stream的流处理和面向Batch的批处理API，其中面向流处理对应DataStreamAPI,面向批处理对应DataSetAPI。

libraries层：应用框架层，在API层之上构建满足特定的应用实现计算框架，分别对应于面向流处理和面向批处理两类。



#### 4、Flink的基础编程模型

![Flink的基础编程模型](D:\github\MyKnowledgeRepository\picture\Flink基础编程模型.png)

Flink程序的基本构建是数据输入来自一个Source， Source代表数据的输入端，经过Transformation进行转换，然后在一个或多个Sink接收器中结束。



#### 5、Flink集群有哪些角色？各自有什么作用？

flink集群有jobmanager、taskmanager、client三个角色。

Jobmanager扮演着集群中的管理者Master角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，故障恢复等。同时管理集群中的从节点taskmanager。

taskmanager是实际负责执行计算的worker，在其上执行flink job的一组task，每个taskmanager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候向jobmanager汇报资源状态。

client是flink程序提交的客户端，当用户提交程序时，首先会创建一个client，client会对用户提交的程序提交到集群处理，所以client需要从用户提交的flink程序配置中获取 jobmanager的地址，并建立jobmanager连接，将flink job提交给jobmanager



#### 6、Flink的运行必须依赖Hadoop组件吗？

flink可以完全独立于Hadoop，在不依赖于hadoop组件下运行。但是作为大数据的基础设施，hadoop体系是任何大数据框架都绕不过去的。flink可以集成众多hadoop组件，例如yarn、hbase、hdfs等等。flink可以和yarn集成做资源调度，也可以读写hdfs，或者利用hdfs做检查点



#### 7、Tast Slot的概念

taskmanager是实际负责执行计算的worker， taskmanager是一个jvm进程，并会以独立的线程来执行一个task或多个subtask。

为了控制taskmanager能接受多少个task，flink提出了task slot的概念。

简单的说，taskmanager会将自己节点上管理的资源分为不同的slot，固定大小的资源子集。这样避免了不同job的task互相竞争内存资源，但是需要注意的是，slot只会做内存隔离，没有做cpu隔离。



#### 8、说说Flink的常用算子

Map：DataStream -> DataStream，输入一个参数产生一个参数，它的功能是对输入的参数进行转换操作。

Fliter：过滤指定条件的数据。

KeyBy：按照指定的key进行分组。

Reduce：用来进行结果汇总合并。

Window：窗口函数，根据某些特性将每个 key的数据进行分组（例如：5秒内到达的数据）



#### 9、你们的Hadoop集群和Flink集群规模多大

我们能使用的hadoop集群有2万+节点。

我们的Flink是独立运行的，我们部门的集群资源有160核400G，几乎全部用完，资源很紧张。



#### 10、Flink分区策略

数据分区在 flink中叫做partition。本质上来说，分布式计算就是把一个作业切分成子任务 task，将不同的数据交给不同的task计算。

目前flink支持8种分区策略：

1. GlobalPartitioner

   数据会被分发到下游算子的第一个实例中进行处理

2. ForwardPartition

   用于记录输出到下游本地的算子实例，他要求上下游算子并行度一样。

3. ShufflePartitioner

   随机将元素进行分区

   ```java
   dataStream.shuffle();
   ```

4. RebalancePartitioner

   数据会被循环发送到下游的每一个实例中进行处理。

   ```scala
   dataStream.rebalance()
   ```

5. RescalaPartitioner

   根据上下游算子的并行度，循环的方式输出到下游算子的每个实例。如上游有2个 Source，下游有6个 Map，那么每个 Source 会分配3个固定的下游 Map，不会向未分配给自己的分区写入数据。这一点与 ShufflePartitioner 和 RebalancePartitioner 不同， 后两者会写入下游所有的分区。

   ```scala
   dataStream.rescale();
   ```

6. BroadcastPartitioner

   将该记录广播给所有分区，即有N个分区，就把数据复制N份，每个分区1份

   ```scala
   dataStream.broadcast();
   ```

7. KeyGroupStreamPartitioner

   Hash分区器，会将数据按key的Hash值输出到下游算子实例中

8. CustomPartitionerWrapper

   用户自定义分区器，需要用户自己实现partitioner接口



#### 11、Flink的并行度，并行度如何设置？

Flink中的任务被分为多个并行任务来执行，其中每个并行的实例处理一部分数据。

它支持不同级别设置并行度：配置文件、客户端、 env、算子

从优先级上来看：算子级别 >  env级别 > client级别 > 配置文件



#### 12、Flink的 slot和parallelism有什么区别？

slot是指taskmanager的并发执行能力，假设我们将taskmanager.numberOfTaskSlots配置为3，那么每个taskmanager中分配3个TaskSlot，3个 taskmanager一共有9个TaskSlot。

parallelism是指 taskmanager实际使用的并发能力，假设把parallelism.default设置为1，那么9个taskslot只能使用1个，有8个空闲。



#### 13、Flink有没有重启策略？有几种？

flink实现多种重启策略。当 Task 发生故障时，Flink 需要重启出错的 Task 以及其他受到影响的 Task ，以使得作业恢复到正常执行状态。

固定延迟重启策略，如果发生故障，系统会重启作业 3 次，每两次连续的重启尝试之间等待 10 秒钟

```scala
val env = ExecutionEnvironment.getExecutionEnvironment()
env.setRestartStrategy(RestartStrategies.fixedDelayRestart(
  3, // 尝试重启的次数
  Time.of(10, TimeUnit.SECONDS) // 延时
))
```

故障率重启策略，在Job失败后会重启，但是超过失败率后，Job会最终被认定失败。

```scala
val env = ExecutionEnvironment.getExecutionEnvironment()
env.setRestartStrategy(RestartStrategies.failureRateRestart(
  3, // 每个测量时间间隔最大失败次数
  Time.of(5, TimeUnit.MINUTES), //失败率测量的时间间隔
  Time.of(10, TimeUnit.SECONDS) // 两次连续重启尝试的时间间隔
))
```

无重启策略，Job直接失败，不会尝试进行重启

```scala
val env = ExecutionEnvironment.getExecutionEnvironment()
env.setRestartStrategy(RestartStrategies.noRestart())
```

后备重启策略, 如果没有定义其他重启策略，默认选择固定延时重启策略。



#### 14、Flink的分布式缓存，如何使用？

Flink提供了一个分布式缓存，类似于hadoop，可以使用户在并行函数中很方便的读取本地文件，并把它放在taskmanager节点中，防止task重复拉取。
工作机制如下：程序注册一个文件或者目录，通过ExecutionEnvironment注册缓存文件并为它起一个名称。
当程序执行，Flink自动将文件或者目录复制到所有taskmanager节点的本地文件系统，仅会执行一次。用户可以通过这个指定的名称查找文件或者目录，然后从taskmanager节点的本地文件系统访问它。

```java
//获取运行环境
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

//1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试
env.registerCachedFile("/Users/wangzhiwu/WorkSpace/quickstart/text","a.txt");
```



#### 15、Flink的广播变量

可以理解广播变量就是一个公共的共享变量，将每一个数据集广播后，不同的task都可以在节点上获取到，每个节点只存一份，如果不使用广播，每一个task都会拷贝一份数据集，造成内存资源浪费。



#### 16、Flink窗口函数

flink支持两种划分窗口的方式，按照time和count。

如果根据时间划分窗口，他就是一个time-window；如果根据数据划分窗口，那么他就是一个count-window。

flink支持窗口的两个重要属性（size和interval）

如果size=interval，那么就会形成滚动窗口（无重叠数据）

如果size>interval，那么就会形成滑动窗口（有重叠数据）

如果size<interval，那么这种窗口将会丢失数据



#### 17、Flink的状态存储

Flink在计算过程中需要存储中间状态，来避免数据丢失和状态恢复。

Flink提供三种状态存储方式：

- MemoryStateBackend（内存存储）

在java堆中保存状态对象，key/value状态和窗口算子都会以Hash表的方式保存状态值，触发器等。

当进行checkpoints的时候，会对状态做快照，生成的快照将和checkpoint ACK消息一起发给job manager，jobmanager会将状态存储到对内存中。

适合场景：本地开发和调试、状态很小的作业

- FsStateBackend（文件系统存储）

FsStateBackend需要配置一个checkpoint路径，一般配置为hdfs目录。

FsStateBackend将工作状态数据保存在taskmanager的java内存中，进行快照时，将快照写入配置的路径，然后将写入的文件路径告知jobmanager。

jobmanager中保存所有状态的元数据信息

适合场景：大状态、长窗口的作业

- RocksDBStateBackend（Rocks DB存储）

RocksDBStateBackend需要配置一个checkpoint路径，一般配置为hdfs目录。

RocksDBStateBackend将工作状态保存在taskmanager的RockDB数据库中，当checkpoint时，RockDB中的所有数据会被传输到配置的目录中，少量的元数据信息保存在jobmanager内存中。

适用场景：超大状态、超长窗口的作业



#### 18、Flink的时间类型

有3类时间，分别是数据本身的产生时间，进入Flink系统的时间和被处理的时间。

Event Time是每条数据在其生成设备上发生的时间，它嵌入在数据中，即使数据发生乱序、延迟，也能提供正确的结果，它和操作系统的时间无关。

Processing Time是指执行相应操作的机器的系统时间，在分布式环境中，它容易受到事件到达系统的速度，以及Flink内部处理的先后顺序的影响，它不能准确地反映数据发生的时间序列。

Ingestion Time是事件进入Flink的时间，就是在Source处获取到这个数据的时间，它不受Flink分布式系统处理事件的先后顺序和数据传输的影响，但它也不能准确反映数据发生的时间序列。



#### 19、Flink水印是什么，有什么作用？（WaterMark机制）

watermark是flink处理eventime窗口计算提出的一种机制，本质上是一种时间戳，经常和window一起处理乱序事件。一个带有时间戳的 watermark到达，相当于告诉 flink系统，任何小于这个时间戳的数据都已经到达。



### 20、Storm、Spark Stream、Flink三者比较

| 技术         | 速度       | 吞吐量 | 准确性 |
| ------------ | ---------- | ------ | ------ |
| Storm        | 快、低延迟 | 小     | 低     |
| Spark Stream | 稍微较慢   | 大     | 高     |
| Flink        | 快         | 大     | 高     |



### Flink 中级

#### 1、Flink是如何支持流批一体的？

flink开发者认为批处理是流处理的一种特殊情况，批处理是有限的流处理。flink使用一个引擎支持了DataSet API 和 DataStream API



#### 2、Flink是如何做到高效的数据交换的？

在一个flink job中，数据是需要在不同的 task中进行交换的，整个数据交换是由TaskManager负责的，TaskManager网络组件首先从buffer中收集records，然后再发送。records不是一条条发送，而是积累到一个批次后再发送，batch技术可以更高效地利用网络。



#### 3、Flink是如何做容错的？

 flink实现容错主要靠强大的CheckPoint机制和State机制。CheckPoint负责定时制作分布式快照，对程序中的快照进行备份； State用来存储计算过程中的中间状态。



#### 4、Flink中window出现数据倾斜，有什么办法解决？

window产生数据倾斜指的是数据在不同窗口内堆积的数据量相差过多。本质上产生的原因是数据源头发送的数据量速度不同导致的。可以通过两种方式解决：

1. 在数据进入窗口前做预聚合
2. 重新设计聚合窗口的key



#### 5、Flink中使用聚合函数keyby、groupby等函数时，出现数据热点该如何处理？

数据倾斜和数据热点是所有大数据都要处理的问题，处理这样的问题可以从两个方面入手。

- 业务上规避问题

假设一个订单表，北京和上海两个订单量增长几十倍，其余城市数据量不变。这时做聚合时，北京和上海就会出现数据堆积，我们可以单独处理北京和上海的数据

- key的设计

把热key进行拆分，比如上述的北京和上海，可以把北京和上海按照地区进行拆分聚合。



#### 6、Operator chain（算子链）了解吗？

为了高效地分布式执行， flink会尽可能地将opetator的subtask链接（chain）在一起形成 task。每个task在一个线程中执行，将operator链接成task是非常有效的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。



#### 7、Flink什么情况下会形成算子链？

条件：

1. 上下游并行度一致
2. 下游节点的入度为1（也就是说下游节点没有其他节点的输入）
3. 上下游节点都在同一个slot group中
4. 下游节点的策略为always（可以与上下游链接，map、flatmap、fliter等算子默认为always）



#### 8、消费kafka数据时，如何处理脏数据？

可以在前面加一个fliter算子，将不符合规则的数据过滤出去



### 9、Flink是如何保证exactly-once语义的？

Exactly-once 精确一次指的的是Flink应用从Source端开始到Sink端结束，数据必须经过起始点和结束点

Flink通过两阶段提交和状态保存来实现端到端的一致性语义。

状态保存就是Checkpoint，通过给程序快照的方式使得将历史某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。

两阶段提交：

1. 对于每个checkpoint，sink任务会启动一个事务，并将接下来所有接收的数据添加到事务里
2. 然后将这些数据写入外部sink系统，但不提交他们，这时只是预提交
3. 当它收到checkpoint完成的通知时，它才正式提交事务，实现结果的真正写入
4. 这种方式真正实现了exactly-once，它需要一个提供事务支持的外部sink系统





## 八、数据仓库

**定义**：数据仓库（data warehouse）是一个面向主题的（subject oriented）、集成的（integrate）、相对稳定的（non-volatile）、反映历史变化（time variant）的数据集合，用于支持管理决策

**工具**：阿里的工具 dataworks

### 1、数据分层

每个企业根据自己的业务分成不同的层次，但最基础的分层思想是分为三层：数据运营层(ODS层)、数据仓库层(DW)、数据应用层(ADS)

数据分层的主要原因是在管理数据的时候，对数据有一个清晰的掌控，能够为当前的业务提供快速的数据支撑。但也不要为了分层而分层，没有最好，只有合适。

京东数仓分层：BDM -> FDM -> GDM -> ADM，但根据我们的业务，我们只使用了三层， FDM -> GDM -> ADM

**ODS层**

保持数据原貌不做任何修改，起到备份数据的作用。后续数据仓库加工的来源

**DW层**

DW数据分层为DWD、DWM、DWS

DWD:data warehouse details数据细节层，是业务层与数据仓库的隔离层，主要是对ODS数据层做一些数据清洗和规范化操作。

DWM:data warehouse middle数据中间层，在DWD的基础上进行轻微的聚合操作，算出相应的统计指标。

DWS:data warehouse service数据服务层，基于DW的数据做聚合，用于后续的业务应用

**ADS层**

该层主要是提供数据产品和数据分析使用的数据。报表数据一般存在这里。



### 2、为什么要对数仓进行分层？

数据分层的主要原因是在管理数据的时候，对数据有一个清晰的掌控，能够为当前的业务提供快速的数据支撑。但也不要为了分层而分层，没有最好，只有合适。

它有以下优点：

1. 划清层次结构：每一个数据分层都有它的作用域，这样在使用表的时候能更方便地定位和理解
2. 数据血缘追踪：我们最终给下游是直接能使用的业务表，但它的数据源有很多，如果有一张源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害
3. 减少重复开发：开发一些通用的中间层数据，能够减少极大的重复计算



### 3、数据仓库分几层？

最基础的分层思想是三层：ODS层、DW层、ADS层

如果细致划分的话，DW层又划分DWD层、DWM层、DWS层

ODS层：数据源层。直接接入源数据：业务库、埋点日志、消息队列等。保持数据原貌不做任何修改，起到数据备份的作用。

DWD层：数据明细层。它是业务层和数据仓库层的隔离层，主要是对ODS数据层做一些数据清洗和规范化操作。

DWM层：数据中间层。在DWD的基础上进行轻微的聚合操作，算出相应的统计指标

DWS层：数据服务层。在 DWM基础上，整合汇总一个主题的数据服务层。汇总结果一般为宽表。

ADS层：数据应用层。存放在ES、Redis、MySQL等系统中，供数据分析和挖掘使用。



### 4、维度表和事实表

维度表：一般是对事实的描述信息，每一张维度表对应现实世界的一个对象或者概念吧，比如：用户、商品、日期、地区等

事实表：每行数据代表一个业务事件（下单、支付、退款、评价等）。比如2021.11.11，某用户1001号在北京地区买了某商品。



### 5、拉链表

所谓拉链就是记录历史，记录一个事物从开始一直到当前状态的所有变化的信息。链表中有开始时间（start_time）和结束时间（end_time）两个字段，同时有dt和dp两个分区字段，end_time也可做分区；

dp:一般有ACTIVE（线上）和EXPIRED（过期）两个分区。ACTIVE表示数据当前在线上使用，EXPIRED表示数据过期，已变更，为历史状态，其end_time是数据变更时具体的时间。

dt:数据所在的时间分区，记录数据从ACTIVE转移到EXPIRED的日期，即数据发生变更的时间，大部分与end_time一致。拉链表不建议使用dt分区

拉链表处理的业务场景：主要处理缓慢变换维的业务场景（用户表，订单表等）



### 6、关系型数据库范式理论

1NF:原子性，属性不可再分割（即列不可再分割）

2NF:非主键属性依赖于整个主键，而不是部分依赖。

​	例如 主键（学号，课程）-> 成绩，姓名。但 学号 -> 姓名，所以姓名是部分依赖于主键，所以不符合2NF

3NF:非主键属性只依赖于主键，不能依赖于其他非主键属性

MySQL关系模型，主要应用于OLTP系统中，为了保证数据的一致性以及避免冗余，大部分表都遵循3NF

Hive维度模型，主要应用于OLAP系统中，因为关系模型虽然冗余少，但是在大规模数据跨表分析查询中，这会大大降低执行效率。Hive把各种表整理成两种：事实表和维度表两种。



### 7、维度建模

在维度建模上分为三种模型：

星型模型（一级维度表）：它由一个事实表和一组维表组成。每个维表都有一个维作为主键，所以这些维表的主键组合成事实表的主键。事实表的非主键属性称为事实。

![](https://pic1.zhimg.com/80/v2-3ab86050013d9381c1555482df933f58_720w.jpg)

![](https://img-blog.csdn.net/20180821111411563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01lZ3VzdGFzX0pKQw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



雪花模型（多级维度）:当有一个或者多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上。雪花模型是对星型模型的扩展，它对星型模型的维表进一步层次化。

![](https://pic3.zhimg.com/80/v2-fcb612793eadb680a5f35b8f23b9ff7a_720w.jpg)

![](https://img-blog.csdn.net/20180821111435225?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01lZ3VzdGFzX0pKQw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



星座模型（星型模型+多个事实表）:由多个事实表组合，维表是公共的，可以被多个事实表共享。

优缺点：星型模型的设计带来的好处是能够提升查询效率，不必进行大量的join。雪花模型的设计比较符合数据库范式理念，数据冗余少，但在查询的时候可能需要join多张表从而导致查询效率下降。

根据项目经验，一般建议使用星型模型，因为在实际项目中，往往关注的是查询性能问题。



### 8、数据仓库开发流程

1. 数据调研

   - 需求分析
   - 业务调研
   - 数据调研

2. 数据规划（概念模型）

   要做成什么样子。确定主题、分层、事实、维度

3. 模型设计（逻辑模型）

   设计表里有什么字段呀！按照什么模型进行组织，比如星型模型、雪花模型

4. 模型开发（物理模型）

   写代码



## 九、Click house

### 1、Click house是什么？

Click house是一个联机分析处理（OLAP）的列式数据库管理系统，它的一个特点就是快，它很吃 CPU，它的瓶颈也主要体现在CPU上。



### 2、为什么使用ClickHouse?

ClickHouse是一个联机分析处理的列式数据库管理系统，它 除了具备DBMS系统的功能以外，使用它最主要的原因是因为它快。它具备以下核心特性使得它非常快。

1. 它采用列式存储，这对于分析型请求非常高效。比如我们分析仅读取其中的5列，那么通过列存储，我们仅需读取必要的列数据。相比普通的行储存，可减少10倍左右的读取、处理等开销，大大提高了性能。

2. 数据压缩好。采用列存储，相同列的数据连续存储，这样数据的局部规律性非常强，有利于获得更高的数据压缩比。

3. 它向量化执行。向量化执行可以看作消除程序中循环的优化。（就比如现在只有一台榨汁机，榨一个苹果需要1分钟。现在有8个顾客，那需要一个一个苹果来榨，需要榨8分钟。那为了提升榨汁的速度，直接从1台榨汁机提升到8台，此时一分钟就可以榨8杯果汁。非向量化的执行方式是1台榨汁机重复循环制作n次，而向量化执行的方式是用n台榨汁机只执行1次。）

   clickhouse为了实现向量化执行，需要利用cpu的SIMD指令(single instruction multiple data ),即单条指令操作多条数据。简单地说就是通过并行的方式提高性能。

   它的原理是cpu寄存器层面实现数据并行的操作，在存储结构中，存储媒介离CPU越近，数据访问速度越快。

   这也是为什么clickhoue很吃cpu的原因，它的主要瓶颈也体现在cpu上。















