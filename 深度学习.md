# 深度学习

## 机器学习和深度学习本质

本质就是找参数，就是在一个多维的参数空间里面，来找到模型对应的最优的参数，把这个最优的参数给模型，就可以使用这个模型来做分类、回归等任务。



学习基本块怎么做，然后针对我的任务，对这些基本块进行组装，适合你要完成任务的目标。



## 基础知识补充

什么是回归算法？

回归算法是监督型算法的一种，线性回归旨在寻找到一根线，这根线到达所有样本点的距离之和都是最小的。常用在预测和分类领域



## 常见基本函数的用法

**python两大法宝函数**

dir()函数：知道工具箱，以及工具箱里有什么东西

```python
dir(pytorch)
```

help()函数：知道每个工具是如何使用，工具的使用用法

```python
help(pytorch)
```



**pytorch加载数据集**

DataSet类：加载数据集以及对应的label分类

```python
from torch.utils.data import Dateset

# 查看Dataset用法
help(Dataset)
Dataset??

# 定义一个自己的Data类
import	torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
class DiabetesDataset(Dataset):
    def _init_ (self):
    	pass
    def _getitem_ (self,index):
    	pass
    def _len_ (self):
    	pass
    
dataset = DiabetesDataset()
train_loader = DataLoader(dataset = dataset,
                            batch_size = 32,
                            shuffle = True,
                            num_workers = 2)
```



DateSet和DataLoader

epoch表示训练的周期。你的**所有**数据集都进行了一次前向传播和反向传播，这就叫一个epoch。

batch-size表示每次训练的时候所用的样本数量，完成一次前馈传播和反向传播一个更新所用的样本数量。

iteration表示batch分成了几份。

比如有10000个样本，假设batch-size是1000个，那么iteration就是10。



Dataset是抽象类，需要定义自己的类去继承，实现里面的方法（init、getitem、len）

DataLoader是一个类帮我们加载数据的





**深度学习训练过程可视化**

Visdom

画三维图时：matplotlib、np.meshgrid()

指数加权均值作平滑



**pytorch的张量tensor**

使用pytorch的tensor做运算其实就是在构建计算图，使用tensor.backward()会计算权重的梯度，并且会释放这次构建的计算图。

当你只是想获取tensor的数值时，但又不想构建计算图时，可以使用tensor.item()来获取

tensor.item()适用于tensor里只有一个元素时

如果有多个元素可以通过 tensor.numpy()[0]获得

w.grad.data.zero_()表示把权重里的数据都清零，不然在下一次运算中会进行累加。



tensor.detach()和tensor.data。detach()和data生成的都是无梯度的纯tensor，x.data和x.detach()新分离出来的tensor的requires_grad=False

```python
import torch
a = torch.tensor([1,2,3.], requires_grad = True)
print(a)	# tensor([1., 2., 3.], requires_grad=True)
b = a.data
print(b.requires_grad)	# False
print(b)	# tensor([1., 2., 3.])
c = a.detach()	# False
print(c.requires_grad)	# tensor([1., 2., 3.])
print(c)
```

tensor.data还是和之前的数据共享同一份内存，所以data的数据改变，之前的数据也改变

```python
a = torch.tensor([1,2,3.], requires_grad = True)
c = a.data  # 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
print(a) # tensor([1., 2., 3.], requires_grad=True)
c.zero_() # c发生变化，a也会变化
print(a) # tensor([0., 0., 0.], requires_grad=True)
```

tensor.detach()还是和之前的数据共享同一份内存，所以data的数据改变，之前的数据也改变

```python
a = torch.tensor([1,2,3.], requires_grad = True)
c = a.detach()  # 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
print(a) # tensor([1., 2., 3.], requires_grad=True)
c.zero_() # c发生变化
print(a) # tensor([0., 0., 0.], requires_grad=True)
```

tensor.data：由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，但是这种改变对于autograd是没有察觉的，它依然按照求导规则来求导，导致得出完全错误的导数值却浑然不知。它的风险性就是如果我再任意一个地方更改了某一个张量，求导的时候也没有通知我已经在某处更改了，导致得出的导数值完全不正确，故而风险大。

```python
import torch
 
a = torch.tensor([1,2,3.], requires_grad = True)
out = a.sigmoid()
c = out.data  # 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
c.zero_()     # 改变c的值，原来的out也会改变
print(c.requires_grad)
print(c)
print(out.requires_grad)
print(out)
print("----------------------------------------------")
 
out.sum().backward() # 对原来的out求导，
print(a.grad)  # 不会报错，但是结果却并不正确
'''运行结果为：
False
tensor([0., 0., 0.])
True
tensor([0., 0., 0.], grad_fn=<SigmoidBackward>)
----------------------------------------------
tensor([0., 0., 0.])
'''
```

tensor.detach()：由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，这个时候如果依然按照求导规则来求导，由于out已经更改了，所以不会再继续求导了，而是报错，这样就避免了得出完全牛头不对马嘴的求导结果。

```python
import torch
 
a = torch.tensor([1,2,3.], requires_grad = True)
out = a.sigmoid()
c = out.detach()  # 需要走注意的是，通过.detach() “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
c.zero_()     # 改变c的值，原来的out也会改变
print(c.requires_grad)
print(c)
print(out.requires_grad)
print(out)
print("----------------------------------------------")
 
out.sum().backward() # 对原来的out求导，
print(a.grad)  # 此时会报错，错误结果参考下面,显示梯度计算所需要的张量已经被“原位操作inplace”所更改了。

'''
False
tensor([0., 0., 0.])
True
tensor([0., 0., 0.], grad_fn=<SigmoidBackward>)
----------------------------------------------
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
'''
```

总结：

1.x.data和x.detach()都是从原有计算中分离出来的一个tensor变量，生成的都是无梯度的纯tensor,并且通过同一个tensor数据操作，是共享一块数据内存。

2.tensor.data 是不安全的，tensor.detach()是安全的。体现在求导过程中



from torchvision import transform	是对图像做处理的类



## 神经网络

### 概览

神经网络可以当作是能够拟合任何函数的黑盒子，只要训练数据足够，给定特定的x，就能得到希望的y



神经网络的工作就是不断地调节omega的值，让整个权重达到我想要的状态，那这个状态的达到，是通过我一个一个的样本，不断地去调节它去得到的



激活函数的引入一方面是为了将我们的数值规范到[0,1]区间，这样可以避免数值在网络中不断地被放大。另一方面，由于神经网络的加权都是线性运算，如果不引入非线性的激活函数，即便是多层的线性运算，最后也只会等效于一个线性运算。

不加激活函数计算出的结果是这样的

![](https://pic2.zhimg.com/80/7c6e12aed30bf315eed8df6476d7ef7b_720w.jpg?source=1940ef5c)

![](https://pic2.zhimg.com/80/c46188f6f517a15142133129e47d1ae8_720w.jpg?source=1940ef5c)

通过激活函数进行变换

![](https://pic3.zhimg.com/80/32cbeac5eaea9d655b9a50e4d8d0a687_720w.jpg?source=1940ef5c)



最后结果会变成这样

![](https://pic2.zhimg.com/80/3e4d3aabb90f51f467437a17861d3bf7_720w.jpg?source=1940ef5c)

![](https://pic3.zhimg.com/80/fab8a7ae1cb63992f70e160d7f03c067_720w.jpg?source=1940ef5c)

梯度下降的方法去寻找最佳权重

前向传播和反向传播



### 导数/偏导数/梯度/激活函数/loss

**导数**

它反映的是函数在某一点的变化率。

导数是一个标量，它的长度反映了变化率的大小。



**偏微分**

导数处理的是一维函数的变化率，当处理更高维度的变化率时，因此产生了偏微分。

偏微分讲的是一个函数对它的自变量的变化率的描述程度，它也是一个标量。

导数的方向是可以随便指定的，可以指向x，也可以指向y。但是偏微分讲的是给定的自变量的方向。



**梯度**

梯度定义的是所有偏微分的向量。

它就是把所有的偏微分当成一个向量来理解，所以梯度是一个向量，不是一个标量。

梯度的长度某种程度上反映了这个函数的变化趋势（是快还是慢）

梯度的方向代表的是一个函数增长的方向。

因此可以做一个直观的感受：函数的梯度是一个向量，向量的方向代表的是函数在当前点的增长的方向，向量的模代表的是函数在当前这个点的增长的速率。



梯度和导数的区别：导数是给定的方向，梯度是所有方向的综合。对于一维的函数它只有一个方向，因此一维的函数的梯度和导数是一个东西。只不过导数是没有方向的，梯度是有方向的。



梯度下降在深度学习中使用挺少的，一般使用随机梯度下降。（随机就是随机选取一个样本的损失函数的值，不是拿整体损失函数的均值来计算。也就是对每一个样本的梯度进行更新）

为什么使用随机梯度下降？

优点：如果碰到有鞍点的costfuction，那么最后会停留到鞍点那里不动。但是你使用随机的话，样本是有随机噪声的，那么你即使陷入到鞍点，随机噪声会把你向前推动，冲出鞍点的范围，向最优值前进。

缺点：不能并行计算。因为下一次更新的权重值，跟上一次更新的权重值是有关系的，也就是有依赖的。

所以考虑折中的方式，即batch，即批量地进行计算。



梯度的物理意义是函数值增加最快的方向，或者说，沿着梯度的方向更加容易找到函数的极大值；反过来说，沿着梯度相反的方向，更加容易找到函数的极小值。

利用梯度这一性质，在深度学习训练中，可以通过梯度下降法一步步地迭代优化一个事先定义的损失函数，即得到一个较小的损失函数。



**如何利用梯度找到一个极值解？**

一般搜索的是极小值，如果搜索极大值前面添加一个负号就行。

导数大于零是正方向增加，导数小于零是负方向增加。梯度下降就是朝着最快减小的方向走。（本质就是贪心算法，选择当前最好的一个值。但是贪心算法不一定得到最优的结果，他得到的是局部最优，不能证明是全局最优。）

通过这个函数进行寻找 
$$
\theta_t+_1 = \theta_t - \alpha_t\nabla f(\theta_t)
$$
对于凸函数一定可以找到一个全局最小值。



局部最小值和鞍点会影响你的优化器，影响搜索的过程。

鞍点就是梯度为0，这样就会导致值不会再更新，但它又不是最优点。

哪些因素还会影响搜索的过程呢？1.初始状态 2.学习率 3.动量(也就是如何逃离局部最小值)



**激活函数**

什么叫激活函数？

可以理解为一个阈值函数，当大于某个阈值时，他就会激活下一个神经元，它是一个固定的变量，0或1

![激活函数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\激活函数.png)

像这样的激活函数是不可导的，这样的函数我们不能直接使用梯度下降的方法进行优化；

所以出现了启发式搜索的方法来求解单层感知机的一个最优解的情况；

为了解决单层感知机激活函数，激活阶梯函数不可导的情况，提出了一个连续光滑的函数，叫sigmoid

![sigmoid函数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\sigmoid函数.png)

sigmoid导数情况

![sigmoid导数情况](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\sigmoid导数情况.png)

sigmod函数使用的非常多，主要是因为它是连续的光滑的，而且它的值压缩在[0,1]区间。

但是它有缺陷，当你的数值接近无穷时，你的导数接近于零，将会根据梯度公式，值长期得不到更新，出现梯度弥散现象。

pytorch中Sigmoid函数的实现

```python
z = torch.linspace(-100,100,10)
torch.sigmoid(z)
```

Tanh激活函数，在RNN用的比较多,它的值是[-1,1]的区间

![Tanh激活函数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\tanh激活函数.png)

Tanh导数

![Tanh导数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\tanh导数.png)

pytorch中tanh函数实现

```python
z = torch.linspace(-1,1,10)
torch.tanh(z)
```

深度学习中的奠基石的激活函数ReLU激活函数（整型的线性单元）

![RLU激活函数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\RLU激活函数.png)



ReLU导数

![RLU导数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\RLU导数.png)

为什么它特别适合深度学习？

当Z<0时，导数为0；当Z>0时。它的梯度永远是1。因此它在做向后传播的时候，计算梯度非常方便，不会放大，也不会缩小，这样就不会出现梯度弥散和梯度爆炸的情况。对于搜索最优解，存在先天的优势，就是导数计算非常简单

pytorch实现relu函数

```python
z = torch.linspace(-1,1,10)
torch.relu(z)
```

优先使用relu函数，遇到特殊情况，可以尝试使用其他激活函数



Softmax激活函数，适用于多分类，相当于对每个类别求概率，他们最终概率之和是为1的。它能够放大较大的哪个值

![Softmax激活函数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\Softmax激活函数.png)

softmax求导

![softmax求导](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\softmax求导.png)





**loss函数**

作用：损失函数是用来估量模型的预测值f(x)与真实值y的不一致程度，它是一个非负的实值函数，损失函数值越小，模型的鲁棒性就越好。（鲁棒性可以理解为异常或者危险情况下，系统生存的能力）



常见的loss函数：MSE(均方差)

用于分类的：cross entropy loss（交叉熵损失）

1.可以用于二分类的

2.也可以用于多分类的

3.跟softmax激活函数搭配使用的



MSE损失函数

![MSE损失函数](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\MSE函数.png)

MSE损失函数的梯度

![MSE损失函数的梯度](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\MSE损失函数的梯度.png)





$$
\theta 取绝于具体的网络形式，如果\theta=wx + b，则\theta对w求导是x，对b求导是1
$$


pytorch自动求导api

```python
import torch
import torch.nn.functional as F
# 对 pred = xw + b进行求导,初始化 x=1,w=2,b=0
x = torch.ones(1)
w = torch.full([1],2)

# mse_loss第一参数是预测的值，第二个参数是真实值
MSE = F.mse_loss(x*w,torch.ones(1)) # MSE = 1

# 表示MSE对w求导
torch.autograd.grad(MSE,[w])

# 或者使用backward
MSE.backward()
w.grad
```



交叉熵（适合softmax函数的损失函数）

熵表示不确定性，熵值越大不确定性就越大，熵值越小不确定性就越小Z



信息的作用在于消除不确定性。信息量 = logN(N表示等可能事件数量，比如抛硬币，有两种可能，即N=2，那信息量的只需要一个Bit就行，即1表示正面，0表示反面。如果是测试四张花色的牌，则有四种可能，即N=4，那信号量只需要两个bit，即00表示黑桃、01表示红心、10表示花色、11表示方块)或者信息量 = -log(p)(p表示概率)

一个事件的等可能性事件越多，那么传输一个事件的信号量就越大。而等可能性事件越多，意味着哪个事件发生的不确定性越大。

那么事件不是等可能性的如何计算呢？

通过加概率的方式进行均衡，这就是信息熵（pi表示概率，信息量的期望）。信息量其实就是信息熵的一种特例
$$
信息量：I = -log(p_i) \\
信息熵：H = -\Sigma p_iI = -\Sigma p_ilog(p_i)
$$
事件发生的概率越低，那么它含有的信息量就越大，因为它帮我排除的不确定性越大。

信息熵描述的是一个系统内发生一个事件时，这个事件能给你带来的信息量的期望。可以感性地理解为一个系统如果由大量小概率事件构成，那么它的信息熵越大



BCE损失函数
$$
loss = -\frac {1}{N} \Sigma ^N _n=_1 y_nlog \hat y_n + (1-y_n)log(1- \hat y_n)
$$

CrossEntropyLoss

它的过程是：整个输出的数值经过Softmax运算，并经过Log运算，与真实值计算（-YlogY^）

所以如果使用CrossEntropyLoss损失函数时，神经网络最后一层，是不要做激活的，因为激活的那部分包含在CrossEntropyLoss函数里



### 感知机的梯度求导

单层感知机的单输出的梯度求导。t是目标值，E是loss函数

总结：loss函数对当前权值的求导的值跟当前节点x有关系

说明：上标表示第几层。下标表示这一层的第几个节点。w权值下标表示上一层的第几个节点和当前层的第几个节点连接，比如 w10表示上一层的第一个节点和这一层的第0个节点连接



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\单层感知机的梯度求导.png)



代码实现

```python
# x的特征有10个
x = torch.randn(1,10)
w = torch.randn(1,10,requires_grad=True)
o = torch.sigmoid(x@w.t()) # t()表示转置,答案是一个数
loss = F.mse_loss(torch.ones(1,1),o) # 损失函数
loss.backward() # 损失函数对w求导
w.grad
```



单层感知机的多输出的梯度求导

总结：wjk的求导只跟它连接的输入节点和它连接的输出的激活函数有关系

![单层感知机的多个节点的梯度求导](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\单层感知机的多个节点梯度求导.png)

代码实现

```python
import torch
import torch.nn.functional as F
x = torch.randn(1,10) # 1行10列
w = torch.randn(2,10,requires_grad=True) # 2行10列
o = torch.sigmoid(x@w.t()) # @这个符号表示是按照矩阵运算的乘积的形式，不是单纯的对应元素相乘。所以o是一个1行2列的数torch.Size([1, 2])
loss = F.mse_loss(torch.ones(1,2),o)
loss.backward() # 对权值w求导
w.grad # 取出对w求导后的值，2行10列
```



### 链式法则

对应神经网络中，x相当于输入层，u相当于隐含层，y是输出层，也就是预测值

![链式法则](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\链式法则.png)



**神经网络中的链式法则应用**

这样就很清晰明了了，由后往前逐步求导，让计算权值w的导数变得如此简单，妙啊！！！

![神经网络中的链式法则应用](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\神经网络中的链式法则.png)



链式法则验证

```python
# 链式法则验证
x = torch.tensor(1.)
w1 = torch.tensor(2.,requires_grad=True)
b1 = torch.tensor(1.)
w2 = torch.tensor(2.,requires_grad=True)
b2 = torch.tensor(1.)

y1 = x * w1 + b1
y2 = y1 * w2 + b2

# y2对y1求导
dy2_dy1 = torch.autograd.grad(y2,[y1],retain_graph=True)[0]
# y1对w1求导
dy1_dw1 = torch.autograd.grad(y1,[w1],retain_graph=True)[0]
# y2直接对W1求导
dy2_dw1 = torch.autograd.grad(y2,[w1],retain_graph=True)[0]

# 看两者是否相同
dy2_dw1 == dy2_dy1 * dy1_dw1 # 返回true
```

### 反向传播

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\反向传播梯度求导公式.png)



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\反向传播梯度求导1.png)





![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\反向传播求导总结.png)



总结：简单理解就是从后往前求导，先求出当前层的求导信息；然后再往前求出之前层的求导信息；以此往复，就可以求出第一层的权值信息。



### 熵的各种公式

**熵**

熵是一个物理学概念，表示一个系统的不确定性程度，或者一个系统的混乱程度。

熵越大，表示不确定也就越大。

**信息熵**

信息熵也就是熵，只不过它代表领域是信息论这边。就相当于在中国其他地域会叫你帅哥，但在广东会叫你靓仔一样。

**相对熵**

相对熵=KL散度（相对熵就是KL散度）

KL散度：是两个概率分布间差异的非对称性度量。即KL散度是用来衡量同一个随机变量的两个不同分布之间的距离。
$$
D_KL(p||q) = \Sigma p(x_i)log(\frac {p(x_i)} {q(x_i)})
$$
特性：

1.非对称性。DKL(p||q) != DKL(q||p)，只有p和q的概率分布完全一样才相等

2.非负性。DKL(p||q)恒大于等于0，p和q概率分布完全一样才相等0



计算

![KL散度](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\KL散度.png)

KL散度公式变形。KL散度 = 交叉熵 - 信息熵

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\KL散度公式变形.png)



**交叉熵**

交叉熵：主要用于度量同一个随机变量X的预测分布Q与真实分布P之间的差距

差距可以理解为距离、误差、失望值、混乱程度等。

交叉熵公式

![交叉熵](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\交叉熵公式.png)



交叉熵例子

结论：

1.交叉熵越小，预测越准确。

2.交叉熵只跟真实标签的预测概率值有关

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\交叉熵例子.png)

交叉熵最简公式
$$
Cross \_ Entropy(P,Q) = - log Q(c_i)
$$
交叉熵的二分类公式。也叫BCE损失函数

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\交叉熵的二分类公式.png)

注意：普通公式、最简化公式、二分类公式计算二分类问题，算出交叉熵结果是一样的



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\交叉熵做损失函数.png)

交叉熵损失函数
$$
loss = -\frac {1}{N} \Sigma ^N _n=_1 y_nlog \hat y_n + (1-y_n)log(1- \hat y_n)
$$




**softmax**

softmax是什么？

1.将数字转换成概率的神器

2.进行数据归一化的利器



公式

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\softmax公式.png)

损失函数。只跟真实值为1的有关系，其他都是零
$$
Loss(\hat Y,Y) = - Y log \hat Y
$$
**sigmoid**

1.sigmoid函数也叫logistic函数

2.取值范围(0,1)

3.神经网络常用的激活函数

4.常被用于二分类

公式
$$
S(x) = \frac {1}{1 + e^{-x}}
$$




### 杂谈

连续的值通常都叫做regression,sigmoid函数也称为regression

把矩阵看成一种空间变换的函数。

神经网络其实就是在寻找非线性变换的一种空间变换的过程

学习能力必须要有泛化能力才是最好的，如果学习能力特别强，那么噪声数据也学到了，这是我们不想要的

[0,1]分布给神经网络训练效果是最好



### 构建神经网络的步骤

1. 准备数据集，有 X 值 和 Y值(真实值)

2. 设计模型，计算得出 y^ (y hat，预测值)

3. 构造损失函数和优化器。最后计算出来的loss值是标量，也就是一个值。

4. 训练模型（向前反馈、向后传播、更新权重）

   ![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\神经网络构建步骤.png)



增量式开发的思想去写程序，不要一上来就把所有的网络都构建好，不然将来出了问题很难找到问题的根源



### 数据预处理

**标准分数**



$$
Z = \frac {x - \mu}{\sigma}
\\
x表示的是原始的值 \\
\mu 表示的是平均值 \\
\sigma 表示的是标准差
$$
从数学的角度去理解：

标准分数的大小说明当前值大小在全体值的位置，用来说明在所属那批数值中的相对位置。

比如：假设x表示考生的原始分数。原始分数只能反映考生作答正确的程度，但是原始分一般不能反映考生间的差异状况，因此才有了标准分数的出现。



从神经网络的角度理解：

减去均值：数据进入神经网络前，提取特征，提取的特征是要代表每个类别的特点，也就是神经网络更关注的是类别之间的差异。

除以方差：减去均值之后的数值范围可能变化很大，比如图像像素可能在0到255之间晃动，如果不做处理，这些特征输入到神经网络中，会表现出有些地方变化大，有些地方变化小，这样对网络影响是比较大的。我们希望的是关注网络整体的变化，而不是局部的变化影响输出。如果除以标准差，就可以控制像素取值范围，比如[0,1]之间



### 深度学习的过程

1.填充深度学习理论，数学和工程。《深度学习》、《动手学深度学习》

2.阅读pytorch文档，通读一遍

3.复现经典论文。读代码 -> 写代码 -> 读代码...循环往复，直到自己能实现出来

4.在有前三面的基础上，通过不断地阅读论文，扩充视野，当你阅读的论文多了，想法自然而然会从脑海里浮现出来。



### 阅读论文步骤

1.标题 + 作者

2.摘要

3.结论

4.导言

5.相关工作

6.模型

7.实验

8.评论



## 卷积神经网络

**为什么需要卷积神经网络**

像全连接网络那样计算，如果维度很大的话将会造成数据计算爆炸。引入卷积，就可以减少相应的维度，降低运算量。卷积核就相当于特征提取器，在减少数据量的同时还提取了特征，使网络的计算更好



**卷积神经网络是怎样减少参数的**

1.参数共享。相比全连接网络，一个参数对应一个像素特征，卷积网络中有卷积核，与特征之间不是一一对应的（比如9个参数隐射为一个特征），实现了参数共享

2.稀疏连接。比如，我们看输入图像左上角的9个像素和卷积核进行卷积之后，对应得到的特征图中的左上角的1个像素（这个像素只会受到输入图像左上角的9个像素的影响，不会受到其他像素的影响）

### 卷积

卷积的物理意义：如果有一个系统，输入是不稳定的，输出是稳定的，那么可以用卷积来求系统的存量。

数学公式：

*表示卷积操作
$$
f(x) * g(x) = \int_{-\infty}^{\infty} f(x)g(t-x)dx
$$
f函数就是不稳定的输入

g函数就是稳定的输出

他们两相乘求的就是系统的存量



另一种理解：之前的每一个时刻（位置、距离）对当前时刻的影响，他都要进行计算。



总结：

1.不稳定输入、稳定输出，求系统存量

2.周围像素点如何产生影响

3.一个像素点如何试探





### 图像卷积

卷积神经网络的主要用途就是用它去识别图片里的内容。

狭义理解：

图像的卷积操作就是拿图片和卷积核（比如2*2的矩阵或者3 * 3的矩阵）先相乘再相加。

所以图像是f函数，卷积核是g函数。因为图像里的像素点总是在变换，f函数对应的就是系统不稳定的输入。而卷积核是不变的，g函数对应的是系统稳定的输出。

广义理解：

卷积核的理解：他其实就是规定，周围的像素点是如何对当前的像素点产生影响的。

其实g函数不是卷积核，而是要旋转180度之后才是卷积核，只是计算的过程中把旋转的步骤省略了。

其中的关键就是要把卷积当作是过去对现在的影响，周围像素点对当前像素点的影响。

而g函数就是规定了如何影响的关键

新的理解：

卷积就是对周围像素点的一个主动的试探和选择，通过卷积核把周围有用的特征给保留下来



### 池化层

池化层的作用夹在连续的卷积层中间，用于压缩数据和参数的量，减少过拟合。

换言之，如果输入是图像的话，那么池化层最主要的作用是压缩图像。

简单理解就是降维，减少数据量的同时还保持数据的特征



Subsampling（下采样）

下采样的目的是减少下一层的数据量，降低运算的需求



下采样也叫池化层，池化层的方法主要有MaxPooling和AveragePooling

maxpooling是减少宽度和高度



### 卷积神经网络

**第一层在干什么？跟卷积有什么关系**

卷积神经网络的第一步就是把局部特征给挑出来，把这些局部特征交给神经网络。



卷积层的输入和输出，只跟通道和卷积核大小有关；而全连接层是和你变换之后数据大小有关



为什么卷积神经网络权重会比较少呢？这是因为权重是共享的。他不是和每一个元素都建立联系，而是一个区域的元素建立联系



卷积神经网络使用Relu函数作为激活函数更多一点





卷积以及下采样叫做特征提取



**图像卷积运算**

channel为一层的运算,就是输入变量和卷积核做数乘，然后相加

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel1-1.png)



然后不断移动方框，计算出所有的值

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel1最后结果.png)

channel为3的话需要和3个卷积进行数乘

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel3.png)



卷积的channel要和输入的channel对应上

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel3对应上.png)



如何把输出channel从1变成m呢？则需要m个卷积，每个卷积的channel等于输入的channel，计算完后进行拼接

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\拼接卷积.png)



简单卷积的过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\卷积的运算过程.png)





### GoogleNet

**解决了什么**

提高大规模图像分类和检测精度



**使用方法**

提出了Inception神经网络结构，该架构能够很好地减少计算量，同时增加了网络的深度和宽度。

Inception就是把多个卷积或池化操作，放在一起组装成一个网络模块，设计神经网络时以模块为单位去组装整个网络结构。

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\Inception.png)

Inception为什么做成这样？

有些超参数，比如kernel是 3 * 3 的还是 5 * 5的，你要选哪个卷积核比较好用，这就比较难。所以它的思想就是你不知道哪个比较好用，那我在一个块里把这几种卷积都用一下，然后把结果拼接在一起。将来，如果 3 * 3 的比较好用，那么 3 * 3的权重会变得比较大，而其他路线上的权重相对就会变小。所以它提供了几种候选的神经网络的配置，然后通过训练它自动地找到卷积最优的组合。



Inception计算过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\Inception计算过程.png)





**1 * 1的卷积的作用**

可以将C1的通道数改为C2的通道数。降低运算量，也可以简单理解为一种降维的方式。

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\1-1卷积核.png)





### 残差网络

残差网络是为深度神经网络隐藏层过多时的网络退化问题而提出的。所谓的网络退化就是梯度逐渐趋于0（梯度消失），导致权重不会再变化。

![](https://image.jiqizhixin.com/uploads/editor/fdc85c5b-039a-4edd-bfa5-7aeb01684134/1541508117061.jpeg)



当z=F(x)+x时，求导，它不是在0附近的导数，而是在1附近的导数，若干个这样的数相乘，就能够非常好地去解决梯度消失的问题。

既然F(x)和x要做加法，即那两层的输出要和x的输入张量维度必须是一样的（即通道、宽度、高度都得一样）



## 循环神经网络

### RNN

**为什么需要循环神经网络**

全连接神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理**序列**的信息，即前面的输入和后面的输入是有关系的。



循环神经网络里使用tanh作为激活函数更多一点



RNN是对时序数据的建模

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN神经网络.png)

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN神经网络2.png)



**RNN Cell**



RNN Cell 的本质就是一个线性层，能把某一个维度映射到另外一个维度里的空间里。

这个线性层和平常的线性层有什么不一样呢？它是共享的。



把输入的张量维度和隐藏层的张量维度搞清楚了，RNN就会变得非常容易

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN Cell.png)

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN Cell实现方式.png)

**RNN使用**

inputs是所有x1、x2、...序列的和

out是全部隐层的结果

hidden是最后一个隐层的结果，有几层就有几个

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN 使用.png)

**numLayers**

numLayers表示有几层RNN

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\numLayers.png)



反向传播过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN 梯度如何训练.png)



**pytorch中RNN的使用**

x[seq len,batch,feature len]

seq len：表示有几个单词,即单词的数量

batch：表示有几句话，即句子的数量

feature len：表示有一个单词有几维



![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN model使用.png)



input_size：是单词的维度

hiden_size：是指我的memory多长来表达，即h得维度

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\输出out.png)



**RNN梯度消失的有什么不同？**

RNN梯度消失和普通网络不太一样，RNN的梯度是一个总的梯度和，它的梯度消失并不是变为0，而是说总梯度被近距离梯度主导，被远距离梯度忽略不计。可以使用LSTM缓解远距离梯度消失



### Bidirectional RNN

双向RNN,不仅只考虑了前面的输入，后面的输入也考虑。

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\BidirectionalRNN.png)



### LSTM

**基本原理**

长短期记忆网络是比较复杂的RNN，它有3个gate。

1.当外界的某个神经网络的输出想要写到memory cell里面的时候，他必须先通过一个闸门，叫作input gate。这个input gate被打开的时候，你才能够把值写入到memory cell里面。如果他被关起来的时候，其他神经网络就没有办法把值写入到memory cell里面。那至于input gate是打开还是关闭，这个是神经网络自己学的，也就是说它自己学习要什么时候把它打开，什么时候把它关闭。

2.输出的地方也有一个output gate，他决定外界的神经网络可不可以从memory cell里面取值，当它关闭的时候，就没有办法把值读出来。

3.第三个gate叫做forget gate。它决定什么时候memory cell要把过去记得的东西忘掉，或者是把过去记得东西做一下格式化。forget gate打开的时候表示记得，关闭的时候表示遗忘



LSTM有4个输入，1个输出。所以LSTM所需要的参数量是RNN的四倍

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\RNN\LSTM.png)

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\LSTM2.0.png)



LSTM Example

输入3维数据，输出1维

当x2=1的时候，input gate就会打开更新memory的值，当x2=-1的时候，memory就会重置

当x3=1的时候，output gate就会打开

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\RNN\LSTM exp.png)



**为什么被提出？**

因为RNN只会记得前一个时间点的事情，但是LSTM的话，它可以记得比较长一点或者比较久一点的事情，只要forget gate不进行格式化，它的值就会存起来



### GRU

门控循环单元，效果跟LSTM差不多，但是更轻量，也就是参数更少。

并不是所有的关注都很重要，有些很重要，有些可以忘记等。

它有两个门，一个是更新门，一个是重置门。

更新门：这个数据比较重要，我需要用它尽量去更新我的隐藏状态

重置门：可以忘掉某些东西。



## 注意力机制

**本质**

注意力机制最早是在图像领域提出的，类似于人类视觉，就是在众多信息中把注意力集中在一个点上，选出关键信息，忽略其他不重要的信息。



注意力机制核心是关注重点信息，而忽略掉不重点的信息。



**什么叫注意力机制？**

为了解决序列到序列模型记忆长序列能力不足的问题，当生成一个目标语言单词时，不光考虑前一个时刻的状态和已经生成的单词，还考虑当前要生成的单词和源语言句子中的哪些单词更相关，即更关注源语言哪些词，这种做法就做注意力机制。



### 注意力机制

**公式**

Q、K、V三个矩阵
$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}})V
$$
Q、K点乘是为计算Q和K哪个跟接近，距离越大越接近，表示越关注

参数说明：

Q:query(查询)

K:key(键)

V:value(值)



**K为什么要转置？**

这是因为Q和K的维度是相同的，只有转置了才可以相乘（矩阵乘法）。而且转置了之后才能表示这些查询和这些关键字一一对比的效果。



**为什么使用softmax?**

Q和K乘出来的是一个实数值，要想把它变成一个权重或者说要想把它变成一个相似性，我们得把他们转化到0到1之间，把它转化成一种概率，转化成一种权重，所以这边使用了一个softmax。



**为什么除以根号dk呢？**

1.Q、K相乘值很大，softmax在做反向传播的时候，梯度很小，梯度很小的话，就会造成梯度的消失。我们知道梯度小的话，那么你的训练就会很慢，所以为了避免这种情况把点积的结果除以根号dk

2.除以根号dk也是为了保持控制方差为1

3.transformer原论文中这个值是64，所以根号下d_k为8



**淘宝例子讲解注意力机制**



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\淘宝例子讲解注意力机制.png)







### 自注意力机制（Self-attention）

**什么叫自注意力机制**

受注意力机制启发，当要表示序列中某一个时刻的状态时，可以通过该状态与其他状态之间的相关性(注意力)计算。



**注意力机制和自注意力机制的区别**

注意力机制的Q和KV是分布在不同区域的，Q在target区域，而KV在Source区域。

自注意力机制机制的QKV分布在同一个区域，要么都在target区域，要么都在Source区域。





**它解决了什么问题**

通过自注意力机制，可以直接计算两个距离较远的时刻之间的关系。而神经网络中，由于信息是沿着时刻逐层传递的，因此当两个相关性较大的时刻距离较远时，会产生较大的信息损失。虽然引入了门控机制模型，如LSTM等，可以部分解决这种长距离依赖的问题，但是治标不治本。因此注意力机制的自注意力模型已经逐步取代了循环神经网络，成为自然语言处理的标准模型。



自注意力机制的作用是查找一句话内部的关系

比如这its可能代表这个Law，也可能代表application

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\自注意力机制的解释.png)



**self-attention是怎么运作的呢？**

self-attention会计算一整个sequence，你输入几个向量，它就输出几个向量。

输出的向量有什么特别的地方吗？输出的向量考虑了一整个sequence才得到的，即考虑了上下文语义。

self-attention不是只可以用一次，他可以用多次。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention1.png)



这里a是输入向量，b是输出向量。b的输出是考虑了每一个输入向量的

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention2.png)

b1向量是如何产生的呢？(b2、b3、b4类似)

a1先跟每一个向量计算出相关性

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention3.png)

相关性怎么计算呢？

把输入的两个向量分别乘以不同的矩阵，得到q和k两个值，然后将q和k作点积

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention4.png)



具体的过程

计算每一个向量之间的关联性。这边演示的是计算a1跟其他的关联性

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention5.png)



计算出每一个向量的关联性之后，接下来会做一个Softmax，得出a'

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention6.png)



得到a'之后，基于a'抽取出这Seq中重要的资讯。

如何抽取呢？我们会把a1到a4乘以一个V矩阵，得到一个新的向量v1、v2、v3、v4。然后各自乘以他们的a'，加起来

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention7.png)



如何从a得到Q、K、V呢？

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention8.png)

Q、K矩阵相乘，得到attention分数，再经过Softmax



w是如何来的？

w是参数，权重矩阵，是通过learn，学习得到的。一开始是初始化的，然后通过不断训练调整，最后达到最优，使得模型效果最好。



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention9.png)



为什么使用矩阵来进行运算？

矩阵乘法实现了并行计算



接下来就是通过V和A'相乘得到输出向量b

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention10.png)





**self-attention就是一些列矩阵相乘，唯一需要学习的权重就是W，其他参数都是已知的**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention11.png)

**CNN和Self-attention的关系**

CNN是Self-attention的特例，Self-attention只要设定合适的参数，它可以做到和CNN一模一样的效果。

CNN可以看作是简化版的self-attention，因为在做CNN的时候我们只考虑边解范围内的资讯，而在做self-attention的时候，我们是考虑整张图片的资讯。



**self-attention跟CNN的比较**

从以下图我们可以发现，随着资料量的越来越多，结果就会越来越好。最终在资料量最多的时候。self-attention可以超过CNN。在资料量更少的情况下，CNN是可以得到比self-attention更好的结果。

为什么会这样？可以从弹性来进行解释。

self-attention弹性比较大，所以需要更多的资料和训练量，训练资料少的时候就会over-fitting（过度拟合）

而CNN弹性比较小，在训练资料比较少的时候结果比较好，在训练资料多的时候他没有办法从更大量的训练中得到好处。



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention vs CNN.png)



**RNN和Self-attention的区别**

1.对RNN最右边的输出要考虑最左边的输入，那么它必须把最左边的输入存到memory里面，然后接下来都不能忘掉，一路带到最右边，才能在最后一个时间点被考虑。但对于self-attention来说，没有这个问题，它只要输出一个query，那边输出一个key，只要他们能够match，天涯若比邻，可以从非常远的vector轻易地抽取资讯。

2.RNN不能并行计算，它能够并行计算，处理所有的输出。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\RNN和Self-attention.png)



### 多头注意力机制（Multi-head Self-attention）

**是什么**

多头注意力机制就是对注意力机制的简单堆叠

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\多头注意力机制.png)



**为什么需要多头注意力？**

1.想要寻找不同方面的相关性，一个q和一个k相乘，是一个相关性；另一个q和另一个k相乘，是另一个相关性；

因此我们需要多个q，不同q负责不同种类的相关性。

2.不同的头可以提取到不同的特征，提取到不同的信息，提取到的信息越多，准确率就越高。

3.多头机制赋予attention多种子表达方式，每一组都是随机初始化，经过训练后，输入向量可以被映射到不同的子表达空间中。简单来说，多头就相当于我们可以在多个参数空间里面寻找参数，这样准确率跟效率肯定更高。



只有一组qkv的叫单头注意力，有多组qkv的叫多头注意力



**原理**

Q、K、V首次经过一个线性变换，然后输入到放缩点积Attention（Scaled Dot-Product attention）中，进行h次计算，即多头。然后将h次的放缩点积Attention结果进行拼接，再进行一次线性变换得到的值作为多头Attention的结果

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\多头注意力机制.png)



**多头注意力过程**

怎么得到不同的q呢？你可以用a乘以一个矩阵得到q，然后将q和另外两个矩阵相乘，得到q1和q2；k和v同理；

接下来的操作就是和上面一样，只不过q、k、v只和自己相关的相乘，即q1、k1、v1相乘，q2、k2、v2相乘；最后把输出的b1、b2接起来和一个矩阵相乘，得到最后输出的b

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention12.png)

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention13.png)

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\self-attention\self-attention14.png)







## BERT 和 Transformer

### BERT 和 Transformer的关系

Transformer是特征提取器，和CNN、RNN并列用于特征抽取的一种深层级网络结构。

BERT 可以视为两阶段处理流程，这个流程使用的框架便是Transformer,简单理解就是，BERT利用Transformer学会了如何编码、存储信息。

BERT其实就是Transformer里面的Encoder

**BERT**

BERT由两阶段构成，每个阶段有自己的特点和目标。第一个阶段是预训练阶段，第二个阶段是Fine-Tuning阶段。

预训练阶段用大量无监督的文本通过自监督方式进行训练，把文本包含的语言知识以参数形式编码到Transformer中，Fine-Tuning一般是有监督的，数据量比较小，在模型结构上做分类任务以解决当前任务。

简单地理解，在预训练阶段Transformer学到了很多初始化的知识，第二阶段就是把初始化网络学到的语言知识拿来用。



**Transformer**

Transformer是一个典型的层级结构



**seq2seq**

输入一个序列，输出也一个序列。输出序列的长度由模型决定。



### Transformer

#### Transformer结构图

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\结构1.png)

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\结构2.png)

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\结构3.png)

Transformer是一个典型的层级结构，Transformer由若干个Block堆叠而成，作为基本构件，每一个Block是一个小生态系统，里面涉及到很多技术，最关键的是4个部分：Layer Norm、Skip-Connection、自注意力机制和前馈神经网络

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\结构4.png)



#### Encoder

Encoder具体细节过程



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Encode过程.png)



Encoder大致可以分为三个部分，输入部分、注意力机制、前馈神经网络

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Encoder部门.png)

输入部分分为Embeding和位置嵌入。



**Embedding**

Embedding在数学上表示就是一个mapping，`f:X->Y`

每一个Y只有唯一的X对应，并且是结构保存的，即在X空间中，X1<X2，那么映射到Y空间时，Y1<Y2的。



Word Embedding也是如此，就是找一个映射或者函数，生成一个新的空年间上的表达。在NLP中，word embedding会给每一个词一个向量，而这个向量是有语义的，如果把word embedding可视化的话，你会发现所有动物可能聚集在一块，所有植物聚集在一块。

Word Embedding的优势：

1.它可以将文本通过一个低维向量来表示，不像 one-hot 那么长

2.语义相似的词在向量空间上也比较接近

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\text_as_data\WordEmbedding.png)



Embedding的过程：

1.嵌入层Embedding会给不同的单词分配一个索引

2.接下来会创建嵌入矩阵，我们要决定每一个索引分配多少维向量，一般分配32和50

比如我们现在有四个索引，每个索引是6维向量，索引1表示deep，索引2表示learning，索引3表示is，索引4表示very，创建好嵌入矩阵

![](https://image.jiqizhixin.com/uploads/editor/07addd29-d822-413e-9694-928e09ac6da6/640.png)

我们就可以使用嵌入矩阵而不是庞大的one-hot编码向量来保持每个向量更小。简而言之，嵌入层embedding在这里做的就是把单词“deep”用向量[.32, .02, .48, .21, .56, .15]来表达。然而并不是每一个单词都会被一个向量来代替，而是被替换为用于查找嵌入矩阵中向量的索引。



**为什么需要位置编码**

Transformer不同于RNN，在处理数据时不考虑数据的位置信息，所以需要在数据中加入位置信息，让处于不同位置的相同数据有所不同，相互区分。







**位置编码公式**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\位置编码公式.png)

说明：

pos表示词的位置，代表的是行数

i表示的是列数

2i表示维度中偶数的位置，2i+1表示维度中奇数的位置。偶数位置使用sin，奇数位置使用cos



得到位置编码512个维度，和词向量的512维度相加，得到一个512维度的信息作为整个Transformer的输入

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\词向量+位置编码.png)





**为什么位置嵌入是有用的？**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\线性组合.png)

可以这么理解：“我爱你”。“你”的位置信息，可以被不同奇偶位置的“我”和"爱"表示出来。



#### 注意力机制



**公式**

Q、K、V三个矩阵
$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt {d_k}})V
$$
Q、K点乘是为计算Q和K哪个跟接近，距离越大越接近，表示越关注

参数说明：

Q:query(查询)

K:key(键)

V:value(值)



为什么除以根号dk呢？

1.Q、K相乘值很大，softmax在做反向传播的时候，梯度很小，梯度很小的话，就会造成梯度的消失。

2.除以根号dk也是为了保持控制方差为1



**过程**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Attention过程.png)



获取q、k的过程

q1 = x1 * Wq

k1 = x1 * Wk

v1 = x1 * Wv

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\获取QK的过程.png)

计算Q、K相似度，得到attention的值

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\计算得到Attention的值.png)

实际代码使用的是矩阵，方便并行。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Transformer矩阵.png)

1.先把词转化成词向量，然后把词向量转化成QKV





#### 残差和layer Norm

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\结构4.png)

**残差的作用**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\残差网络求导.png)





残差网络是为深度神经网络隐藏层过多时的网络退化问题而提出的。所谓的网络退化就是梯度逐渐趋于0（梯度消失），导致权重不会再变化。当z=F(x)+x时，求导，它不是在0附近的导数，而是在1附近的导数，若干个这样的数相乘，就能够非常好地去解决梯度消失的问题。



**Layer Norm**

为什么使用Layer Normalization?而不是使用Batch Normalization?

BN在NLP中的效果很差

BN理解的重点在于它是针对batch中样本在同一维度特征在做处理。

LN针对的是对同一个样本的所有单词做缩放。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\例子.png)

以上面为例，BN是对“我 今”、“爱 天”、“中 天”...做均值方差

而LN是对整个句子做均值方差。显然BN是不合常理的方式，因为同一句话里才是有语义关系。



#### Decoder

**详解实例过程**

1.先输入一个开始符号，经过decoder，再经过softmax会输出概率的distribution的值，这个size的大小等于你的词汇集大小，会选择最大的概率输出，即输出机。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\decoder1.png)

2.把机作为输入，输出最大概率的器。以此类推

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\decoder2.png)

3.也就是说上一个输出会作为下一个的输入进行训练

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\decoder3.png)



**decoder结构**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\decoder结构.png)



![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\decoder.png)

**masked多头注意力是怎样的，或者说怎么理解**

selft-attention的输出都是看过每一个输入之后才做的决定,输出b1的时候是看过a1、a2、a3和a4的输入之后才做的决定。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\masked1.png)

当它把self-attention转化成masked self-attention的时候，它的不同点在于不能再看右边的部分。也就是说产生b1的时候只考虑a1的资讯，产生b2的时候只考虑a1、a2的资讯，不能再考虑a3和a4。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\masked2.png)

讲得更具体就是,以b2举例，拿2的q2和a1的k和a2的k进行计算，不考虑a3和a4

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\masked3.png)



**为什么要Masked？**

可以简单理解为在做测试的时候，是不知道后面的信息。你训练的时候是知道，但你测试的时候是没有后面信息的，为了防止训练和预测之间有gap，所以需要一个mask，来掩盖后面的信息

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\masked.png)



**encoder和decoder之间是如何传递资讯的**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\cross-attention1.png)

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\cross attention.png)



decoder产生一个q，去encoder这边抽取相应的kv资讯出来，输出的内容v当作接下来full connected里面的Input



**如何交互**

Encoder所有输出的值和每一个Decoder做交互

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Encoder交互.png)

具体交互过程：

Encoder输出生成的是KV矩阵，Decoder是生成Q矩阵。多头注意力机制一定是有QKV三个矩阵

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Decoder过程1.png)

虚线代表KV矩阵的输出

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Transformer\Decoder过程2.png)



#### Transformer面试题讲解

### BERT

**全称**

Bidirectional Encoder Representations from Transformers

#### BERT整体结构

BERT的基础架构只使用了Transformer的Encoder部分，使用了多个Encoder堆叠在一起，记住是Encoder堆叠在一起。

BERT base使用了12层的Encoder，BERT large使用了24层的Encoder。



BERT基本模型结构由多层Transformer构成，包含两个预训练任务：掩码语言模型(Masked Language Model MLM)和下一个句子预测(Next Sentence Prediction NSP)

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\BERT整体结构.png)



#### 输入表示

BERT输入表示由词向量、块向量和位置向量之和组成

BERT的Input = token emb + segment emb + position emb

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\BERT输入表示.png)



SEP是一个分割符号，表示句子的分隔。

CLS向量不能代表整个句子的语义信息。





#### 基本预训练任务（如何对BERT做预训练：MLM + NSP）

BERT基本预训练任务由掩码语言模型和下一个句子预测构成。

BERT在预训练的时候使用大量的无标注语料，所以在预训练设计的时候一定要靠无监督来做，因为是没有标签的。

对于无监督目标函数，有一两组常用的目标函数。

1.AR autoregressive，自回归模型；只考虑单侧信息，典型的就是GPT（Generative Pre-Training，它基于 Transformer 架构，GPT模型先在大规模语料上进行无监督预训练、再在小得多的有监督数据集上为具体任务进行精细调节（fine-tune）的方式。先训练一个通用模型，然后再在各个任务上调节，这种不依赖针对单独任务的模型设计技巧能够一次性在多个任务中取得很好的表现）

2.AE autoencoding，自编码模型；从损坏的输入数据中预测重建原始数据。可以使用上下文信息，Bert就是使用的AE

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\AR和AE.png)



**1.掩码语言模型(MLM)**

掩码语言模型出现的原因：

传统基于条件概率建模的语言模型只能从左至右或者是从右至左（逆序）建模文本序列。如果同时进行顺序建模和逆序建模文本，则会导致信息泄露。顺序建模表示根据“历史”的词预测“未来”的词。与之相反，逆序建模是根据“未来”的词预测“历史”的词。如果对上述两者同时建模则会导致在顺序建模时“未来”的词已被逆序建模暴露

为了真正实现文本的双向建模，即当前时刻的预测同时依赖于“历史”和“未来”，BERT采用了类似于完形填空的做法，即掩码语言模型(MLM)



在BERT中，采用了15%的掩码比例，即输入序列中15%的WordPieces子词被掩码。当掩码时，模型使用 [MASK]标记替换原单词以表示该位置已被掩码。然而，这样会造成预训练阶段和下游任务精调阶段之间的不一致性，因为人为引入的 [MASK]标记并不会在实际的下游任务中出现。为了缓解这个问题，当对输入序列掩码时，并非总是将其替换为[MASK]标记，而会按概率选择以下三种操作中的一种：
• 以80%的概率替换为 [MASK]标记；
• 以10%的概率替换为词表中的任意一个随机词；
• 以10%的概率保持原词不变，即不替换。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\MLM.png)

mask这个模型打破文本原有信息，在预训练的时候让他做文本的重建。





**2.下一个句子预测(NSP next sentence prediction)**

除了MLM任务，BERT还引入了第二个预训练任务——下一个句子预测（NSP）任务，以构建两段文本之间的关系。NSP任务是一个二分类任务，需要判断句子B是否是句子A的下一个句子



在NSP任务中，BERT使用了[CLS]位的隐含层表示进行分类预测。具体地，[CLS]位的隐含层表示由
上下文语义表示h的首个分量h0构成，因为[CLS]是输入序列中的第一个元素。在得到[CLS]位的隐含层表示h0后，通过一个全连接层预测输入文本的分类概率。



NSP样本如下：

1.从训练语料库中取出两个连续的段落作为正样本

2.从不同的文档中随机创建一对段落作为负样本

缺点：主题预测和连贯性预测合并为一个单项任务。



#### 如何训练BERT

**最普通的做法**

1.获取谷歌中文BERT

2.基于任务数据进行微调



映射到毕业论文上的做法：

1.直接使用Hugging face的Longformer模型

2.基于自己的游记文本数据进行微调



**比较好的做法**

比如做微博文本情感分析：

1.在大量通用的预料上训练一个LM(language model) - pertrain 这是一个预训练过程，通常直接使用已经训练好的模型，比如谷歌中文BETT

2.在相同领域上继续训练LM。 - domain transfer 领域相关训练。 比如在大量微博文本上继续训练这个BERT

3.在任务相关的小数据上继续训练LM。- task transfer 任务相关训练。在第二个步骤中，大量微博文本其实有很多不属于我这个任务相关的，不属于微博情感分析的数据，所以这一步没必要用到他们。我们更聚焦于跟文本情感分析相关的数据上。

4.在任务相关的数据上做具体任务 - fine tune 微调模型。



先 damain transfer 再进行 task transfer 最后 fine tune 性能是最好的



映射到自己的毕业论文上的步骤：

1.直接使用Hugging face上面的Longformer中文模型

2.拿没有标记的旅游文本数据继续训练Longformer

3.拿旅游评论的数据继续训练

4.在自己标注好的一百份游记文本数据中进行微调模型。



**如何在相同领域数据中进行further pre-trainning**

1.动态mask。就是每次epoch去训练mask,而不是一直使用同一个。BERT在训练的时候其实是使用固定的mask，就是把文本mask之后存在本地，然后每次训练的时候都是使用的是同一个文件。比如我爱吃饭，每次训练的时候都mask了这个吃，这样是不太好的。而动态mask，就是每次epoch训练之前，对数据做mask，相当于每次epoch训练的时候单词是不一样的，不是一直使用同一个文件。

2.n-gram mask：其实比如ERNIE和SpanBERT都是类似做了实体词的mask



**参数设置**

Batch Size：16，32，64，128影响不太大，看机器性能

Learning rate：5e-5，3e-5，2e-5，尽可能小一点避免灾难遗忘

Number of epochs：3~4 （微调的时候）

Weighted decay 修改后的adam,使用 warmup，搭配线性衰减。



训练的时候可以做数据的增强（就是做简单的EDA）、自蒸馏、外部知识的融入（比如融入一些知识图谱的知识），这些都是比较吃机器。



**灾难性遗忘**

是指新的数据集上训练模型，会遗忘旧数据上学习到的知识，在旧数据上测试会发生很大的掉点。



#### BERT代码实现

BERT代码最核心的一点是：MLM损失函数的计算



两个重点

第一个重点：BERT只使用了transformer的编码部分

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\BERT编码部分.png)



再进一步细化

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\BERT编码部分1.png)





![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\BERT编码部分2.png)

原始标签：代表是字符对应的索引。0是PAD符号，1是CLS符号，2是SEP...

mask之后：挑选15%的字符进行mask，按照8：1：1的比例进行

embedding：每次词汇对应的维度是768

我们把第一个字符也就是CLS，接linear层去做二分类任务。把mask掉的位置输出是768维度，接linear层在词表范围内做softmax(BERT应该是22128个词)，挑选出最有可能的词汇，然后去做损失。

补0的原因是为了不参与损失函数的计算。

MLM损失函数对15% mask的词汇都计算了损失。



#### BERT处理长文本

**方法**

bert支持的最长序列长度为512。处理评论，标题等短文本基本上够用了，但对于较长的文本，比如新闻正文，会经常出现超出字符限制的情况。

**1.截断法**

截断法是非常常用办法，大致分为三种，head截断，tail截断，head+tail 截断。

```
1.head截断即从文本开头直到限制的字数。
2.tail截断是从结尾开始往前截断。
3.head+tail 截断，开头和结尾各保留一部分，比例参数是一个可以调节超参数。
```

最长序列字数是512，其中还包括一些特殊token，在文本分类中，要包含开头的[CLS]和结尾的[SEP]，因此实际只能最多装510个字。如果一个长文本的重要信息是在开头，可能head截断效果是比tail截断要好。同理，tail截断对信息点在结尾的长文本效果较好。具体哪种截断效果好，不同数据集不一样，需要多试。使用head+tail 截断，一般而言是好于单一的截断方式。

截断法丢失序列信息，显得非常暴力，一般使用在文本不是特别长的场景。如果是篇章级，文本长度好几千，如果直接使用截断法，必然会丢失大量信息。因此面对这种场景，首先想到的是“拆”。

**2.Pooling法**

将一个整段的文本拆分为多个segment，每一个segment的长度小于510。segment的拆分可以暴力地通过510的大小进行chunk，或者通过断句的方式，将相邻的句子放入一个segment。每一个segment都通过BERT，对得到的[CLS]进行Pooling。可以是用Max-Pooling、Mean-Pooling。亦或将 Max-Pooling、Mean-Pooling进行concat，然后再通过一个FC。如果考虑性能、只能使用一个Pooling的话，就使用Max-Pooling，因为捕获的特征很稀疏、Max-Pooling会保留突出的特征，Mean-Pooling会将特征打平。这一点和TextCNN后接Max-Pooing是一个道理。

Pooling法将所有序列都放入模型之中。考虑到了全局的信息，对文本很长且截断敏感的任务有较好的效果。但有一些缺点

1. 性能较差，原来截断法需要encode一次，Pooling法需要encode多次，篇章越长，速度越慢。
2. segment之间的联系丢失，可能会出badcase。

**3.压缩法**

压缩法的宗旨是选取“精华”，去除“糟粕”。断句之后整个篇章分割成segment，通过规则或者训练一个小模型，将无意义的segment进行剔除。

[CogLTX : bert处理长文本代码解析](https://mp.weixin.qq.com/s/yj6F5YAK9pJeEpTbKFJPMw)

[github地址](https://github.com/Sleepychord/CogLTX)

Transformer-XL也是处理长文本的



##### CogLTX

**基础知识**

CogLTX:Congnize Long TeXts like human(像人类一样认知长文本)



SOTA(state-of-the-art 最先进的)

SOTA model：state-of-the-art model（最先进的模型），并不是特指某个具体模型，而是指在该项研究任务中，目前最好/最先进的模型

SOTA result：state-of-the-art result，指在该项研究任务中，目前最好的模型的结果/性能/表现



MLP：多层感知机，也叫人工神经网络



**研究问题**

BERT不能处理长文本，由于其空间和时间复杂度呈平方级增长。



**研究方法**

CogLTX会训练一个评分模型(判断模型)来识别关键的句子，然后串联它们进行推理，并且通过排练(重复)和衰减实现多步骤推理



**研究结论**

CogLTX在训练的过程中只需要固定的内存，就可以使遥远的句子之间集中注意力。（简单理解应该就是能够获取到更远距离的句子信息）



**CogLTX的不足**

CogLTX是在假设关键句子的情况下定义了一个用于长文本理解的通道，但是极端困难的序列级任务会违反它，因此有效的变分贝叶斯方法(估计z的分布)仍然值得研究。

此外，CogLTX还有一个缺点，在区块之前错过了先行项，但是可以在每个句子前加上实体名可以缓解这个问题，并且在未来可以通过位置感知检索竞争或共引用解析来解决这个问题。



**引言**

BERT是一个很棒的预训练模型，它构建了很多先进的应用程序，实验的文本长度都没超过BERT。但并不是所有的文本都不会超过BERT的限制(BERT能处理的序列是512)，真实环境中有很多很长很复杂的文本。



一个简单的解决办法就是使用滑动窗口，这种方法会牺牲长距离之间词的“注意力”，这也会成为BERT的瓶颈。问题的根源在于Transformers的高时间和空间复杂度(O(n^2))。

另一个研究试图简化Transformers的结构，但目前很少人把它成功地应用到BERT



BERT的最大长度限制提醒了我们有限容量的工作记忆，那人类是怎样理解长文本的呢？

最新研究表明工作记忆的内容是会随着时间的推移而衰减，除非通过排练(重复)，即通过集中注意力并刷新头脑中信息。然后检索比较，用长期记忆中的相关条目不断更新被忽略的信息，在工作的记忆中收集足够的信息进行推理。



CogLTX-Congize Long TeXT like human，像人类一样认知长文本，它的原理很简单，就是通过串联关键句子进行推理。



CogLTX的关键步骤是MemRecall，通过将相关文本块作为情景记忆来识别相关文本块的过程，简单来说就是通过相关性去识别相关的东西。它会去模拟工作记忆的过程：检索比较、排练(重复)和衰退。

还有一个BERT，叫judge，它是用来评分块的相关性，它会与最初的BERT reasoner一起训练。



**背景介绍**

长文本挑战。对长文本直接和表面的障碍是在BERT中预训练的最大位置嵌入是512。然而其实提供了更大的位置嵌入，内存消耗的计算也是难以承受的，因为所有的激活都存储在训练期间的反向传播中。



相关工作。滑动窗口缺乏远距离关注，先前的工作试图通过mean-pooling、max-pooling或者额外的MLP或者LSTM来聚合每个窗口的结果。但这些方法在长距离交互时任然很弱。



在对transformer进行长文本的研究中，很多只是压缩或者重复使用了之前步骤的结果，无法应用到BERT中，如Transformer-XL和Compressive Transformer。Reformer使用位置敏感的散列来实现基于内容的群组关注，但它对GPU不友好，仍然需要对BERT的使用进行验证。BlockBERT[35]砍掉不重要的注意力头，将BERT从512 token提升到1024。最近的里程碑Longformer，定制CUDA内核以支持窗口注意和全局注意的特殊tokens。然而由于数据集大多在Longformer窗口大小的4倍范围内，它的有效性和研究还不够充分。



**详细方法过程**

**1.方法论**

CogLTX的基本假设“对于大多数NLP任务来说，文本中的几个关键句子存储了足够和必要的信息来完成任务”。更具体地说，假设存在一个由长文本 x 中的一些句子组成的短文本 z，满足


$$
reasoner(X^+) \approx reasoner(Z^+)
$$




![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\MemRecall.png)

task a：范围提取任务，例如回答问题

task b：序列级任务，例如文本分类

task c：基于标记的任务，例如POS标记

MemRecall是从长文本x中提取关键文本块z的过程，然后将z发送给BERT，称为reasoner，以完成特定的任务。



切分文本成块使用的是动态编程的方法，z中的所有块都会自动排序，以保持x中的原始相对顺序。

参数：

Punctiation costs:每一个标点符号的损失

basic cost:“硬截断”的基本损失

max block size:最大块的长度

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\Block Split.png)



在CogLTX中，MemRecall和两个BERTs的联合训练是必不可少的。MemRecall利用判断模型(评分模型)检索关键块算法，然后在推理过程中将关键块送入reasoner完成任务。



**2.MemRecall**

MemRecall的目标是从长文本 x 中提取关键块 z。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\MemRecall2.png)

用于回答问题的MemRecall。长文本 x 被分成[x0...x40]。在第一步中，排练后的x0和x8保留在 z 中。x8中的“Old School”有助于在下一个步骤中检索答案块x40。



**输入**。尽管其目的是提取关键块，但三种任务的具体设置有所不同。在task a、c任务中，问题Q或子序列x[i]作为查询，用于检索相关块。但是，(b)中没有查询，相关性仅由训练数据隐式定义。那如何统一呢？



MemRecall通过接受初始的z+作为x之外的附加输入来回答这个问题。z+是在MemRecall期间维护的简短“关键文本”，用来模拟工作记忆。任务(a)(c)中的查询成为z+中的初始信息，以引发回忆。然后，判断模型在z+的帮助下学习预测任务特定的相关性。



**模型**。MemRecall使用的唯一模型是上面提到的 judge，一个BERT来为每个标记评分相关性。

假设z+ = [[CLS] Q [SEP]z0[SEP]…zn-1),
$$
judge(z^+) = sigmoid(MLP(BERT(z^+))) \in (0,1)^{len(z^+)}
$$

区块zi的分数记为judge(z+)[zi]，是该区块中token分数的平均值



**过程**。MemRecall始于一场检索比赛，每个块xi分配一个粗关联分数 judge(z+)[zi]，得分高的模块插入到 z 中，它优于向量空间模型的原因在于 xi 通过 transformer与当前的z+完全相互作用，避免了嵌入过程中的信息丢失。



接下来的排演衰减期赋予每个zi一个良好的相关性评分judge(z+)[zi]。只有得分最高的区块会被保存在z+中。

细分值的动机是，没有分块之间的交互和比较，粗分值的相对大小不够准确，类似于重新计算的动机



MemRecall本质上支持多步骤推理，通过使用新的z+重复这个过程。如果z+中新块的更多信息证明它们的相关性不够强(得分较低)，上一步保留的块也会衰减，而先前的多步推理方法忽略了这一点（也就是前人的研究并没有考虑到这一点）。



参数：

strides：每一步将保留多少新块

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\MemRecall算法.png)



**3.训练**

下游任务的多样性对CogLTX训练BERTs提出了挑战。算法1总结了不同设置下的解。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\算法1.png)

**3.1 对judge进行监督训**

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\super_train_judge.png)

训练样本z要么是从x中采样的连续块Zrand序列(对应检索比赛的数据分布)，要么是所有相关和随机选择的不相关块Zrelv的混合(近似于排练的数据分布)。



**3.2 对reasoner进行监督训练**

reasoner面临的挑战是如何在训练和推理过程中保持数据分布的一致性，这是监督学习的一个基本原则。理想情况下，reasoner的输入也应该由MemRecall在训练期间生成，但并不能保证所有相关块都能被检索到。例如在回答问题时，如果MemRecall漏掉了答案块，训练就不能进行。最后，近似地发送检索竞赛中所有相关块和“优胜者”块来训练reasoner。



**3.3 对judge进行无监督训练**

很多任务都没有提供相关性标签。因为CogLTX假设所有相关的块都是必要的，所以我们通过干预推断出相关标签:通过从z中删除块来测试块是否是不可缺少的。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\CogLTX\unsuper_train_judge.png)

其中Z-zi是z去除了zi的结果，t是一个阈值。在训练reassoner的每一次迭代后，我们切割z中的每个块，根据损失的增加调整其关联标签。不显著的增加表明区块是不相关的，它可能不会再次“赢得检索比赛”来训练reasoner在下一个迭代，因为它将被贴上不相关的标签来训练judge。然后真正相关的块可能进入z下一个迭代并被检测出来。


**3.4 与潜在变量模型的连接**





问题：

1.judge和reasoner是两个BERT模型吗？judge和reasoner的具体过程？

2.Retrieval competition做的工作是哪些？它的过程是怎样的

3.Rehearsal-Decay的过程是怎样的？

4.论文Method Training模块不太懂

5.Block Split算法的过程。输入的Punctuation costs cost，basic cost 是什么？



## 预训练模型

### 概览

#### What is pre-train model



#### How to fine-tune



#### How to pre-train





### 自然语言基础

**概念**

所谓模型预训练(Pre-train)，即首先在一个原任务上预先训练一个初始模型，然后在下游任务（也称目标任务）上继续对该模型进行微调（Fine-tune）,从而提高下游任务准确率的目的。



现有的神经网络在训练时，一般是基于向后传播算法，先对网络中的参数随机初始化，再利用梯度下降等优化算法不断优化模型参数。

而预训练的思想是，模型参数不再是随机初始化，而是通过一些任务进行预训练，得到一套模型参数，然后用这套模型参数对模型初始化，再进行训练。



**点互信息**

点互信息可以解决高频词误导计算结果的问题。如果一个词与很多词共现，则降低其权重；反之，如果一个词只与个别词共现，则提高其权重。对于词w和上下文c，PMI计算公式为
$$
PMI(w,c) = \log_2 \frac{P(w,c)}{P(w)P(c)} 
$$
P(w,c)、P(w)、P(c)分别是w与c共现概率、w的概率、c的概率



**词向量**

为什么要做词向量？或者说为什么要做embedding?

1.抽象的事物应该有一个低维的表示

2.计算机和神经网络善于处理低纬度信息。

3.解决one-hot编码问题。one-hot编码是一 种特殊的高维到低维的映射，具有稀疏性，且向量长度较长并随着样本集变化而变化。one-hot编码无法表示两个实体之间的关系，embedding在一定程度上可以解决这个问题

简单地说，embedding的目标是寻找一个长度固定的向量，它可以表示一个实体的"本质"



词向量又分为静态词向量和动态词向量，静态词向量指的是一个单词不管上下文如何变化只有一个唯一的词向量表示，所以它最大的缺点是无法表达多意性，动态词向量指的会根据上下文动态适应性的调整词向量，可以一定程度上解决单词多意性。



一般而言，动态词向量会归为预训练模型，利润bert模型。但严格来说，静态和动态词向量都是一种预训练思想，即第一阶段训练词向量，第二阶段根据使用预训练的词向量并根据是否进行Fine-tuning。



**马尔可夫假设**

对一个词的预测只与历史中最近的n-1个词相关。



**过拟合**

过拟合：指一个假设在训练数据上能够获得比其他假设更好地拟合（训练误差小），但是在训练数据外的数据集上却不能很好地拟合数据（测试误差大）。此时模型的泛化能力较差，不利于推广。



过拟合产生的原因：训练数据中存在噪音，或者训练数据太少。



### 长文本处理

以自注意力机制为核心的 Transformer 模型是各种预训练语言模型中的重要组成部分。

自注意力机制能够构建序列中各个元素之间的上下文程度，挖掘深层次语义。然后自注意力机制的时空复杂度为O(n^2)，即时间和空间消耗会随着输入序列的长度呈平方级增长。这种问题的存在使得预训练语言模型处理长文本效率较低。



传统处理长文本的方法一般是切分输入文本，其中每份的大小设置为预训练语言模型能够单次处理的最大长度（如512）。最终将多片文本的决策结果进行综合（如对分类结果进行投票）或者拼接（如序列标注或生成任务）得到最终结果。然而，这种方法不能很好地构建文本块之间的联系，挖掘长距离文本依赖的能力较弱。因此，更好的方法还是需要从根本上提高预训练语言模型单次能够处理的最大文本长度，从而能够更加充分地利用自注意力机制。



四个有代表性地处理长文本序列的 Transformer 的变种：Transformer-XL、Reformer、Longformer和BigBird。



#### Longformer

基于稀疏注意力机制的模型-Longformer。Longformer讲输入文本序列的最大长度，提出了三种稀疏注意力模式(Sparse Attention Pattern)降低计算复杂度，分别是滑动窗口注意力机制、扩张滑动窗口注意力和全局注意力。

![](D:\github\MyKnowledgeRepository\img\machine_learning_img\deep_learning\BERT\Longformer注意力机制.png)



**1.滑动窗口注意力机制**

在多数情况下，当前词只会与其相邻的若干个词存在一定的关联，因此对所有的词进行自注意力的计算存在一定的信息冗余。在Longformer中引入了一种固定长度的滑动窗口注意力机制，使得每个词只会与其相邻的k个词（以当前词为中心，左右窗口长度均为k/2）计算注意力。滑动窗口注意力机制可以将自注意力计算的时空复杂O(n^2)降低至O(nk)，即与输入序列的长度n呈线性关系。



这种滑动窗口机制与卷积神经网络类似。在卷积神经网络中，虽然初始的卷积核可能很小，但可以通过多个卷积层的叠加，最终获得整个图像的特征信息。同理，虽然通过上述滑动窗口方法计算出的注意力值是局部的，但可以通过多层Transformer模型将局部信息叠加，从而获取到更长距离的依赖信息。具体地，在一个L层的Transformer模型中，最顶层的感受野（Receptive Field）是L× k（此处假设每一层的窗口大小 k 是固定的）。图8.14（b）给出了一个窗口大小为6的滑动窗口示例，即每个单词（对角线）只会与其前3个和后3个之间的词计算注意力。



**2.扩张滑动窗口注意力**

在滑动窗口中，增加窗口大小k可以使当前词利用到更多上下文信息，但也会增加计算量。为了解决上述问题，Longformer还引入了一种扩张滑动窗口方法。该方法借鉴了卷积神经网络中的扩张卷积（Dilated Convolution）[2]。在扩张滑动窗口中，并不是利用窗口内所有的上下文单词信息，而是引入了扩张率（Dilation Rate）d，即每
间隔d−1采样一次。在一个L层的Transformer模型中，给定一个固定的扩张率d和窗口大小k，最顶层的感受野是L×d×k。



这里结合图8.14（c）理解扩张滑动窗口机制。首先，从计算复杂度来看，窗口大小为12（即扩张率d=2）的扩张滑动窗口方法与窗口大小为6的普通滑动窗口方法是相同的，即每个词只会与前后各3个词计算注意力（深色部分）。而由于扩张滑动窗口采用了间隔采样方法，每个词可以利用到更长的上下文信息，最远可以利用距离当前词6个单位的单词。



**3.全局注意力**

在预训练语言模型中，对于不同类型的任务，其输入表示也是不同的。例如，在掩码语言模型中，模型利用局部上下文信息预测被掩码的单词是什么；在文本分类任务中，通常使用[CLS]位的表示预测类别；对于问答或阅读理解等任务来说，将问题和篇章拼接后，通过多层Transformer学习两者之间的联系。



然而，前面提出的滑动窗口方法无法学习到任务特有的表示模式。因此，Long-former引入了全局注意力方法特别关注一些预先选定的位置，使这些位置能够看到全局信息。图8.14（d） 给出了一个全局注意力和滑动窗口结合的例子。可以看到，对于序列中的第1、2、6和16位的单词，其整行整列的信息都可以被看到。这意味着该词可以利用整个序列的信息，同时整个序列在计算注意力时都能看到当前的词。因此，全局注意力机制是一个对称的操作。



当实际应用时，可以根据任务特点设置全局注意力要关注的位置。例如，在文本分类任务里，可以将[CLS]设置为“全局可见”；在问答类任务里，可以将所有的问题中的单词设置为“全局可见”。由于全局可见的单词数量远小于序列长度，局部窗口（滑动窗口）和全局注意力整体的计算复杂度仍然是O(n)



**研究背景**

Transformer因为其自注意力操作和要处理的序列呈二次方的关系，因此不能长序列文本的问题



**研究问题**

为了解决Transformer的限制，引入Longformer，它带有随序列长度线性伸缩的注意力机制。



## 论文

### 如何去制造创新性

![](D:\github\MyKnowledgeRepository\img\paper\如何制造创新性.png)





