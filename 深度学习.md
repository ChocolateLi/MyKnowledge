# 深度学习

学习基本块怎么做，然后针对我的任务，对这些基本块进行组装，适合你要完成任务的目标。

## 常见基本函数的用法

**python两大法宝函数**

dir()函数：知道工具箱，以及工具箱里有什么东西

```python
dir(pytorch)
```

help()函数：知道每个工具是如何使用，工具的使用用法

```python
help(pytorch)
```



**pytorch加载数据集**

DataSet类：加载数据集以及对应的label分类

```python
from torch.utils.data import Dateset

# 查看Dataset用法
help(Dataset)
Dataset??

# 定义一个自己的Data类
import	torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
class DiabetesDataset(Dataset):
    def _init_ (self):
    	pass
    def _getitem_ (self,index):
    	pass
    def _len_ (self):
    	pass
    
dataset = DiabetesDataset()
train_loader = DataLoader(dataset = dataset,
                            batch_size = 32,
                            shuffle = True,
                            num_workers = 2)
```



DateSet和DataLoader

epoch表示训练的周期。你的**所有**数据集都进行了一次前向传播和反向传播，这就叫一个epoch。

batch-size表示每次训练的时候所用的样本数量，完成一次前馈传播和反向传播一个更新所用的样本数量。

iteration表示batch分成了几份。

比如有10000个样本，假设batch-size是1000个，那么iteration就是10。



Dataset是抽象类，需要定义自己的类去继承，实现里面的方法（init、getitem、len）

DataLoader是一个类帮我们加载数据的





**深度学习训练过程可视化**

Visdom

画三维图时：matplotlib、np.meshgrid()

指数加权均值作平滑



**pytorch的张量tensor**

使用pytorch的tensor做运算其实就是在构建计算图，使用tensor.backward()会计算权重的梯度，并且会释放这次构建的计算图。

当你只是想获取tensor的数值时，但又不想构建计算图时，可以使用tensor.item()来获取

tensor.item()适用于tensor里只有一个元素时

如果有多个元素可以通过 tensor.numpy()[0]获得

w.grad.data.zero_()表示把权重里的数据都清零，不然在下一次运算中会进行累加。



tensor.detach()和tensor.data。detach()和data生成的都是无梯度的纯tensor，x.data和x.detach()新分离出来的tensor的requires_grad=False

```python
import torch
a = torch.tensor([1,2,3.], requires_grad = True)
print(a)	# tensor([1., 2., 3.], requires_grad=True)
b = a.data
print(b.requires_grad)	# False
print(b)	# tensor([1., 2., 3.])
c = a.detach()	# False
print(c.requires_grad)	# tensor([1., 2., 3.])
print(c)
```

tensor.data还是和之前的数据共享同一份内存，所以data的数据改变，之前的数据也改变

```python
a = torch.tensor([1,2,3.], requires_grad = True)
c = a.data  # 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
print(a) # tensor([1., 2., 3.], requires_grad=True)
c.zero_() # c发生变化，a也会变化
print(a) # tensor([0., 0., 0.], requires_grad=True)
```

tensor.detach()还是和之前的数据共享同一份内存，所以data的数据改变，之前的数据也改变

```python
a = torch.tensor([1,2,3.], requires_grad = True)
c = a.detach()  # 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
print(a) # tensor([1., 2., 3.], requires_grad=True)
c.zero_() # c发生变化
print(a) # tensor([0., 0., 0.], requires_grad=True)
```

tensor.data：由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，但是这种改变对于autograd是没有察觉的，它依然按照求导规则来求导，导致得出完全错误的导数值却浑然不知。它的风险性就是如果我再任意一个地方更改了某一个张量，求导的时候也没有通知我已经在某处更改了，导致得出的导数值完全不正确，故而风险大。

```python
import torch
 
a = torch.tensor([1,2,3.], requires_grad = True)
out = a.sigmoid()
c = out.data  # 需要走注意的是，通过.data “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
c.zero_()     # 改变c的值，原来的out也会改变
print(c.requires_grad)
print(c)
print(out.requires_grad)
print(out)
print("----------------------------------------------")
 
out.sum().backward() # 对原来的out求导，
print(a.grad)  # 不会报错，但是结果却并不正确
'''运行结果为：
False
tensor([0., 0., 0.])
True
tensor([0., 0., 0.], grad_fn=<SigmoidBackward>)
----------------------------------------------
tensor([0., 0., 0.])
'''
```

tensor.detach()：由于我更改分离之后的变量值c,导致原来的张量out的值也跟着改变了，这个时候如果依然按照求导规则来求导，由于out已经更改了，所以不会再继续求导了，而是报错，这样就避免了得出完全牛头不对马嘴的求导结果。

```python
import torch
 
a = torch.tensor([1,2,3.], requires_grad = True)
out = a.sigmoid()
c = out.detach()  # 需要走注意的是，通过.detach() “分离”得到的的变量会和原来的变量共用同样的数据，而且新分离得到的张量是不可求导的，c发生了变化，原来的张量也会发生变化
c.zero_()     # 改变c的值，原来的out也会改变
print(c.requires_grad)
print(c)
print(out.requires_grad)
print(out)
print("----------------------------------------------")
 
out.sum().backward() # 对原来的out求导，
print(a.grad)  # 此时会报错，错误结果参考下面,显示梯度计算所需要的张量已经被“原位操作inplace”所更改了。

'''
False
tensor([0., 0., 0.])
True
tensor([0., 0., 0.], grad_fn=<SigmoidBackward>)
----------------------------------------------
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
'''
```

总结：

1.x.data和x.detach()都是从原有计算中分离出来的一个tensor变量，生成的都是无梯度的纯tensor,并且通过同一个tensor数据操作，是共享一块数据内存。

2.tensor.data 是不安全的，tensor.detach()是安全的。体现在求导过程中



from torchvision import transform	是对图像做处理的类



## 神经网络

### 概览

神经网络可以当作是能够拟合任何函数的黑盒子，只要训练数据足够，给定特定的x，就能得到希望的y



神经网络的工作就是不断地调节omega的值，让整个权重达到我想要的状态，那这个状态的达到，是通过我一个一个的样本，不断地去调节它去得到的



激活函数的引入一方面是为了将我们的数值规范到[0,1]区间，这样可以避免数值在网络中不断地被放大。另一方面，由于神经网络的加权都是线性运算，如果不引入非线性的激活函数，即便是多层的线性运算，最后也只会等效于一个线性运算。

不加激活函数计算出的结果是这样的

![](https://pic2.zhimg.com/80/7c6e12aed30bf315eed8df6476d7ef7b_720w.jpg?source=1940ef5c)

![](https://pic2.zhimg.com/80/c46188f6f517a15142133129e47d1ae8_720w.jpg?source=1940ef5c)

通过激活函数进行变换

![](https://pic3.zhimg.com/80/32cbeac5eaea9d655b9a50e4d8d0a687_720w.jpg?source=1940ef5c)



最后结果会变成这样

![](https://pic2.zhimg.com/80/3e4d3aabb90f51f467437a17861d3bf7_720w.jpg?source=1940ef5c)

![](https://pic3.zhimg.com/80/fab8a7ae1cb63992f70e160d7f03c067_720w.jpg?source=1940ef5c)

梯度下降的方法去寻找最佳权重

前向传播和反向传播



### 导数/偏导数/梯度/激活函数/loss

**导数**

它反映的是函数在某一点的变化率。

导数是一个标量，它的长度反映了变化率的大小。



**偏微分**

导数处理的是一维函数的变化率，当处理更高维度的变化率时，因此产生了偏微分。

偏微分讲的是一个函数对它的自变量的变化率的描述程度，它也是一个标量。

导数的方向是可以随便指定的，可以指向x，也可以指向y。但是偏微分讲的是给定的自变量的方向。



**梯度**

梯度定义的是所有偏微分的向量。

它就是把所有的偏微分当成一个向量来理解，所以梯度是一个向量，不是一个标量。

梯度的长度某种程度上反映了这个函数的变化趋势（是快还是慢）

梯度的方向代表的是一个函数增长的方向。

因此可以做一个直观的感受：函数的梯度是一个向量，向量的方向代表的是函数在当前点的增长的方向，向量的模代表的是函数在当前这个点的增长的速率。



梯度和导数的区别：导数是给定的方向，梯度是所有方向的综合。对于一维的函数它只有一个方向，因此一维的函数的梯度和导数是一个东西。只不过导数是没有方向的，梯度是有方向的。



梯度下降在深度学习中使用挺少的，一般使用随机梯度下降。（随机就是随机选取一个样本的损失函数的值，不是拿整体损失函数的均值来计算。也就是对每一个样本的梯度进行更新）

为什么使用随机梯度下降？

优点：如果碰到有鞍点的costfuction，那么最后会停留到鞍点那里不动。但是你使用随机的话，样本是有随机噪声的，那么你即使陷入到鞍点，随机噪声会把你向前推动，冲出鞍点的范围，向最优值前进。

缺点：不能并行计算。因为下一次更新的权重值，跟上一次更新的权重值是有关系的，也就是有依赖的。

所以考虑折中的方式，即batch，即批量地进行计算。





**如何利用梯度找到一个极值解？**

一般搜索的是极小值，如果搜索极大值前面添加一个负号就行。

导数大于零是正方向增加，导数小于零是负方向增加。梯度下降就是朝着最快减小的方向走。（本质就是贪心算法，选择当前最好的一个值。但是贪心算法不一定得到最优的结果，他得到的是局部最优，不能证明是全局最优。）

通过这个函数进行寻找 
$$
\theta_t+_1 = \theta_t - \alpha_t\nabla f(\theta_t)
$$
对于凸函数一定可以找到一个全局最小值。



局部最小值和鞍点会影响你的优化器，影响搜索的过程。

鞍点就是梯度为0，这样就会导致值不会再更新，但它又不是最优点。

哪些因素还会影响搜索的过程呢？1.初始状态 2.学习率 3.动量(也就是如何逃离局部最小值)



**激活函数**

什么叫激活函数？

可以理解为一个阈值函数，当大于某个阈值时，他就会激活下一个神经元，它是一个固定的变量，0或1

![激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\激活函数.png)

像这样的激活函数是不可导的，这样的函数我们不能直接使用梯度下降的方法进行优化；

所以出现了启发式搜索的方法来求解单层感知机的一个最优解的情况；

为了解决单层感知机激活函数，激活阶梯函数不可导的情况，提出了一个连续光滑的函数，叫sigmoid

![sigmoid函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\sigmoid函数.png)

sigmoid导数情况

![sigmoid导数情况](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\sigmoid导数情况.png)

sigmod函数使用的非常多，主要是因为它是连续的光滑的，而且它的值压缩在[0,1]区间。

但是它有缺陷，当你的数值接近无穷时，你的导数接近于零，将会根据梯度公式，值长期得不到更新，出现梯度弥散现象。

pytorch中Sigmoid函数的实现

```python
z = torch.linspace(-100,100,10)
torch.sigmoid(z)
```

Tanh激活函数，在RNN用的比较多,它的值是[-1,1]的区间

![Tanh激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\tanh激活函数.png)

Tanh导数

![Tanh导数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\tanh导数.png)

pytorch中tanh函数实现

```python
z = torch.linspace(-1,1,10)
torch.tanh(z)
```

深度学习中的奠基石的激活函数ReLU激活函数（整型的线性单元）

![RLU激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RLU激活函数.png)



ReLU导数

![RLU导数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RLU导数.png)

为什么它特别适合深度学习？

当Z<0时，导数为0；当Z>0时。它的梯度永远是1。因此它在做向后传播的时候，计算梯度非常方便，不会放大，也不会缩小，这样就不会出现梯度弥散和梯度爆炸的情况。对于搜索最优解，存在先天的优势，就是导数计算非常简单

pytorch实现relu函数

```python
z = torch.linspace(-1,1,10)
torch.relu(z)
```

优先使用relu函数，遇到特殊情况，可以尝试使用其他激活函数



Softmax激活函数，适用于多分类，相当于对每个类别求概率，他们最终概率之和是为1的。它能够放大较大的哪个值

![Softmax激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\Softmax激活函数.png)

softmax求导

![softmax求导](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\softmax求导.png)





**loss函数**

作用：损失函数是用来估量模型的预测值f(x)与真实值y的不一致程度，它是一个非负的实值函数，损失函数值越小，模型的鲁棒性就越好。（鲁棒性可以理解为异常或者危险情况下，系统生存的能力）



常见的loss函数：MSE(均方差)

用于分类的：cross entropy loss（交叉熵损失）

1.可以用于二分类的

2.也可以用于多分类的

3.跟softmax激活函数搭配使用的



MSE损失函数

![MSE损失函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\MSE函数.png)

MSE损失函数的梯度

![MSE损失函数的梯度](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\MSE损失函数的梯度.png)





$$
\theta 取绝于具体的网络形式，如果\theta=wx + b，则\theta对w求导是x，对b求导是1
$$


pytorch自动求导api

```python
import torch
import torch.nn.functional as F
# 对 pred = xw + b进行求导,初始化 x=1,w=2,b=0
x = torch.ones(1)
w = torch.full([1],2)

# mse_loss第一参数是预测的值，第二个参数是真实值
MSE = F.mse_loss(x*w,torch.ones(1)) # MSE = 1

# 表示MSE对w求导
torch.autograd.grad(MSE,[w])

# 或者使用backward
MSE.backward()
w.grad
```



交叉熵（适合softmax函数的损失函数）

熵表示不确定性，熵值越大不确定性就越大，熵值越小不确定性就越小



信息的作用在于消除不确定性。信息量 = logN(N表示等可能事件数量，比如抛硬币，有两种可能，即N=2，那信息量的只需要一个Bit就行，即1表示正面，0表示反面。如果是测试四张花色的牌，则有四种可能，即N=4，那信号量只需要两个bit，即00表示黑桃、01表示红心、10表示花色、11表示方块)或者信息量 = -log(p)(p表示概率)

一个事件的等可能性事件越多，那么传输一个事件的信号量就越大。而等可能性事件越多，意味着哪个事件发生的不确定性越大。

那么事件不是等可能性的如何计算呢？

通过加概率的方式进行均衡，这就是信息熵（pi表示概率，信息量的期望）。信息量其实就是信息熵的一种特例
$$
信息量：I = -log(p_i) \\
信息熵：H = -\Sigma p_iI = -\Sigma p_ilog(p_i)
$$
事件发生的概率越低，那么它含有的信息量就越大，因为它帮我排除的不确定性越大。

信息熵描述的是一个系统内发生一个事件时，这个事件能给你带来的信息量的期望。可以感性地理解为一个系统如果由大量小概率事件构成，那么它的信息熵越大



BCE损失函数
$$
loss = -\frac {1}{N} \Sigma ^N _n=_1 y_nlog \hat y_n + (1-y_n)log(1- \hat y_n)
$$

CrossEntropyLoss

它的过程是：整个输出的数值经过Softmax运算，并经过Log运算，与真实值计算（-YlogY^）

所以如果使用CrossEntropyLoss损失函数时，神经网络最后一层，是不要做激活的，因为激活的那部分包含在CrossEntropyLoss函数里



### 感知机的梯度求导

单层感知机的单输出的梯度求导。t是目标值，E是loss函数

总结：loss函数对当前权值的求导的值跟当前节点x有关系

说明：上标表示第几层。下标表示这一层的第几个节点。w权值下标表示上一层的第几个节点和当前层的第几个节点连接，比如 w10表示上一层的第一个节点和这一层的第0个节点连接



![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\单层感知机的梯度求导.png)



代码实现

```python
# x的特征有10个
x = torch.randn(1,10)
w = torch.randn(1,10,requires_grad=True)
o = torch.sigmoid(x@w.t()) # t()表示转置,答案是一个数
loss = F.mse_loss(torch.ones(1,1),o) # 损失函数
loss.backward() # 损失函数对w求导
w.grad
```



单层感知机的多输出的梯度求导

总结：wjk的求导只跟它连接的输入节点和它连接的输出的激活函数有关系

![单层感知机的多个节点的梯度求导](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\单层感知机的多个节点梯度求导.png)

代码实现

```python
import torch
import torch.nn.functional as F
x = torch.randn(1,10) # 1行10列
w = torch.randn(2,10,requires_grad=True) # 2行10列
o = torch.sigmoid(x@w.t()) # @这个符号表示是按照矩阵运算的乘积的形式，不是单纯的对应元素相乘。所以o是一个1行2列的数torch.Size([1, 2])
loss = F.mse_loss(torch.ones(1,2),o)
loss.backward() # 对权值w求导
w.grad # 取出对w求导后的值，2行10列
```



### 链式法则

对应神经网络中，x相当于输入层，u相当于隐含层，y是输出层，也就是预测值

![链式法则](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\链式法则.png)



**神经网络中的链式法则应用**

这样就很清晰明了了，由后往前逐步求导，让计算权值w的导数变得如此简单，妙啊！！！

![神经网络中的链式法则应用](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\神经网络中的链式法则.png)



链式法则验证

```python
# 链式法则验证
x = torch.tensor(1.)
w1 = torch.tensor(2.,requires_grad=True)
b1 = torch.tensor(1.)
w2 = torch.tensor(2.,requires_grad=True)
b2 = torch.tensor(1.)

y1 = x * w1 + b1
y2 = y1 * w2 + b2

# y2对y1求导
dy2_dy1 = torch.autograd.grad(y2,[y1],retain_graph=True)[0]
# y1对w1求导
dy1_dw1 = torch.autograd.grad(y1,[w1],retain_graph=True)[0]
# y2直接对W1求导
dy2_dw1 = torch.autograd.grad(y2,[w1],retain_graph=True)[0]

# 看两者是否相同
dy2_dw1 == dy2_dy1 * dy1_dw1 # 返回true
```

### 反向传播

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\反向传播梯度求导公式.png)



![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\反向传播梯度求导1.png)





![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\反向传播求导总结.png)



总结：简单理解就是从后往前求导，先求出当前层的求导信息；然后再往前求出之前层的求导信息；以此往复，就可以求出第一层的权值信息。



### 熵的各种公式

**熵**

熵是一个物理学概念，表示一个系统的不确定性程度，或者一个系统的混乱程度。

熵越大，表示不确定也就越大。

**信息熵**

信息熵也就是熵，只不过它代表领域是信息论这边。就相当于在中国其他地域会叫你帅哥，但在广东会叫你靓仔一样。

**相对熵**

相对熵=KL散度（相对熵就是KL散度）

KL散度：是两个概率分布间差异的非对称性度量。即KL散度是用来衡量同一个随机变量的两个不同分布之间的距离。
$$
D_KL(p||q) = \Sigma p(x_i)log(\frac {p(x_i)} {q(x_i)})
$$
特性：

1.非对称性。DKL(p||q) != DKL(q||p)，只有p和q的概率分布完全一样才相等

2.非负性。DKL(p||q)恒大于等于0，p和q概率分布完全一样才相等0



计算

![KL散度](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\KL散度.png)

KL散度公式变形。KL散度 = 交叉熵 - 信息熵

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\KL散度公式变形.png)



**交叉熵**

交叉熵：主要用于度量同一个随机变量X的预测分布Q与真实分布P之间的差距

差距可以理解为距离、误差、失望值、混乱程度等。

交叉熵公式

![交叉熵](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\交叉熵公式.png)



交叉熵例子

结论：

1.交叉熵越小，预测越准确。

2.交叉熵只跟真实标签的预测概率值有关

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\交叉熵例子.png)

交叉熵最简公式
$$
Cross \_ Entropy(P,Q) = - log Q(c_i)
$$
交叉熵的二分类公式。也叫BCE损失函数

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\交叉熵的二分类公式.png)

注意：普通公式、最简化公式、二分类公式计算二分类问题，算出交叉熵结果是一样的



![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\交叉熵做损失函数.png)

交叉熵损失函数
$$
loss = -\frac {1}{N} \Sigma ^N _n=_1 y_nlog \hat y_n + (1-y_n)log(1- \hat y_n)
$$




**softmax**

softmax是什么？

1.将数字转换成概率的神器

2.进行数据归一化的利器



公式

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\softmax公式.png)

损失函数。只跟真实值为1的有关系，其他都是零
$$
Loss(\hat Y,Y) = - Y log \hat Y
$$
**sigmoid**

1.sigmoid函数也叫logistic函数

2.取值范围(0,1)

3.神经网络常用的激活函数

4.常被用于二分类

公式
$$
S(x) = \frac {1}{1 + e^{-x}}
$$




### 杂谈

连续的值通常都叫做regression,sigmoid函数也称为regression

把矩阵看成一种空间变换的函数。

神经网络其实就是在寻找非线性变换的一种空间变换的过程

学习能力必须要有泛化能力才是最好的，如果学习能力特别强，那么噪声数据也学到了，这是我们不想要的

[0,1]分布给神经网络训练效果是最好



### 构建神经网络的步骤

1. 准备数据集，有 X 值 和 Y值(真实值)

2. 设计模型，计算得出 y^ (y hat，预测值)

3. 构造损失函数和优化器。最后计算出来的loss值是标量，也就是一个值。

4. 训练模型（向前反馈、向后传播、更新权重）

   ![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\神经网络构建步骤.png)



增量式开发的思想去写程序，不要一上来就把所有的网络都构建好，不然将来出了问题很难找到问题的根源



### 数据预处理

**标准分数**



$$
Z = \frac {x - \mu}{\sigma}
\\
x表示的是原始的值 \\
\mu 表示的是平均值 \\
\sigma 表示的是标准差
$$
从数学的角度去理解：

标准分数的大小说明当前值大小在全体值的位置，用来说明在所属那批数值中的相对位置。

比如：假设x表示考生的原始分数。原始分数只能反映考生作答正确的程度，但是原始分一般不能反映考生间的差异状况，因此才有了标准分数的出现。



从神经网络的角度理解：

减去均值：数据进入神经网络前，提取特征，提取的特征是要代表每个类别的特点，也就是神经网络更关注的是类别之间的差异。

除以方差：减去均值之后的数值范围可能变化很大，比如图像像素可能在0到255之间晃动，如果不做处理，这些特征输入到神经网络中，会表现出有些地方变化大，有些地方变化小，这样对网络影响是比较大的。我们希望的是关注网络整体的变化，而不是局部的变化影响输出。如果除以标准差，就可以控制像素取值范围，比如[0,1]之间



### 深度学习的过程

1.填充深度学习理论，数学和工程。《深度学习》、《动手学深度学习》

2.阅读pytorch文档，通读一遍

3.复现经典论文。读代码 -> 写代码 -> 读代码...循环往复，直到自己能实现出来

4.在有前三面的基础上，通过不断地阅读论文，扩充视野，当你阅读的论文多了，想法自然而然会从脑海里浮现出来。



### 阅读论文步骤

1.标题 + 作者

2.摘要

3.结论

4.导言

5.相关工作

6.模型

7.实验

8.评论



## 卷积神经网络

**为什么需要卷积神经网络**

像全连接网络那样计算，如果维度很大的话将会造成数据计算爆炸。引入卷积，就可以减少相应的维度，降低运算量。卷积核就相当于特征提取器，在减少数据量的同时还提取了特征，使网络的计算更好



**卷积神经网络是怎样减少参数的**

1.参数共享。相比全连接网络，一个参数对应一个像素特征，卷积网络中有卷积核，与特征之间不是一一对应的（比如9个参数隐射为一个特征），实现了参数共享

2.稀疏连接。比如，我们看输入图像左上角的9个像素和卷积核进行卷积之后，对应得到的特征图中的左上角的1个像素（这个像素只会受到输入图像左上角的9个像素的影响，不会受到其他像素的影响）

### 卷积

卷积的物理意义：如果有一个系统，输入是不稳定的，输出是稳定的，那么可以用卷积来求系统的存量。

数学公式：

*表示卷积操作
$$
f(x) * g(x) = \int_{-\infty}^{\infty} f(x)g(t-x)dx
$$
f函数就是不稳定的输入

g函数就是稳定的输出

他们两相乘求的就是系统的存量



另一种理解：之前的每一个时刻（位置、距离）对当前时刻的影响，他都要进行计算。



总结：

1.不稳定输入、稳定输出，求系统存量

2.周围像素点如何产生影响

3.一个像素点如何试探





### 图像卷积

卷积神经网络的主要用途就是用它去识别图片里的内容。

狭义理解：

图像的卷积操作就是拿图片和卷积核（比如2*2的矩阵或者3 * 3的矩阵）先相乘再相加。

所以图像是f函数，卷积核是g函数。因为图像里的像素点总是在变换，f函数对应的就是系统不稳定的输入。而卷积核是不变的，g函数对应的是系统稳定的输出。

广义理解：

卷积核的理解：他其实就是规定，周围的像素点是如何对当前的像素点产生影响的。

其实g函数不是卷积核，而是要旋转180度之后才是卷积核，只是计算的过程中把旋转的步骤省略了。

其中的关键就是要把卷积当作是过去对现在的影响，周围像素点对当前像素点的影响。

而g函数就是规定了如何影响的关键

新的理解：

卷积就是对周围像素点的一个主动的试探和选择，通过卷积核把周围有用的特征给保留下来



### 池化层

池化层的作用夹在连续的卷积层中间，用于压缩数据和参数的量，减少过拟合。

换言之，如果输入是图像的话，那么池化层最主要的作用是压缩图像。

简单理解就是降维，减少数据量的同时还保持数据的特征



Subsampling（下采样）

下采样的目的是减少下一层的数据量，降低运算的需求



下采样也叫池化层，池化层的方法主要有MaxPooling和AveragePooling

maxpooling是减少宽度和高度



### 卷积神经网络

**第一层在干什么？跟卷积有什么关系**

卷积神经网络的第一步就是把局部特征给挑出来，把这些局部特征交给神经网络。



卷积层的输入和输出，只跟通道和卷积核大小有关；而全连接层是和你变换之后数据大小有关



为什么卷积神经网络权重会比较少呢？这是因为权重是共享的。他不是和每一个元素都建立联系，而是一个区域的元素建立联系



卷积神经网络使用Relu函数作为激活函数更多一点





卷积以及下采样叫做特征提取



**图像卷积运算**

channel为一层的运算,就是输入变量和卷积核做数乘，然后相加

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel1-1.png)



然后不断移动方框，计算出所有的值

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel1最后结果.png)

channel为3的话需要和3个卷积进行数乘

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel3.png)



卷积的channel要和输入的channel对应上

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\channel3对应上.png)



如何把输出channel从1变成m呢？则需要m个卷积，每个卷积的channel等于输入的channel，计算完后进行拼接

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\拼接卷积.png)



简单卷积的过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\卷积的运算过程.png)





### GoogleNet

**解决了什么**

提高大规模图像分类和检测精度



**使用方法**

提出了Inception神经网络结构，该架构能够很好地减少计算量，同时增加了网络的深度和宽度。

Inception就是把多个卷积或池化操作，放在一起组装成一个网络模块，设计神经网络时以模块为单位去组装整个网络结构。

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\Inception.png)

Inception为什么做成这样？

有些超参数，比如kernel是 3 * 3 的还是 5 * 5的，你要选哪个卷积核比较好用，这就比较难。所以它的思想就是你不知道哪个比较好用，那我在一个块里把这几种卷积都用一下，然后把结果拼接在一起。将来，如果 3 * 3 的比较好用，那么 3 * 3的权重会变得比较大，而其他路线上的权重相对就会变小。所以它提供了几种候选的神经网络的配置，然后通过训练它自动地找到卷积最优的组合。



Inception计算过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\Inception计算过程.png)





**1 * 1的卷积的作用**

可以将C1的通道数改为C2的通道数。降低运算量，也可以简单理解为一种降维的方式。

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\卷积网络\1-1卷积核.png)





### 残差网络

残差网络是为深度神经网络隐藏层过多时的网络退化问题而提出的。所谓的网络退化就是梯度逐渐趋于0（梯度消失），导致权重不会再变化。

![](https://image.jiqizhixin.com/uploads/editor/fdc85c5b-039a-4edd-bfa5-7aeb01684134/1541508117061.jpeg)



当z=F(x)+x时，求导，它不是在0附近的导数，而是在1附近的导数，若干个这样的数相乘，就能够非常好地去解决梯度消失的问题。

既然F(x)和x要做加法，即那两层的输出要和x的输入张量维度必须是一样的（即通道、宽度、高度都得一样）



## 循环神经网络

### RNN

**为什么需要循环神经网络**

全连接神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理**序列**的信息，即前面的输入和后面的输入是有关系的。



循环神经网络里使用tanh作为激活函数更多一点



RNN是对时序数据的建模

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN神经网络.png)

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN神经网络2.png)



**RNN Cell**



RNN Cell 的本质就是一个线性层，能把某一个维度映射到另外一个维度里的空间里。

这个线性层和平常的线性层有什么不一样呢？它是共享的。



把输入的张量维度和隐藏层的张量维度搞清楚了，RNN就会变得非常容易

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN Cell.png)

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN Cell实现方式.png)

**RNN使用**

inputs是所有x1、x2、...序列的和

out是全部隐层的结果

hidden是最后一个隐层的结果，有几层就有几个

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN 使用.png)

**numLayers**

numLayers表示有几层RNN

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\numLayers.png)



反向传播过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN 梯度如何训练.png)



**pytorch中RNN的使用**

x[seq len,batch,feature len]

seq len：表示有几个单词,即单词的数量

batch：表示有几句话，即句子的数量

feature len：表示有一个单词有几维



![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\RNN model使用.png)



input_size：是单词的维度

hiden_size：是指我的memory多长来表达，即h得维度

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RNN\输出out.png)



**RNN梯度消失的原因？**

RNN梯度消失和普通网络不太一样，RNN的梯度是一个总的梯度和，它的梯度消失并不是变为0，而是说总梯度被近距离梯度主导，被远距离梯度忽略不计。可以使用LSTM缓解远距离梯度消失



### LSTM





## 多头注意力机制

### 原理

Q、K、V首次经过一个线性变换，然后输入到放缩点积Attention（Scaled Dot-Product attention）中，进行h次计算，即多头。然后将h次的放缩点积Attention结果进行拼接，再进行一次线性变换得到的值作为多头Attention的结果

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\多头注意力机制.png)



### 自注意力机制（Self-attention）

**self-attention是怎么运作的呢？**

self-attention会计算一整个sequence，你输入几个向量，它就输出几个向量。

输出的向量有什么特别的地方吗？输出的向量考虑了一整个sequence才得到的，即考虑了上下文语义。

self-attention不是只可以用一次，他可以用多次。

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention1.png)



这里a是输入向量，b是输出向量。b的输出是考虑了每一个输入向量的

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention2.png)

b1向量是如何产生的呢？(b2、b3、b4类似)

a1先跟每一个向量计算出相关性

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention3.png)

相关性怎么计算呢？

把输入的两个向量分别乘以不同的矩阵，得到q和k两个值，然后将q和k作点积

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention4.png)

具体的过程

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention5.png)

计算出每一个向量的关联性之后，接下来会做一个Softmax，得出a'

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention6.png)



得到a'之后，基于a'抽取出这Seq中重要的资讯。

如何抽取呢？我们会把a1到a4乘以一个V矩阵，得到一个新的向量v1、v2、v3、v4。然后各自乘以他们的a'，加起来

![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\self-attention\self-attention7.png)





## BERT 和 Transformer

### BERT 和 Transformer的关系

Transformer是特征提取器，和CNN、RNN并列用于特征抽取的一种深层级网络结构。

BERT 可以视为两阶段处理流程，这个流程使用的框架便是Transformer,简单理解就是，BERT利用Transformer学会了如何编码、存储信息。



**BERT**

BERT由两阶段构成，每个阶段有自己的特点和目标。第一个阶段是预训练阶段，第二个阶段是Fine-Tuning阶段。

预训练阶段用大量无监督的文本通过自监督方式进行训练，把文本包含的语言知识以参数形式编码到Transformer中，Fine-Tuning一般是有监督的，数据量比较小，在模型结构上做分类任务以解决当前任务。

简单地理解，在预训练阶段Transformer学到了很多初始化的知识，第二阶段就是把初始化网络学到的语言知识拿来用。



**Transformer**

Transformer是一个典型的层级结构

































