# 深度学习

## 常见基本函数的用法

**python两大法宝函数**

dir()函数：知道工具箱，以及工具箱里有什么东西

```python
dir(pytorch)
```

help()函数：知道每个工具是如何使用，工具的使用用法

```python
help(pytorch)
```



**pytorch加载数据集**

DataSet类：加载数据集以及对应的label分类

```python
from torch.utils.data import Dateset

# 查看Dataset用法
help(Dataset)
Dataset??

# 定义一个自己的Data类
class 
```



## 神经网络

### 概览

神经网络的工作就是不断地调节omega的值，让整个权重达到我想要的状态，那这个状态的达到，是通过我一个一个的样本，不断地去调节它去得到的



激活函数的引入一方面是为了将我们的数值规范到[0,1]区间，这样可以避免数值在网络中不断地被放大。另一方面，由于神经网络的加权都是线性运算，如果不引入非线性的激活函数，即便是多层的线性运算，最后也只会等效于一个线性运算。

不加激活函数计算出的结果是这样的

![](https://pic2.zhimg.com/80/7c6e12aed30bf315eed8df6476d7ef7b_720w.jpg?source=1940ef5c)

![](https://pic2.zhimg.com/80/c46188f6f517a15142133129e47d1ae8_720w.jpg?source=1940ef5c)

通过激活函数进行变换

![](https://pic3.zhimg.com/80/32cbeac5eaea9d655b9a50e4d8d0a687_720w.jpg?source=1940ef5c)



最后结果会变成这样

![](https://pic2.zhimg.com/80/3e4d3aabb90f51f467437a17861d3bf7_720w.jpg?source=1940ef5c)

![](https://pic3.zhimg.com/80/fab8a7ae1cb63992f70e160d7f03c067_720w.jpg?source=1940ef5c)

梯度下降的方法去寻找最佳权重

前向传播和反向传播



### 导数/偏导数/梯度/激活函数/loss

**导数**

它反映的是函数在某一点的变化率。

导数是一个标量，它的长度反映了变化率的大小。



**偏微分**

导数处理的是一维函数的变化率，当处理更高维度的变化率时，因此产生了偏微分。

偏微分讲的是一个函数对它的自变量的变化率的描述程度，它也是一个标量。

导数的方向是可以随便指定的，可以指向x，也可以指向y。但是偏微分讲的是给定的自变量的方向。



**梯度**

梯度定义的是所有偏微分的向量。

它就是把所有的偏微分当成一个向量来理解，所以梯度是一个向量，不是一个标量。

梯度的长度某种程度上反映了这个函数的变化趋势（是快还是慢）

梯度的方向代表的是一个函数增长的方向。

因此可以做一个直观的感受：函数的梯度是一个向量，向量的方向代表的是函数在当前点的增长的方向，向量的模代表的是函数在当前这个点的增长的速率。



梯度和导数的区别：导数是给定的方向，梯度是所有方向的综合。对于一维的函数它只有一个方向，因此一维的函数的梯度和导数是一个东西。只不过导数是没有方向的，梯度是有方向的。



**如何利用梯度找到一个极值解？**

一般搜索的是极小值，如果搜索极大值前面添加一个负号就行。

通过这个函数进行寻找 
$$
\theta_t+_1 = \theta_t - \alpha_t\nabla f(\theta_t)
$$
对于凸函数一定可以找到一个全局最小值



局部最小值和鞍点会影响你的优化器，影响搜索的过程。

哪些因素还会影响搜索的过程呢？1.初始状态 2.学习率 3.动量(也就是如何逃离局部最小值)



**激活函数**

什么叫激活函数？

可以理解为一个阈值函数，当大于某个阈值时，他就会激活下一个神经元，它是一个固定的变量，0或1

![激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\激活函数.png)

像这样的激活函数是不可导的，这样的函数我们不能直接使用梯度下降的方法进行优化；

所以出现了启发式搜索的方法来求解单层感知机的一个最优解的情况；

为了解决单层感知机激活函数，激活阶梯函数不可导的情况，提出了一个连续光滑的函数，叫sigmoid

![sigmoid函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\sigmoid函数.png)

sigmoid导数情况

![sigmoid导数情况](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\sigmoid导数情况.png)

sigmod函数使用的非常多，主要是因为它是连续的光滑的，而且它的值压缩在[0,1]区间。

但是它有缺陷，当你的数值接近无穷时，你的导数接近于零，将会根据梯度公式，值长期得不到更新，出现梯度弥散现象。

pytorch中Sigmoid函数的实现

```python
z = torch.linspace(-100,100,10)
torch.sigmoid(z)
```

Tanh激活函数，在RNN用的比较多,它的值是[-1,1]的区间

![Tanh激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\tanh激活函数.png)

Tanh导数

![Tanh导数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\tanh导数.png)

pytorch中tanh函数实现

```python
z = torch.linspace(-1,1,10)
torch.tanh(z)
```

深度学习中的奠基石的激活函数RLU激活函数（整型的线性单元）

![RLU激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RLU激活函数.png)



RLU导数

![RLU导数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\RLU导数.png)

为什么它特别适合深度学习？

当Z<0时，导数为0；当Z>0时。它的梯度永远是1。因此它在做向后传播的时候，计算梯度非常方便，不会放大，也不会缩小，这样就不会出现梯度弥散和梯度爆炸的情况。对于搜索最优解，存在先天的优势，就是导数计算非常简单

pytorch实现relu函数

```python
z = torch.linspace(-1,1,10)
torch.relu(z)
```

优先使用relu函数，遇到特殊情况，可以尝试使用其他激活函数



**loss函数**

作用：损失函数是用来估量模型的预测值f(x)与真实值y的不一致程度，它是一个非负的实值函数，损失函数值越小，模型的鲁棒性就越好。（鲁棒性可以理解为异常或者危险情况下，系统生存的能力）



常见的loss函数：MSE(均方差)

用于分类的：cross entropy loss（交叉熵损失）

1.可以用于二分类的

2.也可以用于多分类的

3.跟softmax激活函数搭配使用的



MSE损失函数

![MSE损失函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\MSE函数.png)

MSE损失函数的梯度

![MSE损失函数的梯度](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\MSE损失函数的梯度.png)





$$
\theta 取绝于具体的网络形式，如果\theta=wx + b，则\theta对w求导是x，对b求导是1
$$


pytorch自动求导api

```python
import torch
import torch.nn.functional as F
# 对 pred = xw + b进行求导,初始化 x=1,w=2,b=0
x = torch.ones(1)
w = torch.full([1],2)

# mse_loss第一参数是预测的值，第二个参数是真实值
MSE = F.mse_loss(x*w,torch.ones(1)) # MSE = 1

# 表示MSE对w求导
torch.autograd.grad(MSE,[w])

# 或者使用backward
MSE.backward()
w.grad
```



Softmax激活函数，适用于多分类，相当于对每个类别求概率，他们最终概率之和是为1的。

![Softmax激活函数](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\Softmax激活函数.png)



### 感知机的梯度求导

单层感知机的梯度求导。t是目标值，E是loss函数



![](D:\github\MyKnowledgeRepository\machine_learning_img\deep_learning\单层感知机的梯度求导.png)



代码实现

```python
# x的特征有10个
x = torch.randn(1,10)
w = torch.randn(1,10,requires_grad=True)
o = torch.sigmoid(x@w.t()) # t()表示转置,答案是一个数
loss = F.mse_loss(torch.ones(1,1),o) # 损失函数
loss.backward() # 损失函数对w求导
w.grad
```



