# 机器学习算法

## 基础概念

[数学符号](https://blog.csdn.net/u014157109/article/details/89437547)



**评价指标**

混淆矩阵

```tex
True Positive(真正，TP)：将正类预测为正类数
True Negative(真负，TN)：将负类预测为负类数
False Positive(假正，FP)：将负类预测为正类数
False Negative(假负，FN)：将正类预测为负类数
```

|      | 预   | 测            | 类           | 别             |
| :--: | ---- | ------------- | ------------ | -------------- |
|  实  |      | Yes           | No           | 总计           |
|  际  | Yes  | TP            | FN           | P（实际为Yes） |
|  类  | No   | FP            | TN           | N（实际为No）  |
|  别  | 总计 | P'(被分为Yes) | N'(被分为No) | P + N          |



准确率(Accuracy)

很容易理解，分对了的样本数除以所有的样本数
$$
ACC = \frac{TP + TN}{TP + TN + FP + FN}
$$
精确率(precision)

实际为正的样本数除以分为正例的样本数
$$
P = \frac{TP}{TP + FP} = \frac{TP}{P'}
$$
召回率(recall)

实际为正例的样本数除以所有正例的样本数，度量有多少个正例被分为正例
$$
R = \frac{TP}{TP + FN} = \frac{TP}{P}
$$
综合指标(F-Measure，或称F-Score)

P和R指标有时候会出现矛盾，需要综合考虑他们，常用的是F1指标，F1综合了P和R的结果，当F1较高时，说明试验方法比较有效
$$
F1 = \frac{2 * P * R}{P + R}
$$




## 聚类算法

### K-Means

两个物品接不接近是由用户去指定标准的



## 文本量化的方式

### 词独立和词关联

![词独立](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\词独立和词关联.png)



### one-hot(独热编码)

核心思想：文档中每个单词的出现都是独立的，每个词都是独一无二的含义，与其他词无关。对单词编码后的向量只有0和1，且只有一个维度是1

**案例**

![one-hot案例](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\one-hot案例.png)



**不足**

稀疏矩阵；高维；无法学习语义；向量间的距离无法反映语义的差异



### bags of word（BOW）

![BOW](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\bagofwords.png)



**这种方式存在的问题**

1.所有不重复的词被认为是相互独立的，也就无法度量文本蕴含的丰富语义

2.由于不重复的词数量太多（形成高维、稀疏矩阵），这给模型，尤其是统计模型，带来了计算的复杂性



**解决办法**

压缩维度、并最大程度挖掘文本蕴含的语义。

核心：把一个维度不重复词数量的高维空间压缩到一个维度低许多的的向量空间

常用的NLP技术有：Word2Vec



### BOW的改进表达TF-IDF

BOW的不足：它把每一个词都当成了一样，其实内文档内每一个词的重要性是不一样的。



这时引入了TF-IDF值

![TF-IDF](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\TF-IDF.png)



**说明**

做log运算是为了不然这个数变得特别大，做了一些平滑的处理。

这个TF是在这个文档的

![一些说明](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\5.png)

通过Sklean包的方法训练得出的数据是进行标准化(归一化)过的



### word2vec

视频地址：[像机器一样学习](https://www.youtube.com/channel/UCm9zvddKLNLmEv-6gOt0Mdw)

![word2vec数据结构](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\word2vec数据结构.png)



![神经网络模型](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\2.png)



![降维过程](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\3.png)



重要假设：文本中离得越近的词语相似度越高

基于这个假设，word2vec使用CBOW和skip-gram来计算词向量矩阵

COB：是使用上下文词来预测中心词

skip-gram：是使用中心词来预测上下文词

![](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\COB和skip-gram.png)

缺点：

1.没有考虑同义词

2.窗口长度有限

3.没有考虑全局的文本信息

4.不是严格意义的语序



### Glove词向量

Glove模型的基本思想是利用词向量对"词-上下文"共现矩阵进行预测(或者回归)，从而实现隐式的矩阵分解。



### Word Embedding

word embedding会给每一个词一个向量，而这个向量是有语义的，如果把word embedding可视化的话，你会发现所有动物可能聚集在一块，所有植物聚集在一块

![](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\WordEmbedding.png)









### 中文文本聚类的步骤



![文本聚类](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\文本聚类步骤.png)





## 情感分析

文本广义分类：主观性文本和客观性文本。主观性文本从情感上又分为正面(积极)、负面(消极)以及中性

按照文本粒度划分：词、句子、篇章

情感词抽取目前主要分为基于语料库和基于词典的两种研究方法。



#### 文本情感分析过程

1.原始数据的获取（一般爬虫获取）

2.数据的预处理（去除无效字符和数据、使用分词工具进行分词、停用词过滤等）

3.特征提取（词频计数模型 N-gram 和词袋模型 TF-IDF、深度学习方法的特征提取一般都是自动的）

4.分类器（SVM、softmax主要实现多分类问题）

5.情感类别输出（积极、消极、中性）

![情感分析过程](D:\github\MyKnowledgeRepository\machine_learning_img\情感分析过程.png)



#### 情感分析的方法

**基于情感词典的情感分析方法**

难点：在于构建情感词典

**基于机器学习的情感分析方法**

提出背景：为了提高情感分类的准确性

**基于深度学习的情感分析方法**

提出背景：为了解决在情感分析问题中忽略上下文语义的问题



![情感分析方法优缺点对比](D:\github\MyKnowledgeRepository\machine_learning_img\情感分析方法优缺点对比.png)



#### 文本粒度情感分析分类

**文档级情感分析**

判断整个文档的情感倾向



缺点：

1.总体上的情感倾向判别结果对人们来说是比较粗糙的，人们可能更喜欢探究一些细节

2.文档级情感分析需要立足于一个前提假设基础上，即假设这个完整文档只对一个实体进行评价，研究可知该假设并不能满足实际需求。实际情况往往是一个文档会评价多个实体，这也在一定程度上削弱了文档级情感分析的实用价值。

**句子级情感分析**

判断句子的情感倾向



缺点：

1.句子本身包含的信息较少，他可能只给人提供观点和感情倾向，却不能给出这些观点或者感情倾向所指的对象实体是什么，这对人们的需求而言，其意义和参考价值就会大幅度降低。

2.前提假设是一个句子只表达了一个观点或只含有一个情感倾向，不能判别和处理复杂句、组合句等

**属性级情感分析**

需要维护一个情感词典，通过情感词典来去判断属性的情感倾向



缺点：

1.情感词典的建立和维护工作量是巨大的，并且因领域不同而存在显著差别

2.目前只支持名词性方面的抽取，还未能由动词表达的属性抽取



#### 中文文本特征选择方法

**常见特征选择方法：**

1.文档频率（DF）

2.信息增益（IG）

3.互信息（MI）

4.CHI统计方法（CHI）







