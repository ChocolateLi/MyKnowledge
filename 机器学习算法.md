# 机器学习算法

## 基础概念

[数学符号](https://blog.csdn.net/u014157109/article/details/89437547)



## 聚类算法

### K-Means

两个物品接不接近是由用户去指定标准的



## 文本量化的方式

### 词独立和词关联

![词独立](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\词独立和词关联.png)



### one-hot(独热编码)

核心思想：文档中每个单词的出现都是独立的，每个词都是独一无二的含义，与其他词无关。对单词编码后的向量只有0和1，且只有一个维度是1

**案例**

![one-hot案例](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\one-hot案例.png)



**不足**

稀疏矩阵；高维；无法学习语义；向量间的距离无法反映语义的差异



### bags of word（BOW）

![BOW](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\bagofwords.png)



**这种方式存在的问题**

1.所有不重复的词被认为是相互独立的，也就无法度量文本蕴含的丰富语义

2.由于不重复的词数量太多（形成高维、稀疏矩阵），这给模型，尤其是统计模型，带来了计算的复杂性



**解决办法**

压缩维度、并最大程度挖掘文本蕴含的语义。

核心：把一个维度不重复词数量的高维空间压缩到一个维度低许多的的向量空间

常用的NLP技术有：Word2Vec



### BOW的改进表达TF-IDF

BOW的不足：它把每一个词都当成了一样，其实内文档内每一个词的重要性是不一样的。



这时引入了TF-IDF值

![TF-IDF](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\TF-IDF.png)



**说明**

做log运算是为了不然这个数变得特别大，做了一些平滑的处理。

这个TF是在这个文档的

![一些说明](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\5.png)

通过Sklean包的方法训练得出的数据是进行标准化(归一化)过的



### word2vec（还没完全明白，暂时不看，因为还用不到）

视频地址：[像机器一样学习](https://www.youtube.com/channel/UCm9zvddKLNLmEv-6gOt0Mdw)

![word2vec数据结构](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\word2vec数据结构.png)



![神经网络模型](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\2.png)



![降维过程](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\3.png)



### 中文文本聚类的步骤



![文本聚类](D:\github\MyKnowledgeRepository\machine_learning_img\text_as_data\文本聚类步骤.png)





## 情感分析

文本广义分类：主观性文本和客观性文本。主观性文本从情感上又分为正面(积极)、负面(消极)以及中性

按照文本粒度划分：词、句子、篇章

情感词抽取目前主要分为基于语料库和基于词典的两种研究方法。

























