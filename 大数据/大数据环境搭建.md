# 大数据环境搭建

# Hadoop单机伪分布式集群搭建

## 1.准备linux环境

1. 关闭防火墙

2. 创建用户和组

   ```shell
   # hadoop缩写hdp
   useradd hadoop
   groupadd hadoop
   ```

3. 修改主机和ip的映射关系

```shell
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.201.100.75 cesdb hadoop
```

4.配置集群（单节点）互信

```shell
# 切换hadoop用户
su hadoop

# 生成公钥
ssh-keygen -t rsa

# 复制公钥
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

# 重启ssh
systemctl restart sshd

# 测试
ssh localhost
ssh cesdb
ssh hadoop
```

## 2.安装jdk

将jdk的包上上传到需要的目录然后解压到相应的目录上，我的jdk目前解压到了/usr/jdk中

将java添加到环境变量中

```shell
vim /etc/profile

# JAVA_HOME
JAVA_HOME=/usr/jdk
PATH=$JAVA_HOME/bin:$PATH
CLASSPATH=:.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME
export PATH
export CLASSPATH
# HADOOP_HOME
export HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

#刷新配置
source /etc/profile
```

## 3.安装hadoop - 3.3.6

1. 从官网下载[下载hadoop-3.3.6](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz)

2. 将文件上传到 /data/u01/soft

3. 解压文件

   ```shell
   tar -zxvf hadoop-3.3.6 -C /data/u01/app/hadoop
   ```

4. 配置hadoop文件

   第一个：hadoop-env.sh

   ```shell
   cd /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop
   
   vim hadoop-env.sh
   
   export JAVA_HOME=/usr/jdk
   export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
   ```

   第二个：core-site.xml

   ```shell
   <configuration>
           <property>
                   <name>fs.defaultFS</name>
                   <value>hdfs://cesdb:8020</value>
                   <description>hdfs内部通讯访问地址</description>
           </property>
           <property>
                   <name>hadoop.tmp.dir</name>
                   <value>/data/u01/app/hadoop/hadoop-3.3.6/data/tmp</value>
           </property>
           <property>
                   <name>hadoop.proxyuser.hadoop.hosts</name>
                   <value>*</value>
           </property>
           <property>
                   <name>hadoop.proxyuser.hadoop.groups</name>
                   <value>*</value>
           </property>     
   </configuration>:
   
   ```

   第三个：hdfs-site.xml

   ```shell
   <configuration>
   
     <property>
         <name>dfs.namenode.name.dir</name>
         <value>/data/u01/app/hadoop/hadoop-3.3.6/data/namenode</value>
         <description> namenode 存放name table(fsimage)本地目录需要修改,如果没有需要自己创建文件目录)</description>
     </property>
     <property>
         <name>dfs.datanode.data.dir</name>
         <value>/data/u01/app/hadoop/hadoop-3.3.6/data/datanode</value>
         <description>datanode存放block本地目录（需要修改,如果没有需要自己创建文件目录）</description>
     </property>
     <property>
             <!--由于只有一台机器,hdfs的副本数就指定为1-->
             <name>dfs.replication</name>
             <value>1</value>
      </property>
      <property>
       <name>dfs.permissions.enabled</name>
       <value>false</value>
   </property>
   
   </configuration>
   
   ```

   第四个：mapred-site.xml

   ```shell
   <configuration>
           <!-- 指定mr运行在yarn上 -->
           <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
           </property>
   </configuration>
   ```

   第五个：yarn-site.xml

   ```shell
   <configuration>
   
   <!-- Site specific YARN configuration properties -->
   <!-- 指定YARN的老大（ResourceManager）的地址 -->
   <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>cesdb</value>
   </property>
   <!-- reducer获取数据的方式 -->
   <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
   </property>
   <!-- ResourceManager Web UI Port Configuration -->
     <property>
       <name>yarn.resourcemanager.webapp.address</name>
       <value>cesdb:8082</value>
       <description>Address where the ResourceManager web application will listen on.</description>
     </property>
   <property>
       <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
     </property>
     <property>
       <name>yarn.application.classpath</name>
       <value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
     </property>
   
   </configuration>
   
   
   ```

5. 第一个启动集群需要格式化namenode

   ```shell
   hdfs namenode -format
   ```

6. 启动hadoop

   ```shell
   # 先启动HDFS
   sbin/start-dfs.sh
   
   # 再启动YARN
   sbin/start-yarn.sh
   
   # jps验证是否成功
   [hadoop@cesdb logs]$ jps
   85961 Jps
   191593 DataNode
   192825 SecondaryNameNode
   225583 ResourceManager
   191372 NameNode
   225836 NodeManager
   ```

7. 访问

   ```shell
   http://10.201.100.75:9870（HDFS管理界面）
   http://10.201.100.75:8082 （MR管理界面）
   ```

# Hadoop完全分布式搭建

## 分布式集群的网络和节点规划

### 网络规划

| 主机名 | IP地址        | 节点类型 |
| ------ | ------------- | -------- |
| cesdb  | 10.201.100.75 | master   |
| cesdb1 | 10.201.100.84 | slave1   |
| cesdb2 | 10.201.100.85 | slave2   |

### 节点规划

| 服务               | cesdb | cesdb1 | cesdb2 |
| ------------------ | ----- | ------ | ------ |
| NameNode           | √     |        |        |
| Secondary NameNode |       | √      |        |
| DataNode           | √     | √      | √      |
| ResourceManager    | √     |        |        |
| NodeManager        | √     | √      | √      |
| JobHistoryServer   |       |        | √      |

## 环境准备

### 配置网络映射和主机名

更改主机名

```bash
hostnamectl set-hostname <newhostname>
```

编辑网络配置文件

```bash
vim /etc/hosts

# 在文件的最后添加ip地址 主机名（三台机器都要添加）
10.201.100.75 cesdb
10.201.100.84 cesdb1
10.201.100.85 cesdb2
```

### 设置SSH无密码登录节点

这是root用户下的，如果是hadoop用户下，也需要这样操作

1.在三台机器上生成密钥文件

```bash1
ssh-keygen -t rsa
```

2.将本机公钥复制到其他机器上（三台机器都要这样操作）

```bash
ssh-copy-id cesdb
ssh-copy-id cesdb1
ssh-copy-id cesdb2
```

3.查看是否成功免密登录

```bash
ssh cesdb2
```

### Java环境

cesdb本身安装了java和hadoop，可以把相应的jdk复制到cesdb1和cesdb2中，并配置/etc/profile文件

```bash
scp -r /usr/jdk root@cesdb1:/usr
scp -r /usr/jdk root@cesdb2:/usr

# 将cesdb的/etc/profile文件内容复制给cesdb1和cesdb2
JAVA_HOME=/usr/jdk
PATH=$JAVA_HOME/bin:$PATH
CLASSPATH=:.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME
export PATH
export CLASSPATH
# HADOOP_HOME
export HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_CLASSPATH=$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
# HIVE_HOME
export HIVE_HOME=/data/u01/app/hive/apache-hive-3.1.3
export PATH=$PATH:$HIVE_HOME/bin
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*

#刷新配置
source /etc/profile
```



## 安装配置Hadoop集群

### 先备份单机版集群数据

```bash
hdfs dfs -copyToLocal /user /data/u01/backup/hadoop_backup
```

集群搭建好后，将之前的数据备份到hadoop上

```bash
 hdfs dfs -copyFromLocal /data/u01/backup/hadoop_backup/user /
```

### 备份要修改的文件数据

因为之前的单机版hadoop是可以正常启动，所以我现把修改的配置文件先全部备份一份，以免有突发情况，我也可以恢复回去。

```bash
core-site.xml  hadoop-env.sh  hdfs-site.xml  mapred-site.xml yarn-site.xml
先把这几个文件备份起来
```

### 修改配置文件

**hadoop-env.sh**

```bash
vim hadoop-env.sh

export JAVA_HOME=/usr/jdk
export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
```

**core-site.xml **

```bash
vim core-site.xml 

<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://cesdb:8020</value>
                <description>hdfs内部通讯访问地址</description>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/data/u01/app/hadoop/hadoop-3.3.6/data/tmp</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hadoop.hosts</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hadoop.groups</name>
                <value>*</value>
        </property>
</configuration>

```

**hdfs-site.xml **

```bash
vim hdfs-site.xml 

<configuration>

 <property>
      <name>dfs.namenode.http-address</name>
      <value>cesdb:9870</value>
 </property>
 <property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>cesdb1:9868</value>
</property>
  <property>
      <name>dfs.namenode.name.dir</name>
      <value>/data/u01/app/hadoop/hadoop-3.3.6/data/namenode</value>
      <description> namenode 存放name table(fsimage)本地目录需要修改,如果没有需要自己创建文件目录)</description>
  </property>
  <property>
      <name>dfs.datanode.data.dir</name>
      <value>/data/u01/app/hadoop/hadoop-3.3.6/data/datanode</value>
      <description>datanode存放block本地目录（需要修改,如果没有需要自己创建文件目录）</description>
  </property>
  <property>
          <!--由于只有一台机器,hdfs的副本数就指定为1-->
          <name>dfs.replication</name>
          <value>3</value>
   </property>
<property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
</property>
</configuration>

```

**yarn-site.xml**

```bash
vim yarn-site.xml

<configuration>

<!-- Site specific YARN configuration properties -->
<!-- 指定YARN的老大（ResourceManager）的地址 -->
<property>
     <name>yarn.resourcemanager.hostname</name>
     <value>cesdb</value>
</property>
<!-- reducer获取数据的方式 -->
<property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
</property>
<!-- ResourceManager Web UI Port Configuration -->
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>cesdb:8082</value>
    <description>Address where the ResourceManager web application will listen on.</description>
  </property>
<property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.application.classpath</name>
    <value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
  </property>
 <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
    <description>该参数是配置是否启用日志聚集功能</description>
 </property>
 <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>106800</value>
   <description>该参数是配置聚集的日志在hdfs上保存的最长时间</description>

 </property>
 <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/user/container/logs</value>
   <description>该参数是指定日志聚合目录</description>
 </property>

</configuration>

```

**mapred-site.xml**

```bash
<configuration>
        <!-- 指定mr运行在yarn上 -->
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
<property>
     <name>mapreduce.jobhistory.address</name>
     <value>cesdb2:10020</value>
     <description>设置mapreduce的历史服务器安装的位置及端口号</description>
 </property>
 <property>
     <name>mapreduce.jobhistory.webapp.address</name>
     <value>cesdb2:19888</value>
     <description>历史服务器的web页面地址和端口</description>
 </property>
 <property>
     <name>mapreduce.jobhistory.intermediate-done-dir</name>
     <value>${hadoop.tmp.dir}/mr-history/tmp</value>
     <description>设置存放日志文件的临时目录</description>
 </property>
 <property>
     <name>mapreduce.jobhistory.done-dir</name>
     <value>${hadoop.tmp.dir}/mr-history/done</value>
     <description>设置存放运行日志文件的最终目录</description>
 </property>
</configuration>

```



### 分发文件

在Master节点上安装及配置好hadoop系统，其他slave节点完成ssh、jdk等的安装、免密登录等，既可以将在cesdb上配置好的Hadoop分发给其他节点

```bash
scp -r /data/u01/app/hadoop root@cesdb1:/data/u01/app/
scp -r /data/u01/app/hadoop root@cesdb2:/data/u01/app/

# 在cesdb1和cesdb2中修改用户组(我的是以hadoop用户安装的，所以修改为hadoop)
chown -R hadoop:hadoop hadoop

```

### 启动集群

在启动hadoop集群前，需要先格式化NameNode，在Master主机下操作

```bash
hdfs namenode -format
```

全部启动命令：`start-all.sh`

```bash
start-all.sh        #启动HDFS和YARN
stop-all.sh         #停止HDFS和YARN
```

启动和停止历史（日志）服务器

```bash
mr-jobhistory-daemon.sh start historyserver     #启动historyserver
mr-jobhistory-daemon.sh stop historyserver     #停止historyserver
```

如果发现hadoop集群中datanode没有成功启动

解决方案：[hadoop集群启动后datanode没有启动](https://blog.csdn.net/huguihua2002/article/details/100079564)



### 时间同步

Hadoop集群对时间要求非常高，主节点与各从节点的时间都必须同步。NTP使用来使计算机时间同步的一种协议。配置时间同步服务器（NTP服务器）主要就是为了进行集群的时间同步。RHEL7操作系统默认安装有chrony时间同步服务，直接修改相关配置文件即可使用chrony服务做时间同步。

检查当前时区

```bash
timedatectl

# 如果不一致，设置为一致的
timedatectl set-timezone Asia/Shanghai
```

修改chrony配置文件

```bash
vim /etc/chrony.conf

server 199.168.200.120 iburst
```

重启时间同步服务

```bash
systemctl enable chronyd.service 
systemctl restart chronyd.service 
systemctl status chronyd.service 
```

查看时间同步源

```bash
chronyc sources -v
```

显示时间同步源

```bash
chronyc sourcestats 
```

测试集群间的时间同步

```bash
date '+%Y-%m-%d %H:%M:%S'
```

# Hive搭建

在已经搭建好的 Hadoop 环境中安装和配置 Hive，并使用 Oracle 作为元数据存储，需要进行以下步骤：

## 1. 下载和安装 Hive
首先，从 Apache 官网下载 Hive，并进行安装：

```bash
# 下载 Hive
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# 解压缩 Hive
tar -xzvf apache-hive-3.1.3-bin.tar.gz -C /data/u01/soft/hive
```

## 2. 配置环境变量
配置 Hive 的环境变量：

```bash
vim /etc/profile
```

在文件末尾添加以下内容：

```bash
# Hive environment variables
export HIVE_HOME=/data/u01/app/hive/apache-hive-3.1.3
export PATH=$PATH:$HIVE_HOME/bin
```

使环境变量生效：

```bash
source /etc/profile
```

## 3. 下载并配置 Oracle JDBC 驱动
从 Oracle 官网下载 Oracle JDBC 驱动（例如 `ojdbc8.jar`），然后将其复制到 Hive 的 lib 目录：

```bash
cp /path/to/ojdbc8.jar $HIVE_HOME/lib/
```

## 4. 配置 Hive 使用 Oracle 作为元数据存储

创建一个日志文件夹

```shell
mkdir logs
```

进入配置文件夹

```shell
cd /data/u01/app/hive/apache-hive-3.1.3/conf

touch hive-site.xml
cp hive-env.sh.template hive-env.sh
# 不手动添加的话,hive不打印日志！！！
cp hive-log4j2.properties.template hive-log4j2.properties
cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties
```

修改hive-env.sh

```shell
vim hive-env.sh

HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
```

编辑 Hive 的配置文件 `hive-site.xml`：

```bash
vim hive-site.xml
```

添加以下配置：

```xml
<configuration>
    
    <property>
  <name>fs.defaultFS</name>
  <value>hdfs://cesdb:8020</value>
</property>
   <!--
	//oracle服务的方式
	jdbc:oracle:thin:@//199.168.200.55:1521/docare

	//oracle sid的方法是
	jdbc:oracle:thin:@10.201.100.75:1521:cesdb
	--> 
    
  <!-- Oracle JDBC connection string -->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:oracle:thin:@10.201.100.75:1521:cesdb</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>oracle.jdbc.OracleDriver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>用户名</value>
    <description>Username to use against metastore database</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>密码</value>
    <description>Password to use against metastore database</description>
  </property>


  <!-- Hive Execution Configuration -->
  <property>
    <name>hive.execution.engine</name>
    <value>mr</value>
    <description>Execution engine to use. mr is mapreduce, tez is Tez, spark is Spark</description>
  </property>
    
    <property>
        <!--hive表在hdfs的位置-->
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <property>
        <name>hive.security.authorization.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.security.authorization.createtable.owner.grants</name>
        <value>ALL</value>
    </property>
    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
    </property>
    <property>
  		<name>hive.metastore.partition.management.task.frequency</name>
  		<value>1d</value>
 		 <description>Frequency at which the partition management task runs to add partitions.</description>
	</property>

</configuration>
```

## 5. 初始化 Hive Metastore
先给Hadoop配置init-env.sh

```shell
cd /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop

touch init-env.sh

vim init-env.sh

#!/bin/bash
# 添加新的环境变量
export HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
export HADOOP_CONF_DIR=$HADOOP_HOME/conf
export HADOOP_LOG_DIR=/data/u01/app/hadoop/hadoop-3.3.6/logs
export PATH=$PATH:$HADOOP_HOME/bin

source init-env.sh
```

给Hive配置init-env.sh

```shell
cd /data/u01/app/hive/apache-hive-3.1.3/conf

touch init-env.sh

vim init-env.sh

#!/bin/bash
export HIVE_HOME=/data/u01/app/hive/apache-hive-3.1.3
export HIVE_CONF_DIR=$HIVE_HOME/conf
export PATH=$PATH:$HIVE_HOME/bin

source init-env.sh
```

配置hive启动脚本

启动hive，注：`一定要确保hadoop已经成功启动，才能启动hive，否则连接hive beeline会卡死但是不报错！！！`

```shell
cd /data/u01/app/hive/apache-hive-3.1.3/bin

touch start-all.sh

#!/bin/bash
nohup $HIVE_HOME/bin/hive --service metastore &
nohup $HIVE_HOME/bin/hive --service hiveserver2 &

chmod -R 777 start-all.sh 
```

配置hive停止脚本

```shell
touch stop-all.sh

jps | grep RunJar | awk '{print $1}' | xargs kill -9

chmod -R 777 stop-all.sh 
```

你需要使用 `schematool` 来初始化 Hive Metastore 的 schema：

```bash
schematool -dbType oracle -initSchema
```

## 6. 启动 Hive
现在你可以启动 Hive 并进入 Hive 命令行界面：

```bash
sh start-all.sh
```

判断linux端口使用是否已经监听

```shell
[hadoop@cesdb bin]$ netstat -ntulp |grep 9083
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp6       0      0 :::9083                 :::*                    LISTEN      120633/java         
[hadoop@cesdb bin]$ ps -ef | grep 120633
hadoop   120633      1  8 09:35 pts/11   00:00:15 /usr/jdk/bin/java -Dproc_jar -Dproc_metastore -Dlog4j2.formatMsgNoLookups=true -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/data/u01/app/hive/apache-hive-3.1.3/conf/parquet-logging.properties -Dyarn.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dyarn.root.logger=INFO,console -Djava.library.path=/data/u01/app/hadoop/hadoop-3.3.6/lib/native -Xmx256m -Dhadoop.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /data/u01/app/hive/apache-hive-3.1.3/lib/hive-metastore-3.1.3.jar org.apache.hadoop.hive.metastore.HiveMetaStore
hadoop   128822 191526  0 09:37 pts/11   00:00:00 grep --color=auto 120633
[hadoop@cesdb bin]$ netstat -ntulp | grep 10000
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp6       0      0 :::10000                :::*                    LISTEN      120634/java         
[hadoop@cesdb bin]$ ps -ef | grep 120634
hadoop   120634      1 11 09:35 pts/11   00:00:27 /usr/jdk/bin/java -Dproc_jar -Dproc_hiveserver2 -Dlog4j2.formatMsgNoLookups=true -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/data/u01/app/hive/apache-hive-3.1.3/conf/parquet-logging.properties -Djline.terminal=jline.UnsupportedTerminal -Dyarn.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dyarn.root.logger=INFO,console -Djava.library.path=/data/u01/app/hadoop/hadoop-3.3.6/lib/native -Xmx256m -Dhadoop.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /data/u01/app/hive/apache-hive-3.1.3/lib/hive-service-3.1.3.jar org.apache.hive.service.server.HiveServer2
hadoop   131711 191526  0 09:39 pts/11   00:00:00 grep --color=auto 120634

```



## 7. 测试 Hive

在 Hive 命令行中运行一些基本命令来验证 Hive 的安装：

这是第一台客户端（已经不用了）

```sql
hive

CREATE TABLE test_table (id INT, name STRING);

SHOW TABLES;

INSERT INTO test_table (id, name) VALUES (1, 'Alice'), (2, 'Bob');

select * from test_table;
```

如果这些命令成功执行并返回结果，说明 Hive 安装和配置成功。

第二代客户端 beeline

HiveServer2通过Metastore服务读写元数据。所以在远程模式下，启动HiveServer2之前必须先启动metastore服务。

远程模式下，Beeline客户端只能通过HiveServer2服务访问Hive，而bin/hive是通过Metastore服务访问的，关系如下：

![](D:\Github\MyKnowledgeRepository\img\DW_img\Hive客户端和服务的关系.png)

```shell
cd $HIVE_HOME/bin

beeline
!connect jdbc:hive2://主机名:10000
然后输入用户名和密码

beeline> !connect jdbc:hive2://cesdb:10000
Connecting to jdbc:hive2://cesdb:10000
Enter username for jdbc:hive2://cesdb:10000: hadoop
Enter password for jdbc:hive2://cesdb:10000: ******
Connected to: Apache Hive (version 3.1.3)
Driver: Hive JDBC (version 3.1.3)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://cesdb:10000> select * from test_table;
INFO  : Compiling command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494): select * from test_table
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.name, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494); Time taken: 3.078 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494): select * from test_table
INFO  : Completed executing command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494); Time taken: 0.006 seconds
INFO  : OK
INFO  : Concurrency mode is disabled, not creating a lock manager
+----------------+------------------+
| test_table.id  | test_table.name  |
+----------------+------------------+
| 1              | Alice            |
| 2              | Bob              |
+----------------+------------------+
2 rows selected (3.869 seconds)
```



# Hive集群搭建

1.将配置好的单机Hive即在cesdb节点上的hive分发给cesdb1和cesdb2

```bash
# 要用root用户
scp -r /data/u01/app/hive root@cesdb1:/data/u01/app/
scp -r /data/u01/app/hive root@cesdb2:/data/u01/app/

# 修改目录权限
chown -R hadoop:hadoop hive
```

2.cesdb需要添加上这个配置，cesdb1和cesdb2修改配置hive-site.xml

```bash
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://cesdb:8020</value>
</property>

 <!-- Hive Metastore Thrift URI -->
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://cesdb:9083</value>
  </property>
```

3.进入cesdb，进入命令行，启动hive（不需要初始化元数据了，不然之前的数据全没了，只需要在元数据那台机器启动即可）

```bash
sh start-all.sh
```





# Cron定时调度hive脚本

使用 `cron` 来定时调度 Hive SQL 脚本是一个常见的方式。下面是一个完整的步骤，展示如何编写和配置脚本，以定时运行 Hive SQL 脚本。

1. 编写 Hive SQL 脚本

首先，创建一个 Hive SQL 脚本，例如 `query.hql`：

```sql

CREATE EXTERNAL TABLE IF NOT EXISTS test.dwd_bagl_patientvisit_mi (
  FID BIGINT,
  FPRN STRING,
  FTIMES INT,
  FICDVERSION TINYINT,
  FZYID STRING,
  FAGE STRING,
  FNAME STRING,
  FSEXBH STRING,
  FSEX STRING,
  FBIRTHDAY TIMESTAMP,
  fcydate TIMESTAMP
)
PARTITIONED BY (year STRING, month STRING)
STORED AS ORC
LOCATION '/user/hive/warehouse/test.db/test_hivesql';

SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE test.dwd_bagl_patientvisit_mi PARTITION(year, month)
SELECT 
  FID,
  FPRN,
  FTIMES,
  FICDVERSION,
  FZYID,
  FAGE,
  FNAME,
  FSEXBH,
  FSEX,
  FBIRTHDAY,
  fcydate,
  year(fcydate) AS year,
  month(fcydate) AS month
FROM test.ods_batj_patientvisit_mi
WHERE fcydate >= '2024-01-01' AND fcydate <= '2024-05-31';
```

将该脚本保存到某个目录，例如 `/path/to/query.hql`。

2. 编写 Shell 脚本

创建一个 Shell 脚本，例如 `run_hive_query.sh`，用来运行 Hive SQL 脚本：

```bash
#!/bin/bash

# 定义 Hive 脚本路径
HQL_SCRIPT="/data/chenli/hive/dwd_bagl_patientvisit_mi_test.hql"
LOG_DIR="/data/chenli/cron/logs"
LOG_FILE="${LOG_DIR}/hive_query_$(date +\%Y\%m\%d\%H\%M\%S).log"

# 运行 Hive 脚本
beeline -u 'jdbc:hive2://localhost:10000' -n hadoop -p hadoop -f "$HQL_SCRIPT" > "$LOG_FILE" 2>&1

# 检查执行结果
if [ $? -ne 0 ]; then
  echo "Hive query failed" | mail -s "Hive Query Failed" ****@qq.com
else
  echo "Hive query succeeded" | mail -s "Hive Query Succeeded" ****@qq.com
fi
```

将该脚本保存到某个目录，例如 `/path/to/run_hive_query.sh`。

如果有多个hive任务，可以按照以下方式

```shell
#!/bin/bash

# 定义 Hive 脚本路径
HQL_SCRIPT1="/path/to/query1.hql"
HQL_SCRIPT2="/path/to/query2.hql"

# 定义日志路径
LOG_DIR="/path/to/logs"

# 运行第一个 Hive 脚本
beeline -u 'jdbc:hive2://your-hive-server:10000/default' -n your_username -p your_password -f "$HQL_SCRIPT1" > "$LOG_DIR/hive_query1_$(date +\%Y\%m\%d\%H\%M\%S).log" 2>&1

# 检查第一个脚本的执行结果
if [ $? -ne 0 ]; then
  echo "Hive query1 failed" | mail -s "Hive Query1 Failed" your_email@example.com
  exit 1
fi

# 运行第二个 Hive 脚本
beeline -u 'jdbc:hive2://your-hive-server:10000/default' -n your_username -p your_password -f "$HQL_SCRIPT2" > "$LOG_DIR/hive_query2_$(date +\%Y\%m\%d\%H\%M\%S).log" 2>&1

# 检查第二个脚本的执行结果
if [ $? -ne 0 ]; then
  echo "Hive query2 failed" | mail -s "Hive Query2 Failed" your_email@example.com
  exit 1
fi

```



3. 确保 Shell 脚本具有执行权限

给 Shell 脚本添加执行权限：

```bash
chmod +x /path/to/run_hive_query.sh
```

4. 配置 Crontab 定时任务

编辑 Crontab 文件以配置定时任务：

```bash
crontab -e
```

添加以下行以每天定时运行 Hive SQL 脚本。例如，配置每天凌晨 1 点运行脚本：

```bash
0 1 * * * /path/to/run_hive_query.sh
```

5. 检查和验证 Crontab 配置

保存并退出编辑器后，验证 Crontab 配置是否生效：

```bash
crontab -l
```

6. 监控和日志管理

确保 Shell 脚本将日志输出重定向到指定的日志文件夹，以便后续检查：

```bash
beeline -u 'jdbc:hive2://your-hive-server:10000/default' -n your_username -p your_password -f "$HQL_SCRIPT" > /path/to/logs/hive_query_$(date +\%Y\%m\%d\%H\%M\%S).log 2>&1
```

7. 脚本错误通知

为了确保在脚本执行失败时能收到通知，使用邮件发送错误信息：

```bash
if [ $? -ne 0 ]; then
  echo "Hive query failed" | mail -s "Hive Query Failed" your_email@example.com
fi
```



# Zookeeper集群搭建

Zookeeper集群由基数台机器组成，分为leader和follower两个角色。写入数据时，要写入leader，leader同意后，再通知follower写入。客户端读取数据时，由于数据都是一样的，可以从任意一台机器上读取。而leader是选举出来的，集群中任意一台机器发现没有leader时，则会推荐自己为leader，当超过半数的机器同意它为leader时，选举结束。这样当leader宕机后很快就会选举出新的leader，保证工作正常进行。

## 前期准备

官网下载：[zookeeper下载](https://downloads.apache.org/zookeeper/)，这里我下载的是3.8.4的版本，这个版本相对稳定。要下载apache-zookeeper-3.8.4-bin.tar.gz这个版本的包，带有bin的包，不然启动会报错的。

上传服务器并解压

```
tar -zxvf  apache-zookeeper-3.8.4-bin.tar.gz -C /data/u01/app/zookeeper/
```

配置环境变量

```bash
vi /etc/profile

export ZOOKEEPER_HOME=/data/u01/app/zookeeper/zookeeper-3.8.4
export PATH=$PATH:$ZOOKEEPER_HOME/bin

source /etc/profile
```

## 修改配置文件

配置 zoo.cfg文件

```bash
cd conf
cp zoo_sample.cfg zoo.cfg

# dataDir属性设置Zookeeper的数据存放的位置 /data/u01/app/zookeeper/zookeeper-3.8.4/data/zData
vi zoo.cfg

# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/data/u01/app/zookeeper/zookeeper-3.8.4/data/zData
# the port at which the clients will connect
clientPort=2181

#  参数说明：
tickTime=2000 表示zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，时间单位是毫秒。
initLimit=10 表示Zookeeper服务器集群中连接到leader的Follower服务器初始化连接是最长能忍受多少个心跳的时间间隔数，总的时间长度就是10*2000=20秒。
syncLimit=5 表示标识leader与follower之间发送消息，请求和应答时间长度。最长不能超过多少个tickTime的时间长度，总的时间长度就是5*2000=10秒。
dataDir=/tmp/zookeeper 表示zookeeper的数据存放目录，默认在/tmp/zookeeper，此为修改项，默认情况下Zookeeper将写数据的日志文件也保存在这个目录里，也可以手动指定日志存放的目录。
clientPort=2181 表示客户端（应用程序）连接Zookeeper服务器的端口，Zookeeper会监听这个端口接受客户端的访问请求，默认为2181。
```

指定各个节点信息

```bash
# 打开zoo.cfg文件，在末尾添加几行
server.1=cesdb:2888:3888
server.2=cesdb1:2888:3888
server.3=cesdb2:2888:3888

# 参数说明
各个节点的信息格式为： server.X=A:B:C
X-->指zookeeper节点的myid，标识这个是第几号服务器。
A-->指每个节点的IP地址或者主机名。
B-->指follower和leader交换消息所使用的端口。
C-->指选举leader所使用的端口。

```

创建myid文件

```bash
# 进入zData目录
cd /data/u01/app/zookeeper/zookeeper-3.8.4/data/zData/

vim myid
1
# 输入1即可，另外两台节点分别为2、3
```

## 分发节点

```bash
scp -r /data/u01/app/zookeeper/ root@cesdb1:/data/u01/app
scp -r /data/u01/app/zookeeper/ root@cesdb2:/data/u01/app

# 配置另外两台机器的myid，分别配置2、3
```

## 启动Zookeeper集群及查看状态

```bash
# 需要在每一台机器上执行此命令
zkServer.sh start 

# 查看集群状态
zkServer.sh status

# 关闭zookeeper命令
zkServer.sh stop

# 查看jps
[root@cesdb bin]# jps.sh
==============查询当前所有服务器的jps情况==============
**************cesdb当前jps情况***************
10113 DataNode
10787 ResourceManager
149575 Jps
9893 NameNode
11014 NodeManager
145358 QuorumPeerMain(zookeeper线程)
13933 RunJar
13934 RunJar
**************cesdb1当前jps情况***************
155888 DataNode
109528 QuorumPeerMain(zookeeper线程)
156072 SecondaryNameNode
156361 NodeManager
112141 Jps
**************cesdb2当前jps情况***************
131122 Jps
136547 NodeManager
136208 DataNode
129818 QuorumPeerMain(zookeeper线程)
=======================查询完毕========================
```

## 问题

### 启动失败

[参考链接](https://blog.csdn.net/succing/article/details/127837281)

```
[root@cesdb bin]# zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /data/u01/app/zookeeper/zookeeper-3.8.4/bin/../conf/zoo.cfg
Starting zookeeper ... FAILED TO START

# 查看日志有以下错误
[root@cesdb logs]# cat zookeeper-root-server-cesdb.out 
错误: 找不到或无法加载主类 org.apache.zookeeper.server.quorum.QuorumPeerMain

```

那是因为我是按照3.4的zookeeper去安装的，原因是下载的zookeeper-3.8.4.tar.gz是未编译的，自zk3.5.5版本以后，已编译的jar包，尾部有bin，应该使用的是zookeeper-3.8.4-bin.tar.gz



# DolphinScheduler分布式搭建

**参考资料**

[官方文档](https://dolphinscheduler.apache.org/zh-cn/docs/3.2.2/guide/installation/cluster)

[集群部署方案（2 Master + 3 Worker）](https://www.cnblogs.com/DolphinScheduler/p/18066148)





# Ranger搭建





# 问题总结

## 1.Permission denied: user=dr.who, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x

当前用户权限不足，所以无法操作/目录

解决方案：给相应的目录赋予权限

```shell
hdfs dfs -chmod -R 777 /    --直接赋予根目录的权限
hdfs dfs -chmod -R 777 /test --赋予对应目录的权限
```

[解决方案](https://blog.csdn.net/weixin_42656458/article/details/113176108)



## 2.错误: 找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster

说明yarn没有正确配置

需要添加以下配置

```
<property>
    <name>yarn.application.classpath</name>
<value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
  </property>

```

## 3.配置Map任务时未能找到`org.apache.hive.hcatalog.data.JsonSerDe`类

配置Map任务时未能找到`org.apache.hive.hcatalog.data.JsonSerDe`类

第一种解决方案：

执行Hive时，手动添加jar包

```
ADD JAR $HIVE_HOME/lib/hive-hcatalog-core-3.1.3.jar;
```

第二种解决方案：

配置hive-site.xml

```
<property>
  <name>hive.aux.jars.path</name>
  <value>/user/hive/lib/hcatalog-core.jar</value>
</property>

```

配置yarn-site.xml

```
把hive的jar包路径添加到yarn-site.xml的classpath中
/data/u01/app/hive/apache-hive-3.1.3/lib/*


<property>
    <name>yarn.application.classpath</name>
<value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
  </property>
```

