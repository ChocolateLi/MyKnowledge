# 集群资源配置优化

# yarn-site.xml

```xml
<configuration>
    <!-- NodeManager的总内存和CPU核心数 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>131072</value> <!-- 每台机器的总内存：24GB,现在调整为128G -->
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>64</value> <!-- 每台机器的CPU核心数：16 ，现在调整为64核-->
    </property>

    <!-- YARN调度器每个容器的内存和CPU核心分配 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value> <!-- 单个容器最小内存：1GB，调整为2GB -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>32768</value> <!-- 单个容器最大内存：8GB，调整为32GB -->
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value> <!-- 单个容器最小CPU核心数：2，调整为1核，更加灵活 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>16</value> <!-- 单个容器最大CPU核心数：4，调整为16核 -->
    </property>
    
    <!--启用资源动态分配（如启用 Shuffle Service）。为了更好地支持资源动态分配，建议启用 YARN Shuffle Service-->
    <property>
    	<name>yarn.nodemanager.aux-services</name>
    	<value>spark_shuffle</value>
	</property>
	<property>
    	<name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    	<value>org.apache.spark.network.yarn.YarnShuffleService</value>
	</property>


    <!-- 启用LinuxContainerExecutor和CGroups支持 -->
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.cgroups.enabled</name>
        <value>true</value>
    </property>
</configuration>

```

# mapred-site.xml

```xml
<configuration>
    <!-- Mapper和Reducer的内存和CPU核心配置 -->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>8192</value> <!-- 每个Mapper任务的内存：4GB，调整为8GB -->
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>8192</value> <!-- 每个Reducer任务的内存：4GB，调整为8GB -->
    </property>
    <property>
        <name>mapreduce.map.cpu.vcores</name>
        <value>4</value> <!-- 每个Mapper任务使用2个核心，调整为4核 -->
    </property>
    <property>
        <name>mapreduce.reduce.cpu.vcores</name>
        <value>4</value> <!-- 每个Reducer任务使用2个核心，调整为4核 -->
    </property>
    
    <!-- Mapper和Reducer的JVM堆大小 -->
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx6144m -XX:+UseG1GC</value> <!-- Mapper的JVM堆大小：3GB，调整为6GB -->
    </property>
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx6144m -XX:+UseG1GC</value> <!-- Reducer的JVM堆大小：3GB，调整为6GB -->
    </property>
</configuration>

```

# hadoop-env.sh

```bash
# 设置Hadoop的全局内存和GC配置
export HADOOP_HEAPSIZE=4096  # Hadoop全局内存：4GB
export HADOOP_NAMENODE_OPTS="-Xmx2048m -XX:+UseG1GC"  # NameNode的堆内存：2GB
# export HADOOP_DATANODE_OPTS="-Xmx4096m -XX:+UseG1GC"  # DataNode的堆内存：4GB
export HDFS_DATANODE_OPTS="$HDFS_DATANODE_OPTS -Xmx4096m -XX:+UseG1GC"
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"  # 启用G1GC
# export HDFS_DATANODE_OPTS="$HDFS_DATANODE_OPTS -Xmx4096m -XX:+UseG1GC"


```

# yarn-env.sh

```bash
# 设置YARN的内存和垃圾回收策略
export HADOOP_HEAPSIZE=4096  # YARN的全局内存：4GB
export YARN_RESOURCEMANAGER_OPTS="-Xmx4096m -XX:+UseG1GC"  # ResourceManager内存：4GB
export YARN_NODEMANAGER_OPTS="-Xmx4096m -XX:+UseG1GC"  # NodeManager内存：4GB
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"
```

# hive-env.sh

```bash
# 设置Hive服务的内存配置
export HIVE_HEAPSIZE=4096  # HiveServer2的堆内存：4GB
export HADOOP_HEAPSIZE=4096  # Hive使用的Hadoop客户端内存：4GB
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"  # 启用G1GC
```

