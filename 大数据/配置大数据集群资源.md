# 大数据集群优化

# 资源配置优化

## yarn-site.xml

```xml
<configuration>
    <!-- NodeManager的总内存和CPU核心数 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>131072</value> <!-- 每台机器的总内存：24GB,现在调整为128G -->
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>64</value> <!-- 每台机器的CPU核心数：16 ，现在调整为64核-->
    </property>

    <!-- YARN调度器每个容器的内存和CPU核心分配 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value> <!-- 单个容器最小内存：1GB，调整为2GB -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>32768</value> <!-- 单个容器最大内存：8GB，调整为32GB -->
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value> <!-- 单个容器最小CPU核心数：2，调整为1核，更加灵活 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>16</value> <!-- 单个容器最大CPU核心数：4，调整为16核 -->
    </property>
    
    <!--启用资源动态分配（如启用 Shuffle Service）。为了更好地支持资源动态分配，建议启用 YARN Shuffle Service-->
    <property>
    	<name>yarn.nodemanager.aux-services</name>
    	<value>spark_shuffle</value>
	</property>
	<property>
    	<name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    	<value>org.apache.spark.network.yarn.YarnShuffleService</value>
	</property>


    <!-- 启用LinuxContainerExecutor和CGroups支持 -->
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.cgroups.enabled</name>
        <value>true</value>
    </property>
</configuration>

```

## mapred-site.xml

```xml
<configuration>
    <!-- Mapper和Reducer的内存和CPU核心配置 -->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>8192</value> <!-- 每个Mapper任务的内存：4GB，调整为8GB -->
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>8192</value> <!-- 每个Reducer任务的内存：4GB，调整为8GB -->
    </property>
    <property>
        <name>mapreduce.map.cpu.vcores</name>
        <value>4</value> <!-- 每个Mapper任务使用2个核心，调整为4核 -->
    </property>
    <property>
        <name>mapreduce.reduce.cpu.vcores</name>
        <value>4</value> <!-- 每个Reducer任务使用2个核心，调整为4核 -->
    </property>
    
    <!-- Mapper和Reducer的JVM堆大小 -->
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx6144m -XX:+UseG1GC</value> <!-- Mapper的JVM堆大小：3GB，调整为6GB -->
    </property>
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx6144m -XX:+UseG1GC</value> <!-- Reducer的JVM堆大小：3GB，调整为6GB -->
    </property>
</configuration>

```

## hadoop-env.sh

```bash
# 设置Hadoop的全局内存和GC配置
export HADOOP_HEAPSIZE=4096  # Hadoop全局内存：4GB
export HADOOP_NAMENODE_OPTS="-Xmx2048m -XX:+UseG1GC"  # NameNode的堆内存：2GB
# export HADOOP_DATANODE_OPTS="-Xmx4096m -XX:+UseG1GC"  # DataNode的堆内存：4GB
export HDFS_DATANODE_OPTS="$HDFS_DATANODE_OPTS -Xmx4096m -XX:+UseG1GC"
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"  # 启用G1GC
# export HDFS_DATANODE_OPTS="$HDFS_DATANODE_OPTS -Xmx4096m -XX:+UseG1GC"


```

## yarn-env.sh

```bash
# 设置YARN的内存和垃圾回收策略
export HADOOP_HEAPSIZE=4096  # YARN的全局内存：4GB
export YARN_RESOURCEMANAGER_OPTS="-Xmx4096m -XX:+UseG1GC"  # ResourceManager内存：4GB
export YARN_NODEMANAGER_OPTS="-Xmx4096m -XX:+UseG1GC"  # NodeManager内存：4GB
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"
```

## hive-env.sh

```bash
# 设置Hive服务的内存配置
export HIVE_HEAPSIZE=4096  # HiveServer2的堆内存：4GB
export HADOOP_HEAPSIZE=4096  # Hive使用的Hadoop客户端内存：4GB
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"  # 启用G1GC
```



# 资源调度策略的优化

## yarn-site.xml

```xml
<configuration>
    <!-- 使用容量调度器 -->
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>

    <!-- NodeManager 的总内存和 CPU 核数 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>119016</value> <!-- 每台机器分配 116 GB 内存 -->
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>56</value> <!-- 每台机器分配 56 核 -->
    </property>

    <!-- 每个容器的最小和最大资源 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value> <!-- 单容器最小 1 GB 内存 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>16384</value> <!-- 单容器最大 16 GB 内存 -->
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value> <!-- 单容器最小 1 核 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>8</value> <!-- 单容器最大 8 核 -->
    </property>

    <!-- 启用抢占机制 -->
    <property>
        <name>yarn.resourcemanager.scheduler.monitor.enable</name>
        <value>true</value>
    </property>

    <!-- 启用 Dominant Resource Calculator -->
    <property>
        <name>yarn.scheduler.capacity.resource-calculator</name>
        <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
    </property>
</configuration>

```

## capacity-scheduler.xml

```xml
<configuration>

  <!-- 设置根队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default</value>
    <description>定义根队列下的子队列</description>
  </property>

  <!-- 设置 default 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queues</name>
    <value>queue1,queue2,queue3</value>
    <description>default 队列下的子队列</description>
  </property>

  <!-- 设置 queue1 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.capacity</name>
    <value>40</value>
    <description>queue1 的初始容量为 40%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.maximum-capacity</name>
    <value>100</value>
    <description>queue1 的最大容量为 100%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.user-limit-factor</name>
    <value>2</value>
    <description>单用户在 queue1 队列中可使用的最大资源倍数</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.state</name>
    <value>RUNNING</value>
    <description>queue1 的状态设置为运行</description>
  </property>

  <!-- 设置 queue2 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.capacity</name>
    <value>30</value>
    <description>queue2 的初始容量为 30%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.maximum-capacity</name>
    <value>100</value>
    <description>queue2 的最大容量为 100%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.user-limit-factor</name>
    <value>2</value>
    <description>单用户在 queue2 队列中可使用的最大资源倍数</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.state</name>
    <value>RUNNING</value>
    <description>queue2 的状态设置为运行</description>
  </property>

  <!-- 设置 queue3 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.capacity</name>
    <value>30</value>
    <description>queue3 的初始容量为 30%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.maximum-capacity</name>
    <value>100</value>
    <description>queue3 的最大容量为 100%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.user-limit-factor</name>
    <value>2</value>
    <description>单用户在 queue3 队列中可使用的最大资源倍数</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.state</name>
    <value>RUNNING</value>
    <description>queue3 的状态设置为运行</description>
  </property>

  <!-- 设置抢占机制 -->
  <property>
    <name>yarn.resourcemanager.scheduler.monitor.enable</name>
    <value>true</value>
    <description>启用资源抢占机制</description>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.monitor.policies</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy</value>
    <description>使用容量调度器的抢占策略</description>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring-interval</name>
    <value>3000</value>
    <description>抢占监控间隔为 3000 毫秒</description>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.max-allowable-limit</name>
    <value>0.5</value>
    <description>抢占的最大可用资源比例为 50%.如果集群负载高且任务长时间占用资源，可以设置较大的比例（如 0.7）如果任务对稳定性要求较高，适合设置为较低的比例（如 0.3）。</description>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.total-preemption-per-round</name>
    <value>0.1</value>
    <description>每轮抢占的资源比例为 10%.如果需要快速调整资源分配，可以适当提高比例（如 0.2 或 0.3）.如果希望抢占更平缓，减少任务中断的影响，建议保持较低的比例（如 0.1）.</description>
  </property>

  <!-- 动态资源分配 -->
  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
    <description>基于主导资源（CPU 和内存）的动态资源分配</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.node-locality-delay</name>
    <value>40</value>
    <description>节点本地化调度延迟，单位为调度机会数</description>
  </property>

</configuration>

```

